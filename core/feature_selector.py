# -*- coding: utf-8 -*-
"""ç‰¹å¾é€‰æ‹©æ¨¡å— - å®Œæ•´ä¿®å¤ç‰ˆ (å« SmartFeatureSelector ç±»åŠå›è°ƒä¿®å¤)"""

import pandas as pd
import numpy as np
import streamlit as st
from sklearn.feature_selection import VarianceThreshold
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import warnings

warnings.filterwarnings('ignore')


# ==========================================
# 1. å›è°ƒå‡½æ•°å®šä¹‰åŒº (å¿…é¡»åœ¨ç»„ä»¶æ¸²æŸ“å‰å®šä¹‰)
# ==========================================

def _update_selection_state(new_selection):
    """é€šç”¨å›è°ƒï¼šæ›´æ–°ç‰¹å¾é€‰æ‹©çŠ¶æ€"""
    st.session_state.feature_cols = new_selection
    st.session_state.multiselect_features = new_selection


def _apply_variance_filter_callback(df, candidates, threshold_key):
    """æ–¹å·®ç­›é€‰å›è°ƒ"""
    try:
        threshold = st.session_state[threshold_key]
        selector = VarianceThreshold(threshold=threshold)
        selector.fit(df.fillna(0))
        selected = [candidates[i] for i in selector.get_support(indices=True)]
        _update_selection_state(selected)
        st.session_state['feature_selector_msg'] = f"âœ… æ–¹å·®ç­›é€‰å®Œæˆï¼šå‰©ä½™ {len(selected)} ä¸ªç‰¹å¾"
    except Exception as e:
        st.session_state['feature_selector_error'] = str(e)


def _apply_correlation_filter_callback(df, target_series, k_key):
    """ç›¸å…³æ€§ç­›é€‰å›è°ƒ"""
    try:
        k = st.session_state[k_key]
        corrs = df.corrwith(target_series).abs().sort_values(ascending=False)
        selected = corrs.head(int(k)).index.tolist()
        _update_selection_state(selected)
        st.session_state['feature_selector_msg'] = f"âœ… ç›¸å…³æ€§ç­›é€‰å®Œæˆï¼šå·²é€‰ Top-{k} ç‰¹å¾"
    except Exception as e:
        st.session_state['feature_selector_error'] = str(e)


def _apply_smart_rec_callback(feature_analysis):
    """æ™ºèƒ½æ¨èå›è°ƒ"""
    try:
        recommended = feature_analysis[feature_analysis['æ¨è'] == 'âœ“']['ç‰¹å¾'].tolist()
        _update_selection_state(recommended)
        st.session_state['feature_selector_msg'] = f"âœ… æ™ºèƒ½æ¨èå®Œæˆï¼šå·²é€‰ {len(recommended)} ä¸ªç‰¹å¾"
    except Exception as e:
        st.session_state['feature_selector_error'] = str(e)


def _apply_importance_filter_callback(top_features, candidates):
    """æ¨¡å‹é‡è¦æ€§ç­›é€‰å›è°ƒ (åº•å±‚é€»è¾‘)"""
    try:
        valid_selected = [f for f in top_features if f in candidates]
        ignored = len(top_features) - len(valid_selected)
        _update_selection_state(valid_selected)

        msg = f"âœ… æ¨¡å‹ç­›é€‰å®Œæˆï¼šå·²é€‰ {len(valid_selected)} ä¸ªç‰¹å¾"
        if ignored > 0:
            msg += f" (å¿½ç•¥äº† {ignored} ä¸ªç¼ºå¤±ç‰¹å¾)"
        st.session_state['feature_selector_msg'] = msg
    except Exception as e:
        st.session_state['feature_selector_error'] = str(e)


def _apply_importance_filter_callback_v2(sorted_features, candidates, k_key):
    """æ¨¡å‹é‡è¦æ€§ç­›é€‰å›è°ƒ (å…¨å±€ç‰ˆï¼Œè¯»å– Session State)"""
    try:
        # åŠ¨æ€ä» session_state è·å–å½“å‰çš„ Top-K å€¼
        if k_key in st.session_state:
            k = st.session_state[k_key]
        else:
            k = 20  # é»˜è®¤å…œåº•

        # æˆªå–å‰ K ä¸ªç‰¹å¾
        top_f = sorted_features[:int(k)]

        # å¤ç”¨å·²æœ‰çš„ç­›é€‰é€»è¾‘
        _apply_importance_filter_callback(top_f, candidates)
    except Exception as e:
        st.session_state['feature_selector_error'] = str(e)


def _apply_pca_callback(pca, scaler, numeric_df, current_df, feature_candidates):
    """PCAåº”ç”¨å›è°ƒ"""
    try:
        # æ‰§è¡Œè½¬æ¢
        X = numeric_df.copy().fillna(numeric_df.mean())
        X_scaled = scaler.transform(X)
        X_pca = pca.transform(X_scaled)

        # æ„å»ºæ–° DataFrame
        pc_names = [f"PC{i + 1}" for i in range(pca.n_components_)]
        df_pca = pd.DataFrame(X_pca, columns=pc_names, index=current_df.index)

        # åˆå¹¶
        df_rest = current_df.drop(columns=feature_candidates)
        df_new = pd.concat([df_rest, df_pca], axis=1)

        # æ›´æ–°å…¨å±€æ•°æ®çŠ¶æ€
        st.session_state.processed_data = df_new
        # æ›´æ–°ç‰¹å¾é€‰æ‹©çŠ¶æ€
        _update_selection_state(pc_names)

        # æ¸…ç†ä¸´æ—¶çŠ¶æ€
        st.session_state.pop('_pca_model', None)
        st.session_state.pop('_pca_scaler', None)
        st.session_state.pop('_pca_ready', None)

        st.session_state['feature_selector_msg'] = "âœ… PCA è½¬æ¢å·²åº”ç”¨ï¼æ•°æ®é›†å·²æ›´æ–°ã€‚"
    except Exception as e:
        st.session_state['feature_selector_error'] = str(e)


# ==========================================
# 2. ç±»å®šä¹‰åŒº (SmartFeatureSelector & SmartSparseDataSelector)
# ==========================================

class SmartFeatureSelector:
    """æ™ºèƒ½ç‰¹å¾é€‰æ‹©å™¨ (è¡¥å›ç¼ºå¤±çš„ç±»)"""

    MISSING_VALUE_TOLERANT_MODELS = {'XGBoost', 'LightGBM', 'CatBoost', 'éšæœºæ£®æ—', 'ExtraTrees'}

    def __init__(self, data, feature_cols, target_col, model_name=None):
        self.data = data
        self.feature_cols = feature_cols
        self.target_col = target_col
        self.model_name = model_name
        self.missing_info = {}

    def analyze_missing_values(self):
        selected = self.data[self.feature_cols + [self.target_col]]
        total = selected.size
        missing = selected.isnull().sum().sum()
        col_missing = selected.isnull().sum()

        self.missing_info = {
            'total_cells': total,
            'missing_cells': missing,
            'missing_rate': missing / total if total > 0 else 0,
            'column_missing': col_missing,
            'column_missing_rate': col_missing / len(selected),
            'rows_with_missing': (selected.isnull().sum(axis=1) > 0).sum(),
            'columns_with_high_missing': col_missing[col_missing / len(selected) > 0.3].index.tolist()
        }
        return self.missing_info

    def recommend_strategy(self):
        self.analyze_missing_values()
        recommendations = []

        if self.model_name in self.MISSING_VALUE_TOLERANT_MODELS:
            recommendations.append({
                'strategy': 'model_native',
                'priority': 1,
                'reason': f'{self.model_name}åŸç”Ÿæ”¯æŒç¼ºå¤±å€¼'
            })

        recommendations.append({
            'strategy': 'median',
            'priority': 2,
            'reason': 'ä¸­ä½æ•°å¡«å……ï¼Œå¯¹å¼‚å¸¸å€¼ç¨³å¥'
        })

        return recommendations


class SmartSparseDataSelector:
    """ç¨€ç–æ•°æ®é€‰æ‹©å™¨"""

    def __init__(self, data):
        self.data = data
        self.numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        self.sparsity_info = self._analyze()

    def _analyze(self):
        return {col: {
            'non_null_count': self.data[col].notna().sum(),
            'non_null_ratio': self.data[col].notna().mean(),
            'null_count': self.data[col].isna().sum()
        } for col in self.numeric_cols}

    def get_target_analysis(self):
        analysis = []
        for col, info in self.sparsity_info.items():
            try:
                non_null_ratio = float(info['non_null_ratio'])
                non_null_count = int(info['non_null_count'])
                null_count = int(info['null_count'])
            except TypeError:
                non_null_ratio = float(np.mean(info['non_null_ratio']))
                non_null_count = int(np.mean(info['non_null_count']))
                null_count = int(np.mean(info['null_count']))

            analysis.append({
                'å˜é‡å': col,
                'æœ‰æ•ˆæ ·æœ¬æ•°': non_null_count,
                'æœ‰æ•ˆç‡': f"{non_null_ratio * 100:.1f}%",
                'ç¼ºå¤±æ•°': null_count
            })
        return pd.DataFrame(analysis).sort_values('æœ‰æ•ˆæ ·æœ¬æ•°', ascending=False)

    def get_valid_samples_for_target(self, target_col):
        return self.data[self.data[target_col].notna()].copy()

    def analyze_features_for_target(self, target_col, min_valid_ratio=0.5):
        valid_data = self.get_valid_samples_for_target(target_col)
        n = len(valid_data)
        analysis = []
        for col in self.numeric_cols:
            if col == target_col:
                continue
            valid_count = valid_data[col].notna().sum()
            analysis.append({
                'ç‰¹å¾': col,
                'æœ‰æ•ˆæ•°': valid_count,
                'æœ‰æ•ˆç‡': f"{valid_count / n * 100:.1f}%" if n > 0 else "0%",
                'æ¨è': 'âœ“' if valid_count / n >= min_valid_ratio else 'âœ—'
            })
        return pd.DataFrame(analysis).sort_values('æœ‰æ•ˆæ•°', ascending=False)


# ==========================================
# 3. ç•Œé¢æ¸²æŸ“å‡½æ•° (show_robust_feature_selection)
# ==========================================

def show_robust_feature_selection():
    """å®Œæ•´çš„ç‰¹å¾é€‰æ‹©ç•Œé¢ï¼ˆå« PCA é™ç»´åŠæ¨¡å‹é‡è¦æ€§åé¦ˆï¼Œå·²ä¿®å¤çŠ¶æ€åŒæ­¥é—®é¢˜ï¼‰"""
    st.markdown("### ğŸ› ï¸ ç‰¹å¾é€‰æ‹©ä¸æ•°æ®é›†æ„å»º")

    # æ˜¾ç¤ºå›è°ƒæ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if 'feature_selector_msg' in st.session_state:
        st.success(st.session_state.pop('feature_selector_msg'))
    if 'feature_selector_error' in st.session_state:
        st.error(st.session_state.pop('feature_selector_error'))

    # æ•°æ®æºå›é€€æœºåˆ¶
    if st.session_state.get('processed_data') is not None:
        current_df = st.session_state.processed_data
    elif st.session_state.get('data') is not None:
        current_df = st.session_state.data
        st.info("â„¹ï¸ ä½¿ç”¨åŸå§‹æ•°æ®")
    else:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    # é‡å¤åˆ—åæ£€æŸ¥
    if current_df.columns.duplicated().any():
        current_df = current_df.loc[:, ~current_df.columns.duplicated()]
        st.session_state.processed_data = current_df
    all_columns = current_df.columns.tolist()

    # ç›®æ ‡å˜é‡é€‰æ‹©
    col1, col2 = st.columns([1, 2])
    with col1:
        default_idx = 0
        if st.session_state.get('target_col') and st.session_state.target_col in all_columns:
            default_idx = all_columns.index(st.session_state.target_col)

        target_col = st.selectbox("ğŸ¯ é€‰æ‹©ç›®æ ‡å˜é‡ (Y)", options=all_columns, index=default_idx)
        st.session_state.target_col = target_col

    # ç‰¹å¾å®Œæ•´æ€§æ£€æŸ¥
    numeric_df = current_df.select_dtypes(include=[np.number])
    if target_col in numeric_df.columns:
        numeric_df = numeric_df.drop(columns=[target_col])

    feature_candidates = numeric_df.columns.tolist()

    # è‡ªåŠ¨æ¸…æ´— session_state ä¸­çš„ feature_cols
    if 'feature_cols' in st.session_state and st.session_state.feature_cols:
        valid_features = [f for f in st.session_state.feature_cols if f in feature_candidates]
        if len(valid_features) != len(st.session_state.feature_cols):
            st.session_state.feature_cols = valid_features

    with col2:
        st.metric("å¯ç”¨æ•°å€¼ç‰¹å¾", f"{len(feature_candidates)} ä¸ª")

    # æ£€æŸ¥æ— ç©·å¤§å€¼
    if len(feature_candidates) > 0:
        check_inf = np.isinf(numeric_df).sum().sum()
        if check_inf > 0:
            st.error(f"âš ï¸ æ£€æµ‹åˆ° {check_inf} ä¸ªæ— ç©·å¤§æ•°å€¼(Inf)")
            if st.button("ğŸ§¹ ä¸€é”®ä¿®å¤å¼‚å¸¸å€¼", type="primary"):
                clean_df = current_df.copy()
                cols = numeric_df.columns
                clean_df[cols] = clean_df[cols].replace([np.inf, -np.inf], np.nan)
                clean_df[cols] = clean_df[cols].fillna(clean_df[cols].mean())
                st.session_state.data = clean_df
                st.session_state.processed_data = clean_df
                st.success("âœ… ä¿®å¤å®Œæˆï¼")
                st.rerun()
            return

    st.markdown("---")

    # ç»Ÿä¸€ä½¿ç”¨ Tabs å¸ƒå±€
    tabs = st.tabs(["ğŸ‘† æ‰‹åŠ¨é€‰æ‹©", "ğŸ“‰ æ–¹å·®ç­›é€‰", "ğŸ”— ç›¸å…³æ€§ç­›é€‰", "ğŸ§© PCAé™ç»´", "ğŸ¤– æ™ºèƒ½æ¨è", "â­ æ¨¡å‹é‡è¦æ€§"])

    # --- Tab 1: æ‰‹åŠ¨é€‰æ‹© ---
    with tabs[0]:
        st.markdown("#### æ‰‹åŠ¨é€‰æ‹©ç‰¹å¾")
        col_m1, col_m2 = st.columns(2)
        with col_m1:
            # ä½¿ç”¨ on_click å›è°ƒ
            st.button("å…¨é€‰", key="btn_all_features",
                      on_click=_update_selection_state, args=(feature_candidates,))
        with col_m2:
            # ä½¿ç”¨ on_click å›è°ƒ
            st.button("æ¸…ç©º", key="btn_clear_features",
                      on_click=_update_selection_state, args=([],))

        # å¤šé€‰æ¡†ï¼šä¾èµ– session_state è‡ªåŠ¨åŒæ­¥
        selected_features = st.multiselect(
            "é€‰æ‹©ç‰¹å¾",
            options=feature_candidates,
            default=st.session_state.get('feature_cols', []),
            key="multiselect_features"
        )
        st.session_state.feature_cols = selected_features

    # --- Tab 2: æ–¹å·®ç­›é€‰ ---
    with tabs[1]:
        st.markdown("#### æ–¹å·®é˜ˆå€¼ç­›é€‰")
        st.caption("ç§»é™¤å˜åŒ–å¾ˆå°ï¼ˆåŒ…å«ä¿¡æ¯é‡å°‘ï¼‰çš„ç‰¹å¾ã€‚")
        st.slider("æ–¹å·®é˜ˆå€¼", 0.0, 1.0, 0.0, 0.01, key="var_threshold")

        # ä½¿ç”¨å›è°ƒ
        st.button("åº”ç”¨æ–¹å·®ç­›é€‰", key="btn_var_filter",
                  on_click=_apply_variance_filter_callback,
                  args=(numeric_df, feature_candidates, "var_threshold"))

    # --- Tab 3: ç›¸å…³æ€§ç­›é€‰ ---
    with tabs[2]:
        st.markdown("#### ç›¸å…³æ€§ç­›é€‰")
        st.caption("ä¿ç•™ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§æœ€é«˜çš„ Top-K ç‰¹å¾ã€‚")
        st.number_input("ä¿ç•™ç›¸å…³æ€§æœ€é«˜çš„Kä¸ª", 1, len(feature_candidates), min(20, len(feature_candidates)),
                        key="corr_k")

        # ä½¿ç”¨å›è°ƒ
        st.button("åº”ç”¨ç›¸å…³æ€§ç­›é€‰", key="btn_corr_filter",
                  on_click=_apply_correlation_filter_callback,
                  args=(numeric_df, current_df[target_col], "corr_k"))

    # --- Tab 4: PCA é™ç»´ ---
    with tabs[3]:
        st.markdown("#### ğŸ§© ä¸»æˆåˆ†åˆ†æ (PCA) é™ç»´")

        if len(feature_candidates) < 2:
            st.warning("âš ï¸ å¯ç”¨æ•°å€¼ç‰¹å¾å°‘äº2ä¸ªï¼Œæ— æ³•è¿›è¡ŒPCAåˆ†æã€‚")
        else:
            col_pca1, col_pca2 = st.columns([1, 1])
            with col_pca1:
                pca_method = st.radio("é™ç»´ç›®æ ‡", ["æŒ‰ä¿ç•™æ–¹å·®æ¯”", "æŒ‰æŒ‡å®šç»´åº¦"], horizontal=True, key="pca_method")
            with col_pca2:
                if pca_method == "æŒ‰ä¿ç•™æ–¹å·®æ¯”":
                    var_thresh = st.slider("ç›®æ ‡è§£é‡Šæ–¹å·® (Variance Ratio)", 0.5, 0.999, 0.95, 0.01, key="pca_var")
                    pca_args = {'n_components': var_thresh}
                else:
                    max_comp = len(feature_candidates)
                    n_comp = st.slider("ç›®æ ‡ç»´åº¦ (N Components)", 1, max_comp, min(5, max_comp), key="pca_n")
                    pca_args = {'n_components': n_comp}

            # é¢„è§ˆæŒ‰é’® (ä¸ä¿®æ”¹çŠ¶æ€ï¼Œå¯ä»¥ç”¨å¸¸è§„ button)
            if st.button("ğŸ“Š é¢„è§ˆ PCA åˆ†æç»“æœ", key="btn_pca_preview"):
                try:
                    X = numeric_df.copy().fillna(numeric_df.mean())
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)

                    pca = PCA(**pca_args)
                    pca.fit(X_scaled)

                    n_pc = pca.n_components_
                    explained = pca.explained_variance_ratio_
                    cum_explained = np.cumsum(explained)

                    st.success(f"âœ… è®¡ç®—å®Œæˆï¼šç”Ÿæˆäº† {n_pc} ä¸ªä¸»æˆåˆ†ï¼Œç´¯è®¡è§£é‡Šæ–¹å·® {cum_explained[-1]:.4f}")

                    st.markdown("##### è§£é‡Šæ–¹å·®åˆ†å¸ƒ (Scree Plot)")
                    chart_df = pd.DataFrame({
                        "Component": [f"PC{i + 1}" for i in range(n_pc)],
                        "Individual Variance": explained,
                        "Cumulative Variance": cum_explained
                    })
                    st.line_chart(chart_df.set_index("Component")[["Individual Variance", "Cumulative Variance"]])

                    st.session_state['_pca_model'] = pca
                    st.session_state['_pca_scaler'] = scaler
                    st.session_state['_pca_ready'] = True

                except Exception as e:
                    st.error(f"PCA åˆ†æå‡ºé”™: {e}")

            # åº”ç”¨æŒ‰é’® (ä¿®æ”¹ DataFrame å’Œ Selectboxï¼Œå¿…é¡»ç”¨å›è°ƒ)
            if st.session_state.get('_pca_ready', False):
                st.markdown("---")
                st.warning("âš ï¸ **ç¡®è®¤æ“ä½œ**ï¼šç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å°†åˆ›å»ºæ–°çš„æ•°æ®é›†ã€‚æ‰€æœ‰åŸå§‹æ•°å€¼ç‰¹å¾å°†è¢« PC1, PC2... æ›¿æ¢ã€‚")

                st.button("ğŸš€ åº”ç”¨ PCA è½¬æ¢å¹¶æ›´æ–°æ•°æ®é›†", type="primary", key="btn_pca_apply",
                          on_click=_apply_pca_callback,
                          args=(st.session_state['_pca_model'],
                                st.session_state['_pca_scaler'],
                                numeric_df, current_df, feature_candidates))

    # --- Tab 5: æ™ºèƒ½æ¨è ---
    with tabs[4]:
        sparse_selector = SmartSparseDataSelector(current_df)

        st.markdown("#### ç›®æ ‡å˜é‡æœ‰æ•ˆæ€§åˆ†æ")
        target_analysis = sparse_selector.get_target_analysis()
        st.dataframe(target_analysis.head(10), use_container_width=True)

        feature_analysis = sparse_selector.analyze_features_for_target(target_col)

        st.dataframe(feature_analysis, use_container_width=True)

        # ä½¿ç”¨å›è°ƒ
        st.button("ğŸ¯ æ™ºèƒ½æ¨èç‰¹å¾", key="btn_smart_rec",
                  on_click=_apply_smart_rec_callback, args=(feature_analysis,))

    # --- Tab 6: æ¨¡å‹é‡è¦æ€§ (æ–°å¢) ---
    with tabs[5]:
        st.markdown("#### â­ åŸºäºå·²è®­ç»ƒæ¨¡å‹çš„é‡è¦æ€§ç­›é€‰")
        st.caption("åˆ©ç”¨ä¸Šä¸€è½®ã€æ¨¡å‹è®­ç»ƒã€‘å¾—åˆ°çš„ç‰¹å¾é‡è¦æ€§ï¼ˆæˆ–ç³»æ•°ï¼‰æ¥åå‘ä¼˜åŒ–ç‰¹å¾é›†ã€‚è¿™å¯¹äºå‰”é™¤å™ªå£°ç‰¹å¾éå¸¸æœ‰æ•ˆã€‚")

        model = st.session_state.get('model')
        if model is None:
            st.info("âš ï¸ æš‚æ— æ¨¡å‹è®°å½•ã€‚è¯·å…ˆå‰å¾€ã€ğŸ¤– æ¨¡å‹è®­ç»ƒã€‘é¡µé¢è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼ˆå¦‚éšæœºæ£®æ—ã€XGBoostï¼‰ï¼Œç„¶åå†è¿”å›æ­¤å¤„ã€‚")
        else:
            trained_model_name = st.session_state.get('model_name', 'Unknown')
            st.success(f"å½“å‰å‚è€ƒæ¨¡å‹: **{trained_model_name}**")

            # 1. å°è¯•è·å–ç‰¹å¾å
            feature_names = None
            if hasattr(model, 'feature_names_in_'):
                feature_names = model.feature_names_in_
            elif 'feature_cols' in st.session_state and len(st.session_state.feature_cols) > 0:
                feature_names = np.array(st.session_state.feature_cols)

            # 2. å°è¯•è·å–é‡è¦æ€§
            importances = None
            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_
            elif hasattr(model, 'coef_'):
                importances = np.abs(model.coef_)
                if importances.ndim > 1:
                    importances = importances.mean(axis=0)

            # 3. å±•ç¤ºä¸æ“ä½œ
            if importances is not None and feature_names is not None:
                if len(importances) != len(feature_names):
                    st.warning(
                        f"âš ï¸ ç‰¹å¾åä¸é‡è¦æ€§ç»´åº¦ä¸åŒ¹é… ({len(feature_names)} vs {len(importances)})ï¼Œæ— æ³•ç²¾ç¡®å¯¹åº”ã€‚")
                else:
                    imp_df = pd.DataFrame({
                        'Feature': feature_names,
                        'Importance': importances
                    }).sort_values(by='Importance', ascending=False).reset_index(drop=True)

                    max_imp = imp_df['Importance'].max()
                    if max_imp > 0:
                        imp_df['Importance'] = imp_df['Importance'] / max_imp

                    col_imp1, col_imp2 = st.columns([2, 1])

                    with col_imp1:
                        st.markdown("##### ç‰¹å¾é‡è¦æ€§æ’åº (Top 20)")
                        st.bar_chart(imp_df.set_index('Feature').head(20))

                    with col_imp2:
                        st.markdown("##### âœ‚ï¸ ç­›é€‰è®¾ç½®")
                        max_k = len(feature_names)
                        # key="imp_top_k" ä¾›å›è°ƒè¯»å–
                        st.number_input("ä¿ç•™ Top-K", 1, max_k, min(20, max_k), key="imp_top_k")

                        sorted_features = imp_df['Feature'].tolist()

                        # ä½¿ç”¨å…¨å±€å›è°ƒå‡½æ•° _apply_importance_filter_callback_v2
                        st.button(
                            "âœ… åº”ç”¨ç­›é€‰",
                            type="primary",
                            key="btn_imp_apply",
                            on_click=_apply_importance_filter_callback_v2,
                            args=(sorted_features, feature_candidates, "imp_top_k")
                        )
            else:
                st.warning("âŒ å½“å‰æ¨¡å‹ä¸æ”¯æŒç›´æ¥æå–ç‰¹å¾é‡è¦æ€§ï¼ˆæˆ–è€…æœªè®°å½•ç‰¹å¾åï¼‰ï¼Œè¯·å°è¯•ä½¿ç”¨ SHAP åˆ†ææˆ–æ‰‹åŠ¨ç­›é€‰ã€‚")

    # æ˜¾ç¤ºå·²é€‰ç‰¹å¾æ‘˜è¦
    if st.session_state.get('feature_cols'):
        st.markdown("---")
        st.markdown(f"### âœ… å·²é€‰æ‹© {len(st.session_state.feature_cols)} ä¸ªç‰¹å¾")

        cols = st.columns(4)
        for i, feat in enumerate(st.session_state.feature_cols[:20]):
            with cols[i % 4]:
                st.markdown(
                    f"<span style='background:#E0E7FF;padding:4px 8px;border-radius:12px;font-size:0.85rem;'>{feat}</span>",
                    unsafe_allow_html=True)

        if len(st.session_state.feature_cols) > 20:
            st.caption(f"... ç­‰å…± {len(st.session_state.feature_cols)} ä¸ªç‰¹å¾")

        # æ•°æ®é¢„è§ˆ
        st.markdown("#### ğŸ“‹ æ•°æ®é¢„è§ˆ")
        available_preview = [c for c in st.session_state.feature_cols if c in current_df.columns]
        if available_preview:
            preview_cols = available_preview[:5] + [
                target_col] if target_col in current_df.columns else available_preview[:5]
            st.dataframe(current_df[preview_cols].head(), use_container_width=True)