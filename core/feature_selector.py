# -*- coding: utf-8 -*-
"""ç‰¹å¾é€‰æ‹©æ¨¡å— - å®Œæ•´ç‰ˆ (å«PCAé™ç»´ä¼˜åŒ–)"""

import pandas as pd
import numpy as np
import streamlit as st
from sklearn.feature_selection import VarianceThreshold
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import warnings

warnings.filterwarnings('ignore')


class SmartFeatureSelector:
    """æ™ºèƒ½ç‰¹å¾é€‰æ‹©å™¨"""

    MISSING_VALUE_TOLERANT_MODELS = {'XGBoost', 'LightGBM', 'CatBoost', 'éšæœºæ£®æ—', 'ExtraTrees'}

    def __init__(self, data, feature_cols, target_col, model_name=None):
        self.data = data
        self.feature_cols = feature_cols
        self.target_col = target_col
        self.model_name = model_name
        self.missing_info = {}

    def analyze_missing_values(self):
        selected = self.data[self.feature_cols + [self.target_col]]
        total = selected.size
        missing = selected.isnull().sum().sum()
        col_missing = selected.isnull().sum()

        self.missing_info = {
            'total_cells': total,
            'missing_cells': missing,
            'missing_rate': missing / total if total > 0 else 0,
            'column_missing': col_missing,
            'column_missing_rate': col_missing / len(selected),
            'rows_with_missing': (selected.isnull().sum(axis=1) > 0).sum(),
            'columns_with_high_missing': col_missing[col_missing / len(selected) > 0.3].index.tolist()
        }
        return self.missing_info

    def recommend_strategy(self):
        self.analyze_missing_values()
        recommendations = []

        if self.model_name in self.MISSING_VALUE_TOLERANT_MODELS:
            recommendations.append({
                'strategy': 'model_native',
                'priority': 1,
                'reason': f'{self.model_name}åŸç”Ÿæ”¯æŒç¼ºå¤±å€¼'
            })

        recommendations.append({
            'strategy': 'median',
            'priority': 2,
            'reason': 'ä¸­ä½æ•°å¡«å……ï¼Œå¯¹å¼‚å¸¸å€¼ç¨³å¥'
        })

        return recommendations


class SmartSparseDataSelector:
    """ç¨€ç–æ•°æ®é€‰æ‹©å™¨"""

    def __init__(self, data):
        self.data = data
        self.numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        self.sparsity_info = self._analyze()

    def _analyze(self):
        return {col: {
            'non_null_count': self.data[col].notna().sum(),
            'non_null_ratio': self.data[col].notna().mean(),
            'null_count': self.data[col].isna().sum()
        } for col in self.numeric_cols}

    def get_target_analysis(self):
        analysis = []
        for col, info in self.sparsity_info.items():
            try:
                non_null_ratio = float(info['non_null_ratio'])
                non_null_count = int(info['non_null_count'])
                null_count = int(info['null_count'])
            except TypeError:
                non_null_ratio = float(np.mean(info['non_null_ratio']))
                non_null_count = int(np.mean(info['non_null_count']))
                null_count = int(np.mean(info['null_count']))

            analysis.append({
                'å˜é‡å': col,
                'æœ‰æ•ˆæ ·æœ¬æ•°': non_null_count,
                'æœ‰æ•ˆç‡': f"{non_null_ratio * 100:.1f}%",
                'ç¼ºå¤±æ•°': null_count
            })

        return pd.DataFrame(analysis).sort_values('æœ‰æ•ˆæ ·æœ¬æ•°', ascending=False)

    def get_valid_samples_for_target(self, target_col):
        return self.data[self.data[target_col].notna()].copy()

    def analyze_features_for_target(self, target_col, min_valid_ratio=0.5):
        valid_data = self.get_valid_samples_for_target(target_col)
        n = len(valid_data)

        analysis = []
        for col in self.numeric_cols:
            if col == target_col:
                continue
            valid_count = valid_data[col].notna().sum()
            analysis.append({
                'ç‰¹å¾': col,
                'æœ‰æ•ˆæ•°': valid_count,
                'æœ‰æ•ˆç‡': f"{valid_count / n * 100:.1f}%" if n > 0 else "0%",
                'æ¨è': 'âœ“' if valid_count / n >= min_valid_ratio else 'âœ—'
            })

        return pd.DataFrame(analysis).sort_values('æœ‰æ•ˆæ•°', ascending=False)


def show_robust_feature_selection():
    """å®Œæ•´çš„ç‰¹å¾é€‰æ‹©ç•Œé¢ï¼ˆå« PCA é™ç»´ï¼‰"""
    st.markdown("### ğŸ› ï¸ ç‰¹å¾é€‰æ‹©ä¸æ•°æ®é›†æ„å»º")

    # æ•°æ®æºå›é€€æœºåˆ¶
    if st.session_state.get('processed_data') is not None:
        current_df = st.session_state.processed_data
    elif st.session_state.get('data') is not None:
        current_df = st.session_state.data
        st.info("â„¹ï¸ ä½¿ç”¨åŸå§‹æ•°æ®")
    else:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    # é‡å¤åˆ—åæ£€æŸ¥
    if current_df.columns.duplicated().any():
        current_df = current_df.loc[:, ~current_df.columns.duplicated()]
        st.session_state.processed_data = current_df
    all_columns = current_df.columns.tolist()

    # ç›®æ ‡å˜é‡é€‰æ‹©
    col1, col2 = st.columns([1, 2])
    with col1:
        default_idx = 0
        if st.session_state.get('target_col') and st.session_state.target_col in all_columns:
            default_idx = all_columns.index(st.session_state.target_col)

        target_col = st.selectbox("ğŸ¯ é€‰æ‹©ç›®æ ‡å˜é‡ (Y)", options=all_columns, index=default_idx)
        st.session_state.target_col = target_col

    # ç‰¹å¾å®Œæ•´æ€§æ£€æŸ¥
    numeric_df = current_df.select_dtypes(include=[np.number])
    if target_col in numeric_df.columns:
        numeric_df = numeric_df.drop(columns=[target_col])

    feature_candidates = numeric_df.columns.tolist()

    # è‡ªåŠ¨æ¸…æ´— session_state ä¸­çš„ feature_cols
    if 'feature_cols' in st.session_state and st.session_state.feature_cols:
        valid_features = [f for f in st.session_state.feature_cols if f in feature_candidates]
        if len(valid_features) != len(st.session_state.feature_cols):
            st.session_state.feature_cols = valid_features

    with col2:
        st.metric("å¯ç”¨æ•°å€¼ç‰¹å¾", f"{len(feature_candidates)} ä¸ª")

    # æ£€æŸ¥æ— ç©·å¤§å€¼
    if len(feature_candidates) > 0:
        check_inf = np.isinf(numeric_df).sum().sum()
        if check_inf > 0:
            st.error(f"âš ï¸ æ£€æµ‹åˆ° {check_inf} ä¸ªæ— ç©·å¤§æ•°å€¼(Inf)")
            if st.button("ğŸ§¹ ä¸€é”®ä¿®å¤å¼‚å¸¸å€¼", type="primary"):
                clean_df = current_df.copy()
                cols = numeric_df.columns
                clean_df[cols] = clean_df[cols].replace([np.inf, -np.inf], np.nan)
                clean_df[cols] = clean_df[cols].fillna(clean_df[cols].mean())
                st.session_state.data = clean_df
                st.session_state.processed_data = clean_df
                st.success("âœ… ä¿®å¤å®Œæˆï¼")
                st.rerun()
            return

    st.markdown("---")

    # ç»Ÿä¸€ä½¿ç”¨ Tabs å¸ƒå±€ï¼Œæ— è®ºç‰¹å¾å¤šå°‘éƒ½æä¾›æ‰€æœ‰å·¥å…·
    # è¿™æ ·ç”¨æˆ·åœ¨å°‘é‡ç‰¹å¾æ—¶ä¹Ÿèƒ½ä½¿ç”¨ PCA
    tabs = st.tabs(["ğŸ‘† æ‰‹åŠ¨é€‰æ‹©", "ğŸ“‰ æ–¹å·®ç­›é€‰", "ğŸ”— ç›¸å…³æ€§ç­›é€‰", "ğŸ§© PCAé™ç»´", "ğŸ¤– æ™ºèƒ½æ¨è"])

    # --- Tab 1: æ‰‹åŠ¨é€‰æ‹© ---
    with tabs[0]:
        st.markdown("#### æ‰‹åŠ¨é€‰æ‹©ç‰¹å¾")
        col_m1, col_m2 = st.columns(2)
        with col_m1:
            if st.button("å…¨é€‰", key="btn_all_features"):
                st.session_state.feature_cols = feature_candidates
        with col_m2:
            if st.button("æ¸…ç©º", key="btn_clear_features"):
                st.session_state.feature_cols = []

        selected_features = st.multiselect(
            "é€‰æ‹©ç‰¹å¾",
            options=feature_candidates,
            default=st.session_state.get('feature_cols', []),
            key="multiselect_features"
        )
        st.session_state.feature_cols = selected_features

    # --- Tab 2: æ–¹å·®ç­›é€‰ ---
    with tabs[1]:
        st.markdown("#### æ–¹å·®é˜ˆå€¼ç­›é€‰")
        st.caption("ç§»é™¤å˜åŒ–å¾ˆå°ï¼ˆåŒ…å«ä¿¡æ¯é‡å°‘ï¼‰çš„ç‰¹å¾ã€‚")
        threshold = st.slider("æ–¹å·®é˜ˆå€¼", 0.0, 1.0, 0.0, 0.01, key="var_threshold")
        if st.button("åº”ç”¨æ–¹å·®ç­›é€‰", key="btn_var_filter"):
            selector = VarianceThreshold(threshold=threshold)
            selector.fit(numeric_df.fillna(0))
            selected = [feature_candidates[i] for i in selector.get_support(indices=True)]
            st.session_state.feature_cols = selected
            st.success(f"âœ… ç­›é€‰åå‰©ä½™ {len(selected)} ä¸ªç‰¹å¾")

    # --- Tab 3: ç›¸å…³æ€§ç­›é€‰ ---
    with tabs[2]:
        st.markdown("#### ç›¸å…³æ€§ç­›é€‰")
        st.caption("ä¿ç•™ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§æœ€é«˜çš„ Top-K ç‰¹å¾ã€‚")
        k = st.number_input("ä¿ç•™ç›¸å…³æ€§æœ€é«˜çš„Kä¸ª", 1, len(feature_candidates), min(20, len(feature_candidates)),
                            key="corr_k")
        if st.button("åº”ç”¨ç›¸å…³æ€§ç­›é€‰", key="btn_corr_filter"):
            corrs = numeric_df.corrwith(current_df[target_col]).abs().sort_values(ascending=False)
            selected = corrs.head(int(k)).index.tolist()
            st.session_state.feature_cols = selected
            st.success(f"âœ… å·²é€‰æ‹© {len(selected)} ä¸ªç‰¹å¾")

    # --- Tab 4: PCA é™ç»´ (æ–°å¢) ---
    with tabs[3]:
        st.markdown("#### ğŸ§© ä¸»æˆåˆ†åˆ†æ (PCA) é™ç»´")
        st.info(
            "é€šè¿‡çº¿æ€§å˜æ¢å°†åŸå§‹ç‰¹å¾æ˜ å°„åˆ°ä½ç»´ç©ºé—´ï¼Œç”Ÿæˆäº’ä¸ç›¸å…³çš„ä¸»æˆåˆ† (PC)ã€‚\n\n**æ³¨æ„ï¼š** åº”ç”¨PCAè½¬æ¢åï¼ŒåŸå§‹æ•°å€¼ç‰¹å¾å°†è¢«æ›¿æ¢ä¸º PC1, PC2...ï¼Œè¿™ä¼šä¸¢å¤±ç‰©ç†å«ä¹‰çš„å¯è§£é‡Šæ€§ï¼Œä½†èƒ½æœ‰æ•ˆæ¶ˆé™¤å…±çº¿æ€§å¹¶å‹ç¼©ç»´åº¦ã€‚")

        if len(feature_candidates) < 2:
            st.warning("âš ï¸ å¯ç”¨æ•°å€¼ç‰¹å¾å°‘äº2ä¸ªï¼Œæ— æ³•è¿›è¡ŒPCAåˆ†æã€‚")
        else:
            col_pca1, col_pca2 = st.columns([1, 1])
            with col_pca1:
                pca_method = st.radio("é™ç»´ç›®æ ‡", ["æŒ‰ä¿ç•™æ–¹å·®æ¯”", "æŒ‰æŒ‡å®šç»´åº¦"], horizontal=True, key="pca_method")
            with col_pca2:
                if pca_method == "æŒ‰ä¿ç•™æ–¹å·®æ¯”":
                    var_thresh = st.slider("ç›®æ ‡è§£é‡Šæ–¹å·® (Variance Ratio)", 0.5, 0.999, 0.95, 0.01, key="pca_var")
                    pca_args = {'n_components': var_thresh}
                else:
                    max_comp = len(feature_candidates)
                    n_comp = st.slider("ç›®æ ‡ç»´åº¦ (N Components)", 1, max_comp, min(5, max_comp), key="pca_n")
                    pca_args = {'n_components': n_comp}

            # é¢„è§ˆ/åˆ†ææŒ‰é’®
            if st.button("ğŸ“Š é¢„è§ˆ PCA åˆ†æç»“æœ", key="btn_pca_preview"):
                try:
                    # å‡†å¤‡æ•°æ®ï¼šPCAä¸æ”¯æŒNaNï¼Œè¿™é‡Œç”¨å‡å€¼å¡«å……ï¼ˆå‡è®¾å·²ç»åšè¿‡åŸºæœ¬æ¸…æ´—ï¼‰
                    X = numeric_df.copy()
                    X = X.fillna(X.mean())

                    # æ ‡å‡†åŒ–æ˜¯ PCA çš„å‰ç½®å¿…è¦æ­¥éª¤
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)

                    pca = PCA(**pca_args)
                    pca.fit(X_scaled)

                    # æŒ‡æ ‡è®¡ç®—
                    n_pc = pca.n_components_
                    explained = pca.explained_variance_ratio_
                    cum_explained = np.cumsum(explained)

                    st.success(f"âœ… è®¡ç®—å®Œæˆï¼šç”Ÿæˆäº† {n_pc} ä¸ªä¸»æˆåˆ†ï¼Œç´¯è®¡è§£é‡Šæ–¹å·® {cum_explained[-1]:.4f}")

                    # å¯è§†åŒ–ï¼šScree Plot
                    st.markdown("##### è§£é‡Šæ–¹å·®åˆ†å¸ƒ (Scree Plot)")
                    chart_df = pd.DataFrame({
                        "Component": [f"PC{i + 1}" for i in range(n_pc)],
                        "Individual Variance": explained,
                        "Cumulative Variance": cum_explained
                    })
                    st.line_chart(chart_df.set_index("Component")[["Individual Variance", "Cumulative Variance"]])

                    # ä¿å­˜æ¨¡å‹åˆ° session ä»¥ä¾¿åº”ç”¨
                    st.session_state['_pca_model'] = pca
                    st.session_state['_pca_scaler'] = scaler
                    st.session_state['_pca_ready'] = True

                except Exception as e:
                    st.error(f"PCA åˆ†æå‡ºé”™: {e}")

            # åº”ç”¨æŒ‰é’®
            if st.session_state.get('_pca_ready', False):
                st.markdown("---")
                st.warning(
                    "âš ï¸ **ç¡®è®¤æ“ä½œ**ï¼šç‚¹å‡»ä¸‹æ–¹æŒ‰é’®å°†åˆ›å»ºæ–°çš„æ•°æ®é›†ã€‚æ‰€æœ‰åŸå§‹æ•°å€¼ç‰¹å¾å°†è¢« PC1, PC2... æ›¿æ¢ã€‚æ­¤æ“ä½œä¸å¯é€†ï¼ˆé™¤éé‡æ–°åŠ è½½æ–‡ä»¶ï¼‰ã€‚")

                if st.button("ğŸš€ åº”ç”¨ PCA è½¬æ¢å¹¶æ›´æ–°æ•°æ®é›†", type="primary", key="btn_pca_apply"):
                    pca = st.session_state['_pca_model']
                    scaler = st.session_state['_pca_scaler']

                    # æ‰§è¡Œè½¬æ¢
                    X = numeric_df.copy().fillna(numeric_df.mean())
                    X_scaled = scaler.transform(X)
                    X_pca = pca.transform(X_scaled)

                    # æ„å»ºæ–° DataFrame
                    pc_names = [f"PC{i + 1}" for i in range(pca.n_components_)]
                    df_pca = pd.DataFrame(X_pca, columns=pc_names, index=current_df.index)

                    # åˆå¹¶ï¼šåˆ é™¤æ—§ç‰¹å¾ï¼Œä¿ç•™ç›®æ ‡åˆ—å’Œå…¶ä»–éæ•°å€¼åˆ—ï¼ˆå¦‚æ–‡æœ¬ã€å…ƒæ•°æ®ï¼‰
                    df_rest = current_df.drop(columns=feature_candidates)
                    df_new = pd.concat([df_rest, df_pca], axis=1)

                    # æ›´æ–°å…¨å±€çŠ¶æ€
                    st.session_state.processed_data = df_new
                    st.session_state.feature_cols = pc_names

                    # æ¸…ç†ä¸´æ—¶çŠ¶æ€
                    st.session_state.pop('_pca_model', None)
                    st.session_state.pop('_pca_scaler', None)
                    st.session_state.pop('_pca_ready', None)

                    st.success(f"âœ… æ•°æ®é›†å·²æ›´æ–°ï¼å½“å‰ç‰¹å¾é›†: {pc_names}")
                    st.rerun()

    # --- Tab 5: æ™ºèƒ½æ¨è ---
    with tabs[4]:
        # æ™ºèƒ½ç¨€ç–æ•°æ®åˆ†æ
        sparse_selector = SmartSparseDataSelector(current_df)

        st.markdown("#### ç›®æ ‡å˜é‡æœ‰æ•ˆæ€§åˆ†æ")
        target_analysis = sparse_selector.get_target_analysis()
        st.dataframe(target_analysis.head(10), use_container_width=True)

        if st.button("ğŸ¯ æ™ºèƒ½æ¨èç‰¹å¾", key="btn_smart_rec"):
            feature_analysis = sparse_selector.analyze_features_for_target(target_col)
            recommended = feature_analysis[feature_analysis['æ¨è'] == 'âœ“']['ç‰¹å¾'].tolist()
            st.session_state.feature_cols = recommended
            st.success(f"âœ… æ¨è {len(recommended)} ä¸ªç‰¹å¾")
            st.dataframe(feature_analysis, use_container_width=True)

    # æ˜¾ç¤ºå·²é€‰ç‰¹å¾æ‘˜è¦
    if st.session_state.get('feature_cols'):
        st.markdown("---")
        st.markdown(f"### âœ… å·²é€‰æ‹© {len(st.session_state.feature_cols)} ä¸ªç‰¹å¾")

        cols = st.columns(4)
        for i, feat in enumerate(st.session_state.feature_cols[:20]):
            with cols[i % 4]:
                st.markdown(
                    f"<span style='background:#E0E7FF;padding:4px 8px;border-radius:12px;font-size:0.85rem;'>{feat}</span>",
                    unsafe_allow_html=True)

        if len(st.session_state.feature_cols) > 20:
            st.caption(f"... ç­‰å…± {len(st.session_state.feature_cols)} ä¸ªç‰¹å¾")

        # æ•°æ®é¢„è§ˆ
        st.markdown("#### ğŸ“‹ æ•°æ®é¢„è§ˆ")
        # ç¡®ä¿ preview cols å­˜åœ¨äº current_df ä¸­
        available_preview = [c for c in st.session_state.feature_cols if c in current_df.columns]
        preview_cols = available_preview[:5] + [target_col]
        # è¿‡æ»¤æ‰ä¸å­˜åœ¨çš„åˆ—ï¼ˆé˜²æ­¢PCAè½¬æ¢åå¼•ç”¨æ—§åˆ—åæŠ¥é”™ï¼‰
        preview_cols = [c for c in preview_cols if c in current_df.columns]

        st.dataframe(current_df[preview_cols].head(), use_container_width=True)