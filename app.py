# -*- coding: utf-8 -*-
"""
ç¢³çº¤ç»´å¤åˆææ–™æ™ºèƒ½é¢„æµ‹å¹³å° v1.3.0
æ›´æ–°å†…å®¹ï¼š
1. ä¿®å¤SHAPå›¾è¡¨æ˜¾ç¤ºå’Œç‰¹å¾åç¼ºå¤±é—®é¢˜
2. ä¼˜åŒ–æ‰€æœ‰å›¾è¡¨å¸ƒå±€ï¼Œé˜²æ­¢ç¼©æ”¾å˜å½¢
3. ä¸ºæ‰€æœ‰å›¾è¡¨å¢åŠ æ•°æ®å¯¼å‡º(CSV)åŠŸèƒ½
4. å¢åŠ åŒç»„åˆ†åˆ†å­æŒ‡çº¹æ‹¼æ¥åŠŸèƒ½
5. å¢åŠ è®­ç»ƒè„šæœ¬ä¸€é”®å¯¼å‡ºåŠŸèƒ½
"""
# [æ–°å¢] TensorFlow Sequential (TFS) æ¨¡å‹æ”¯æŒï¼ˆå³ä½¿æœªå®‰è£… TF ä¹Ÿè¦æ˜¾ç¤ºå…¥å£ï¼‰
try:
    from core.tf_model import (
        TFSequentialRegressor,
        TENSORFLOW_AVAILABLE,
        TFS_TUNING_PARAMS,
        TENSORFLOW_IMPORT_ERROR,
        get_tensorflow_version
    )
except Exception as e:
    # ä»»ä½•å¼‚å¸¸éƒ½ä¸åº”é˜»æ­¢åº”ç”¨å¯åŠ¨
    TENSORFLOW_AVAILABLE = False
    TFSequentialRegressor = None
    TFS_TUNING_PARAMS = []
    TENSORFLOW_IMPORT_ERROR = repr(e)

    def get_tensorflow_version():
        return None

# [æ–°å¢] ç‰¹å¾å·¥ç¨‹çŠ¶æ€è¿½è¸ªå™¨
from core.fe_tracker import (
    FeatureEngineeringTracker,
    render_status_sidebar,
    render_status_panel,
    render_data_export_panel,
    create_quick_export_button
)

try:
    import torchani

    TORCHANI_AVAILABLE = True
except ImportError:
    TORCHANI_AVAILABLE = False
import streamlit as st

# =========================
# Operation Log Utilities
# =========================
def _oplog_init():
    if "oplog" not in st.session_state:
        st.session_state["oplog"] = []

def oplog(msg: str):
    """Append a timestamped message to operation log and show it in UI."""
    _oplog_init()
    import datetime as _dtmod
    ts = _dtmod.datetime.now().strftime("%H:%M:%S")
    st.session_state["oplog"].append(f"[{ts}] {msg}")

def oplog_clear():
    st.session_state["oplog"] = []

def oplog_render():
    _oplog_init()
    with st.expander("ğŸ§¾ Operation Log", expanded=False):
        if st.session_state["oplog"]:
            st.code("\n".join(st.session_state["oplog"]))
        else:
            st.caption("No operations yet.")
        c1, c2 = st.columns([1, 5])
        with c1:
            if st.button("Clear Log"):
                oplog_clear()
                st.rerun()
import pandas as pd
<<<<<<< HEAD

from rdkit import Chem as _Chem

def _quick_rdkit_parse_stats(smiles_list, max_check: int = 200):
    """Fast parse-only check (no 3D). Returns (ok_count, checked_count, examples_bad)."""
    checked = 0
    ok = 0
    bad = []
    for s in smiles_list:
        if checked >= max_check:
            break
        if s is None:
            continue
        ss = str(s).strip()
        if (not ss) or (ss.lower() in {"nan", "none", "<na>", "na"}):
            continue
        checked += 1

        # split like 3D worker: ;,ï¼›,| and " + ", then "."
        parts = re.split(r"\s*[;ï¼›|]\s*", ss)
        frags = []
        for p in parts:
            if not p:
                continue
            for q in re.split(r"\s+\+\s+", p):
                frags.extend([x.strip() for x in str(q).split('.') if x and str(x).strip()])

        parsed_any = False
        for frag in frags:
            m = _Chem.MolFromSmiles(frag)
            if m is not None and m.GetNumAtoms() >= 2:
                parsed_any = True
                break

        if parsed_any:
            ok += 1
        else:
            if len(bad) < 5:
                bad.append(ss[:200])
    return ok, checked, bad
=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4
import numpy as np
import os
import matplotlib.pyplot as plt
import torch
import traceback
import json
import io
from datetime import datetime
import multiprocessing as mp
import warnings

warnings.filterwarnings('ignore')

# ç»Ÿä¸€ matplotlib é£æ ¼ï¼ˆå…¨ç«™å›¾è¡¨ä¸€è‡´ï¼‰
try:
    from core.plot_style import apply_global_style
    apply_global_style()
except Exception:
    pass

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="ç¢³çº¤ç»´å¤åˆææ–™æ™ºèƒ½é¢„æµ‹å¹³å°",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from core.data_processor import AdvancedDataCleaner, SparseDataHandler, DataEnhancer
from core.data_explorer import EnhancedDataExplorer
from core.model_trainer import EnhancedModelTrainer, AutoGluonWrapper  # ç¡®ä¿å¼•å…¥ Wrapper
from core.model_interpreter import ModelInterpreter, EnhancedModelInterpreter
from core.molecular_features import AdvancedMolecularFeatureExtractor, RDKitFeatureExtractor
from core.feature_selector import SmartFeatureSelector, SmartSparseDataSelector, show_robust_feature_selection
from core.optimizer import HyperparameterOptimizer, InverseDesigner, generate_tuning_suggestions
from core.visualizer import Visualizer
from core.plot_utils import fig_to_png_bytes, fig_to_html
from core.training_curves import plot_history
from core.training_runs import TrainingRunManager
from core.applicability_domain import ApplicabilityDomainAnalyzer, TanimotoADAnalyzer
from core.ui_config import (
    MANUAL_TUNING_PARAMS,
    MODEL_PARAMETERS,
    DEFAULT_OPTUNA_TRIALS,
    DEFAULT_TEST_SIZE,
    DEFAULT_RANDOM_STATE
)

from config import APP_NAME, VERSION, DATA_DIR
from generate_sample_data import generate_hybrid_dataset, generate_pure_numeric_dataset

# å¯é€‰æ¨¡å—å¯¼å…¥
try:
    from core.molecular_features import OptimizedRDKitFeatureExtractor, MemoryEfficientRDKitExtractor, \
        FingerprintExtractor

    OPTIMIZED_EXTRACTOR_AVAILABLE = True
except ImportError:
    OPTIMIZED_EXTRACTOR_AVAILABLE = False

try:
    from core.graph_utils import GNNFeaturizer, smiles_to_pyg_graph

    GNN_AVAILABLE = True
except ImportError:
    GNN_AVAILABLE = False

try:
    from core.ann_model import ANNRegressor

    ANN_AVAILABLE = True
except ImportError:
    ANN_AVAILABLE = False

try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False


@st.cache_data(ttl=3600, show_spinner="æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶...")
def load_data_file(uploaded_file):
    """å¸¦ç¼“å­˜çš„æ•°æ®åŠ è½½å‡½æ•°ï¼Œé¿å…æ¯æ¬¡äº¤äº’éƒ½é‡æ–°è¯»å–æ–‡ä»¶"""
    # å¿…é¡»é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å¤´ï¼Œå› ä¸ºStreamlitå¯èƒ½ä¼šå¤šæ¬¡è¯»å–åŒä¸€ä¸ªæ–‡ä»¶å¯¹è±¡
    uploaded_file.seek(0)

    if uploaded_file.name.endswith('.csv'):
        return pd.read_csv(uploaded_file)
    else:
        return pd.read_excel(uploaded_file)


# --- [æ–°å¢] ç”Ÿæˆç‹¬ç«‹è®­ç»ƒè„šæœ¬çš„å‡½æ•° ---
def generate_training_script_code(model_name, params, feature_cols, target_col):
    """ç”Ÿæˆç‹¬ç«‹çš„ Python è®­ç»ƒè„šæœ¬"""
    script_template = f'''# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨ç”Ÿæˆçš„æœºå™¨å­¦ä¹ è®­ç»ƒè„šæœ¬
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
æ¨¡å‹ç±»å‹: {model_name}
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
try: from xgboost import XGBRegressor
except ImportError: pass
try: from lightgbm import LGBMRegressor
except ImportError: pass
try: from catboost import CatBoostRegressor
except ImportError: pass



# === TensorFlow (for TFS model) ===
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers, callbacks, regularizers
    tf.get_logger().setLevel('ERROR')
    TENSORFLOW_AVAILABLE = True
except Exception:
    TENSORFLOW_AVAILABLE = False
    tf = None
    keras = None
    layers = None
    callbacks = None
    regularizers = None


def build_tfs_model(input_dim, params):
    # Parse hidden layers like "128,64,32"
    hidden_layers_str = str(params.get('hidden_layers', '128,64,32'))
    try:
        hidden = [int(x.strip()) for x in hidden_layers_str.split(',') if x.strip()]
    except Exception:
        hidden = [128, 64, 32]

    activation = str(params.get('activation', 'relu'))
    dropout_rate = float(params.get('dropout_rate', 0.2))
    l2_reg = float(params.get('l2_reg', 0.001))
    learning_rate = float(params.get('learning_rate', 0.001))
    opt_name = str(params.get('optimizer', 'adam')).lower()

    if TENSORFLOW_AVAILABLE:
        try:
            tf.random.set_seed(42)
        except Exception:
            pass

    model = keras.Sequential(name='TFS_Regressor')
    model.add(layers.Input(shape=(int(input_dim),)))

    reg = None
    try:
        if l2_reg and l2_reg > 0:
            reg = regularizers.l2(l2_reg)
    except Exception:
        reg = None

    for units in hidden:
        units = int(units)
        if activation == 'leaky_relu':
            model.add(layers.Dense(units, kernel_regularizer=reg))
            model.add(layers.LeakyReLU())
        else:
            model.add(layers.Dense(units, activation=activation, kernel_regularizer=reg))
        if dropout_rate and dropout_rate > 0:
            model.add(layers.Dropout(dropout_rate))

    model.add(layers.Dense(1))

    # Optimizer
    if opt_name == 'adamw' and hasattr(keras.optimizers, 'AdamW'):
        opt = keras.optimizers.AdamW(learning_rate=learning_rate)
    elif opt_name == 'sgd':
        opt = keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)
    elif opt_name == 'rmsprop':
        opt = keras.optimizers.RMSprop(learning_rate=learning_rate)
    else:
        opt = keras.optimizers.Adam(learning_rate=learning_rate)

    model.compile(optimizer=opt, loss='mse', metrics=['mae'])
    return model
MODEL_NAME = "{model_name}"
FEATURE_COLS = {json.dumps(feature_cols, ensure_ascii=False)}
TARGET_COL = "{target_col}"
HYPERPARAMETERS = {json.dumps(params, indent=4, ensure_ascii=False)}
DATA_PATH = "data.csv" 

def load_and_train():
    print(f"æ­£åœ¨åŠ è½½æ•°æ®: {{DATA_PATH}}...")
    try:
        if DATA_PATH.endswith('.csv'): df = pd.read_csv(DATA_PATH)
        else: df = pd.read_excel(DATA_PATH)
    except FileNotFoundError:
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶")
        return

    X = df[FEATURE_COLS].values
    y = df[TARGET_COL].values
    y = pd.to_numeric(y, errors='coerce')
    mask = ~np.isnan(y)
    X, y = X[mask], y[mask]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    imputer = SimpleImputer(strategy='median')
    X_train = imputer.fit_transform(X_train)
    X_test = imputer.transform(X_test)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    print(f"æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹: {{MODEL_NAME}}...")
    model = None
    if MODEL_NAME == "éšæœºæ£®æ—": model = RandomForestRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "XGBoost": model = XGBRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "LightGBM": model = LGBMRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "CatBoost": model = CatBoostRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "SVR": model = SVR(**HYPERPARAMETERS)
    elif MODEL_NAME == "å†³ç­–æ ‘": model = DecisionTreeRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "æ¢¯åº¦æå‡æ ‘": model = GradientBoostingRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "AdaBoost": model = AdaBoostRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "å¤šå±‚æ„ŸçŸ¥å™¨": model = MLPRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "çº¿æ€§å›å½’": model = LinearRegression(**HYPERPARAMETERS)
    elif MODEL_NAME == "Ridgeå›å½’": model = Ridge(**HYPERPARAMETERS)
    elif MODEL_NAME == "Lassoå›å½’": model = Lasso(**HYPERPARAMETERS)
    elif MODEL_NAME == "TensorFlow Sequential":
        if not TENSORFLOW_AVAILABLE:
            print("âŒ é”™è¯¯: TensorFlow æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥ï¼Œæ— æ³•è®­ç»ƒ TFSã€‚è¯·å…ˆå®‰è£…: pip install tensorflow")
            return
        print("å¼€å§‹è®­ç»ƒ (TFS)...")
        model = build_tfs_model(X_train.shape[1], HYPERPARAMETERS)
        cbs = []
        if bool(HYPERPARAMETERS.get('early_stopping', True)):
            try:
                patience = int(HYPERPARAMETERS.get('patience', 20))
            except Exception:
                patience = 20
            cbs.append(callbacks.EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True))

        model.fit(
            X_train, y_train,
            epochs=int(HYPERPARAMETERS.get('epochs', 200)),
            batch_size=int(HYPERPARAMETERS.get('batch_size', 32)),
            validation_split=float(HYPERPARAMETERS.get('validation_split', 0.1)),
            verbose=1,
            callbacks=cbs
        )
        y_pred = model.predict(X_test).ravel()
        r2 = r2_score(y_test, y_pred)
        print('è®­ç»ƒå®Œæˆï¼RÂ² Score: %.4f' % r2)
        return

    if model:
        print("å¼€å§‹è®­ç»ƒ...")
        model.fit(X_train, y_train)
        print("æ­£åœ¨è¯„ä¼°...")
        y_pred = model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        print(f"è®­ç»ƒå®Œæˆï¼RÂ² Score: {{r2:.4f}}")

if __name__ == "__main__":
    load_and_train()
'''
    return script_template


# --- å…¨å±€å¸¸é‡ ---
USER_DATA_DB = "datasets/user_data.csv"

# --- è‡ªå®šä¹‰ CSS æ ·å¼ ---
# --- è‡ªå®šä¹‰ CSS æ ·å¼ (å«å›¾ç‰‡é˜²æŠ–) ---
CUSTOM_CSS = """
<style>
    :root { --primary-color: #4F46E5; }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 12px; padding: 20px; color: white; text-align: center;
        margin: 8px 0;
    }
    /* å›¾ç‰‡å®¹å™¨é«˜åº¦å›ºå®šï¼Œé˜²æ­¢é¡µé¢æŠ–åŠ¨ */
    div[data-testid="stImage"] { min-height: 400px; display: flex; align-items: center; justify-content: center; }
    .stPlotlyChart { min-height: 400px; }
</style>
"""


st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


# ============================================================
# Session State åˆå§‹åŒ–
# ============================================================
def init_session_state():
    """åˆå§‹åŒ–æ‰€æœ‰session stateå˜é‡"""
    defaults = {
        'data': None,
        'processed_data': None,
        'molecular_features': None,
        'target_col': None,
        'feature_cols': [],
        'model': None,
        'model_name': None,
        'train_result': None,
        'cv_result': None,
        'scaler': None,
        'imputer': None,
        'pipeline': None,
        'X_train': None,
        'X_test': None,
        'y_train': None,
        'y_test': None,
        'optimization_history': [],
        'best_params': None,
        'molecular_feature_names': [],
        'optimized_model_name': None,  # æ–°å¢ï¼šè®°å½•ä¼˜åŒ–çš„æ¨¡å‹å

        # --- [æ–°å¢] Active Learning ---
        'al_pool_data': None,
        'al_recommendations': None,
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


init_session_state()

if 'fe_tracker' not in st.session_state:
    st.session_state.fe_tracker = FeatureEngineeringTracker()
tracker = st.session_state.fe_tracker


def log_fe_step(operation: str, description: str, params=None, input_df=None, output_df=None,
                features_added=None, features_removed=None, status: str = "success", message: str = ""):
    """è®°å½•ç‰¹å¾å·¥ç¨‹/å»ºæ¨¡å…³é”®æ­¥éª¤åˆ°çŠ¶æ€æ¡ï¼ˆä¸ä¼šå½±å“ä¸»æµç¨‹ï¼‰ã€‚"""
    tr = st.session_state.get("fe_tracker", None)
    if tr is None:
        return
    try:
        tr.log_step(
            operation=operation,
            description=description,
            params=params or {},
            input_df=input_df,
            output_df=output_df,
            features_added=features_added or [],
            features_removed=features_removed or [],
            status=status,
            message=message
        )
    except Exception:
        # æ—¥å¿—è®°å½•å¤±è´¥ä¸åº”å½±å“ä¸»æµç¨‹
        pass


# ============================================================
# ä¾§è¾¹æ æ¸²æŸ“
# ============================================================
def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ å¯¼èˆª"""
    with st.sidebar:
        st.title(f"ğŸ”¬ {APP_NAME}")
        st.caption(f"ç‰ˆæœ¬ {VERSION}")
        st.markdown("---")

        page = st.radio(
            "ğŸ“Œ åŠŸèƒ½å¯¼èˆª",
            [
                "ğŸ  é¦–é¡µ",
                "ğŸ“¤ æ•°æ®ä¸Šä¼ ",
                "ğŸ” æ•°æ®æ¢ç´¢",
                "ğŸ§¹ æ•°æ®æ¸…æ´—",
                "âœ¨ æ•°æ®å¢å¼º",
                "ğŸ§¬ åˆ†å­ç‰¹å¾",
<<<<<<< HEAD
                "ğŸ–¼ï¸ å›¾åƒè½¬SMILES",
=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4
                "ğŸ¯ ç‰¹å¾é€‰æ‹©",
                "ğŸ¤– æ¨¡å‹è®­ç»ƒ",
                "ğŸ“ˆ è®­ç»ƒè®°å½•",
                "ğŸ“Š æ¨¡å‹è§£é‡Š",
                "ğŸ”® é¢„æµ‹åº”ç”¨",
                "âš™ï¸ è¶…å‚ä¼˜åŒ–",
                "ğŸ§  ä¸»åŠ¨å­¦ä¹ ",
                "ğŸ“‹ çŠ¶æ€æ¡è®°å½•",
            ],
            label_visibility="collapsed"
        )

        st.markdown("---")
        st.markdown("### ğŸ“Š æ•°æ®çŠ¶æ€")

        # ä¼˜å…ˆè·å– processed_data (æ¸…æ´—/å¤„ç†åçš„æ•°æ®)
        current_df = st.session_state.get('processed_data')
        original_df = st.session_state.get('data')

        # ç¡®å®šè¦æ˜¾ç¤ºå“ªä¸ªæ•°æ®çš„ä¿¡æ¯
        display_df = current_df if current_df is not None else original_df

        if display_df is not None:
            # 1. æ˜¾ç¤ºè¡Œ/åˆ—æ•°
            status_label = "âœ… å½“å‰æ•°æ® (å·²æ¸…æ´—)" if current_df is not None else "âœ… åŸå§‹æ•°æ®"
            st.success(f"{status_label}\n\n**{display_df.shape[0]} è¡Œ Ã— {display_df.shape[1]} åˆ—**")

            # 2. æ˜¾ç¤ºåˆ†å­ç‰¹å¾çŠ¶æ€
            if st.session_state.get('molecular_features') is not None:
                mf = st.session_state.molecular_features
                st.info(f"ğŸ§¬ åˆ†å­ç‰¹å¾: {mf.shape[1]} ä¸ª")

            # 3. æ˜¾ç¤ºç‰¹å¾é€‰æ‹©çŠ¶æ€
            feature_cols = st.session_state.get('feature_cols')
            target_col = st.session_state.get('target_col')

            if feature_cols:
                st.info(f"ğŸ¯ å·²é€‰ç‰¹å¾ (X): {len(feature_cols)} ä¸ª")

            if target_col:
                st.caption(f"ğŸ¯ ç›®æ ‡å˜é‡ (Y): {target_col}")

        else:
            st.warning("âš ï¸ æœªåŠ è½½æ•°æ®")

        if st.session_state.model is not None:
            st.success(f"ğŸ¤– å·²è®­ç»ƒ: {st.session_state.model_name}")
            # å¦‚æœæœ‰è®­ç»ƒç»“æœï¼Œä¹Ÿå¯ä»¥æ˜¾ç¤ºR2
            if st.session_state.get('train_result'):
                r2 = st.session_state.train_result.get('r2', 0)
                st.caption(f"å½“å‰ RÂ²: {r2:.4f}")

        st.markdown("---")
        st.markdown("### ğŸ”§ ç³»ç»Ÿä¿¡æ¯")
        st.caption(f"CPUæ ¸å¿ƒ: {mp.cpu_count()}")
        if PSUTIL_AVAILABLE:
            mem = psutil.virtual_memory()
            st.caption(f"å†…å­˜ä½¿ç”¨: {mem.percent}%")

        # --- [æ–°å¢] ä¾èµ–æ£€æµ‹ï¼ˆè®© TFS æ¨¡å‹å…¥å£æ›´â€œå¯è§â€ï¼‰ ---
        st.markdown("### ğŸ§© ä¾èµ–æ£€æµ‹")
        tf_ver = None
        try:
            tf_ver = get_tensorflow_version()
        except Exception:
            tf_ver = None

        if bool(TENSORFLOW_AVAILABLE) and tf_ver:
            st.success(f"TensorFlow: {tf_ver}")
        else:
            st.caption("TensorFlow: æœªå®‰è£…æˆ–ä¸å¯ç”¨")
            try:
                err = globals().get('TENSORFLOW_IMPORT_ERROR', '')
                if err:
                    st.caption(f"TF å¯¼å…¥ä¿¡æ¯: {str(err)[:160]}")
            except Exception:
                pass

        st.caption(f"TorchANI: {'å¯ç”¨' if TORCHANI_AVAILABLE else 'ä¸å¯ç”¨'}")

        # [å¢å¼º] åœ¨ä¾§è¾¹æ å§‹ç»ˆæ˜¾ç¤ºçŠ¶æ€æ¡å…¥å£ï¼ˆå³ä½¿æš‚æ— è®°å½•ï¼Œä¹Ÿé¿å…â€œåŠŸèƒ½å­˜åœ¨ä½†ç•Œé¢ä¸æ˜¾ç¤ºâ€ï¼‰
        render_status_sidebar(st.session_state.get('fe_tracker', None))
        return page


def render_top_status_bar():
    """ä¸»åŒºåŸŸé¡¶éƒ¨çš„è½»é‡çŠ¶æ€æ¡ï¼ˆé˜²æ­¢ç”¨æˆ·æŠ˜å ä¾§è¾¹æ åæ‰¾ä¸åˆ°çŠ¶æ€æ¡/TFä¿¡æ¯ï¼‰ã€‚"""
    tr = st.session_state.get('fe_tracker', None)
    if tr is None:
        return

    try:
        stats = tr.get_stats()
        last = tr.get_last_step()
    except Exception:
        stats = {'success': 0, 'warning': 0, 'error': 0}
        last = None

    # TensorFlow çŠ¶æ€
    tf_ver = None
    try:
        tf_ver = get_tensorflow_version()
    except Exception:
        tf_ver = None
    tf_status = "âœ…" if (bool(TENSORFLOW_AVAILABLE) and tf_ver) else "â›”"
    tf_text = f"TensorFlow {tf_status} {tf_ver}" if (bool(TENSORFLOW_AVAILABLE) and tf_ver) else "TensorFlow â›” æœªå®‰è£…/ä¸å¯ç”¨"

    with st.expander("ğŸ“‹ çŠ¶æ€æ¡ï¼ˆå¿«æ·ï¼‰", expanded=False):
        c1, c2, c3 = st.columns([1.2, 1.2, 3.6])
        with c1:
            st.caption("è®°å½•ç»Ÿè®¡")
            st.write(f"âœ… {int(stats.get('success', 0))}  Â·  âš ï¸ {int(stats.get('warning', 0))}  Â·  âŒ {int(stats.get('error', 0))}")
        with c2:
            st.caption("TFS ä¾èµ–")
            st.write(tf_text)
        with c3:
            st.caption("æœ€è¿‘ä¸€æ¬¡")
            if last:
                icon = {"success": "âœ…", "warning": "âš ï¸", "error": "âŒ"}.get(last.get('status', 'success'), "â„¹ï¸")
                st.write(f"{icon} [{last.get('timestamp','')}] {last.get('operation','')} - {last.get('description','')}")
                if last.get('message'):
                    st.caption(last.get('message'))
            else:
                st.write("æš‚æ— è®°å½•ã€‚å®Œæˆä¸€æ¬¡æ•°æ®ä¸Šä¼ /æ¸…æ´—/ç‰¹å¾é€‰æ‹©/è®­ç»ƒåä¼šè‡ªåŠ¨å‡ºç°ã€‚")
        st.caption("æç¤ºï¼šæ›´å®Œæ•´çš„æ—¶é—´çº¿ä¸å¯¼å‡ºåœ¨å·¦ä¾§å¯¼èˆªã€ŒğŸ“‹ çŠ¶æ€æ¡è®°å½•ã€ã€‚")


# ============================================================
# é¡µé¢ï¼šé¦–é¡µ
# ============================================================
def page_home():
    """é¦–é¡µ"""
    st.title("ğŸ”¬ ç¢³çº¤ç»´å¤åˆææ–™æ™ºèƒ½é¢„æµ‹å¹³å°")
    st.markdown(f"**ç‰ˆæœ¬ {VERSION}** ")

    st.markdown("---")

    # åŠŸèƒ½å¡ç‰‡
    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("""
        <div class="metric-card">
            <div class="metric-label">æ•°æ®å¤„ç†</div>
            <div class="metric-value">ğŸ“Š</div>
            <p>æ™ºèƒ½æ¸…æ´— Â· VAEå¢å¼º Â· ç±»åˆ«å¹³è¡¡</p>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown("""
        <div class="metric-card metric-card-success">
            <div class="metric-label">åˆ†å­ç‰¹å¾</div>
            <div class="metric-value">ğŸ§¬</div>
            <p>RDKit Â· æŒ‡çº¹(MACCS) Â· å›¾ç‰¹å¾</p>
        </div>
        """, unsafe_allow_html=True)

    with col3:
        st.markdown("""
        <div class="metric-card metric-card-warning">
            <div class="metric-label">æ¨¡å‹è®­ç»ƒ</div>
            <div class="metric-value">ğŸ¤–</div>
            <p>15+æ¨¡å‹ Â· æ‰‹åŠ¨è°ƒå‚ Â· Optunaä¼˜åŒ–</p>
        </div>
        """, unsafe_allow_html=True)

    st.markdown("---")

    # æ ¸å¿ƒåŠŸèƒ½ä»‹ç»
    st.markdown("## ğŸš€ æ ¸å¿ƒåŠŸèƒ½")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        ### ğŸ“Š æ•°æ®å¤„ç†
        - **æ™ºèƒ½æ•°æ®æ¸…æ´—**: ç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼æ£€æµ‹ã€æ•°æ®ç±»å‹ä¿®å¤
        - **VAEæ•°æ®å¢å¼º**: åŸºäºå˜åˆ†è‡ªç¼–ç å™¨çš„è¡¨æ ¼æ•°æ®ç”Ÿæˆ
        - **ç±»åˆ«å¹³è¡¡**: è§£å†³åŒ–å­¦å•ä½“æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜

        ### ğŸ§¬ åˆ†å­ç‰¹å¾æå–
        - **åˆ†å­æŒ‡çº¹**: MACCS Keys, Morgan (ECFP) æŒ‡çº¹
        - **RDKitæ ‡å‡†ç‰ˆ**: 200+åˆ†å­æè¿°ç¬¦
        - **å›¾ç¥ç»ç½‘ç»œç‰¹å¾**: åˆ†å­æ‹“æ‰‘ç»“æ„ç‰¹å¾
        - **MLåŠ›åœºç‰¹å¾**: ANI-2x é«˜ç²¾åº¦èƒ½é‡/åŠ›
        """)

    with col2:
        st.markdown("""
        ### ğŸ¤– æ¨¡å‹è®­ç»ƒ
        - **é›†æˆæ¨¡å‹**: éšæœºæ£®æ—ã€XGBoostã€LightGBMã€CatBoost
        - **AutoML**: AutoGluon è‡ªåŠ¨å»ºæ¨¡
        - **æ‰‹åŠ¨è°ƒå‚**: å¯è§†åŒ–å‚æ•°é…ç½®ç•Œé¢

        ### ğŸ“Š æ¨¡å‹è§£é‡Š
        - **SHAPåˆ†æ**: ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
        - **å­¦ä¹ æ›²çº¿**: æ¨¡å‹æ”¶æ•›åˆ†æ
        - **é€‚ç”¨åŸŸåˆ†æ**: PCAå‡¸åŒ…è¾¹ç•Œæ£€æµ‹
        """)

    st.markdown("---")

    # å¿«é€Ÿå¼€å§‹
    st.markdown("## âš¡ å¿«é€Ÿå¼€å§‹")
    st.info("""
    1. **ä¸Šä¼ æ•°æ®** â†’ æ”¯æŒCSVã€Excelæ ¼å¼
    2. **æ•°æ®æ¸…æ´—** â†’ ä½¿ç”¨â€œç±»åˆ«å¹³è¡¡â€å¤„ç†é«˜é¢‘å•ä½“
    3. **åˆ†å­ç‰¹å¾** â†’ æå–SMILESæŒ‡çº¹æˆ–æè¿°ç¬¦
    4. **ç‰¹å¾é€‰æ‹©** â†’ é€‰æ‹©ç›®æ ‡å˜é‡å’Œè¾“å…¥ç‰¹å¾
    5. **æ¨¡å‹è®­ç»ƒ** â†’ é€‰æ‹©æ¨¡å‹å¹¶è°ƒæ•´å‚æ•°
    6. **æ¨¡å‹è§£é‡Š** â†’ SHAPåˆ†æå’Œæ€§èƒ½è¯„ä¼°
    """)


# ============================================================
# é¡µé¢ï¼šæ•°æ®ä¸Šä¼ 
# ============================================================
def page_data_upload():
    """æ•°æ®ä¸Šä¼ é¡µé¢"""
    st.title("ğŸ“¤ æ•°æ®ä¸Šä¼ ")

    tab1, tab2 = st.tabs(["ğŸ“ ä¸Šä¼ æ–‡ä»¶", "ğŸ“ ç”Ÿæˆç¤ºä¾‹æ•°æ®"])

    with tab1:
        uploaded_file = st.file_uploader(
            "é€‰æ‹©æ•°æ®æ–‡ä»¶",
            type=['csv', 'xlsx', 'xls'],
            help="æ”¯æŒCSVå’ŒExcelæ ¼å¼"
        )

        if uploaded_file is not None:
            try:
                # ä½¿ç”¨ç¼“å­˜å‡½æ•°åŠ è½½æ•°æ®
                df = load_data_file(uploaded_file)

                # å»é‡ååˆ—
                if df.columns.duplicated().any():
                    st.warning("âš ï¸ æ£€æµ‹åˆ°é‡ååˆ—ï¼Œç³»ç»Ÿå·²è‡ªåŠ¨é‡å‘½åå¤„ç†")
                    df = df.loc[:, ~df.columns.duplicated()]

                st.session_state.data = df
                st.session_state.processed_data = df.copy()

                st.success(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—")

                # [æ–°å¢] è®°å½•åˆ°çŠ¶æ€æ¡
                log_fe_step(
                    operation="æ•°æ®ä¸Šä¼ ",
                    description=f"åŠ è½½æ–‡ä»¶ï¼š{uploaded_file.name}",
                    params={"rows": int(df.shape[0]), "cols": int(df.shape[1]), "type": "csv" if uploaded_file.name.endswith('.csv') else "excel"},
                    output_df=df,
                    message=f"æ•°æ®ç»´åº¦ï¼š{df.shape[0]}Ã—{df.shape[1]}"
                )

                # æ•°æ®é¢„è§ˆ
                st.markdown("### ğŸ“‹ æ•°æ®é¢„è§ˆ")
                st.dataframe(df.head(10), use_container_width=True)

                # åˆ—ä¿¡æ¯
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("#### æ•°å€¼åˆ—")
                    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
                    for col in numeric_cols[:10]:
                        st.markdown(f"<span class='feature-badge'>{col}</span>", unsafe_allow_html=True)
                    if len(numeric_cols) > 10:
                        st.caption(f"... ç­‰å…± {len(numeric_cols)} ä¸ªæ•°å€¼åˆ—")

                with col2:
                    st.markdown("#### æ–‡æœ¬åˆ—")
                    text_cols = df.select_dtypes(include=['object']).columns.tolist()
                    for col in text_cols[:10]:
                        st.markdown(f"<span class='feature-badge'>{col}</span>", unsafe_allow_html=True)
                    if len(text_cols) > 10:
                        st.caption(f"... ç­‰å…± {len(text_cols)} ä¸ªæ–‡æœ¬åˆ—")

            except Exception as e:
                st.error(f"âŒ åŠ è½½å¤±è´¥: {str(e)}")

    with tab2:
        st.markdown("### ç”Ÿæˆç¤ºä¾‹æ•°æ®é›†")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("#### ğŸ§ª æ··åˆæ•°æ®é›†")
            st.caption("åŒ…å«å·¥è‰ºå‚æ•°å’ŒSMILESåˆ†å­ç»“æ„")
            n_samples_hybrid = st.number_input("æ ·æœ¬æ•°é‡", 100, 2000, 500, key="n_hybrid")
            if st.button("ç”Ÿæˆæ··åˆæ•°æ®é›†", type="primary"):
                df = generate_hybrid_dataset(n_samples=n_samples_hybrid)
                st.session_state.data = df
                st.session_state.processed_data = df.copy()
                st.success(f"âœ… å·²ç”Ÿæˆæ··åˆæ•°æ®é›†: {df.shape}")
                log_fe_step(
                    operation="æ•°æ®ç”Ÿæˆ",
                    description="ç”Ÿæˆæ··åˆç¤ºä¾‹æ•°æ®é›†",
                    params={"n_samples": int(n_samples_hybrid), "type": "hybrid"},
                    output_df=df,
                    message=f"æ•°æ®ç»´åº¦ï¼š{df.shape[0]}Ã—{df.shape[1]}"
                )
                st.dataframe(df.head(), use_container_width=True)

        with col2:
            st.markdown("#### ğŸ“Š çº¯æ•°å€¼æ•°æ®é›†")
            st.caption("ä»…åŒ…å«æ•°å€¼å‹å·¥è‰ºå‚æ•°")
            n_samples_numeric = st.number_input("æ ·æœ¬æ•°é‡", 100, 2000, 500, key="n_numeric")
            if st.button("ç”Ÿæˆçº¯æ•°å€¼æ•°æ®é›†", type="primary"):
                df = generate_pure_numeric_dataset(n_samples=n_samples_numeric)
                st.session_state.data = df
                st.session_state.processed_data = df.copy()
                st.success(f"âœ… å·²ç”Ÿæˆçº¯æ•°å€¼æ•°æ®é›†: {df.shape}")
                log_fe_step(
                    operation="æ•°æ®ç”Ÿæˆ",
                    description="ç”Ÿæˆçº¯æ•°å€¼ç¤ºä¾‹æ•°æ®é›†",
                    params={"n_samples": int(n_samples_numeric), "type": "numeric"},
                    output_df=df,
                    message=f"æ•°æ®ç»´åº¦ï¼š{df.shape[0]}Ã—{df.shape[1]}"
                )
                st.dataframe(df.head(), use_container_width=True)


# ============================================================
# é¡µé¢ï¼šæ•°æ®æ¢ç´¢
# ============================================================
def page_data_explore():
    """æ•°æ®æ¢ç´¢é¡µé¢"""
    st.title("ğŸ” æ•°æ®æ¢ç´¢")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    explorer = EnhancedDataExplorer(df)

    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“Š æè¿°ç»Ÿè®¡", "ğŸ”— ç›¸å…³æ€§åˆ†æ", "ğŸ“ˆ åˆ†å¸ƒå›¾", "â“ ç¼ºå¤±å€¼", "ğŸ’¾ å¯¼å‡º"
    ])

    with tab1:
        stats = explorer.generate_summary_stats()

        col1, col2, col3, col4 = st.columns(4)
        col1.metric("æ€»è¡Œæ•°", stats['basic_info']['total_rows'])
        col2.metric("æ€»åˆ—æ•°", stats['basic_info']['total_columns'])
        col3.metric("æ•°å€¼åˆ—", stats['basic_info']['numeric_columns'])
        col4.metric("ç¼ºå¤±å€¼", stats['basic_info']['missing_values'])

        st.markdown("### æ•°å€¼ç‰¹å¾ç»Ÿè®¡")
        if explorer.numeric_cols:
            st.dataframe(df[explorer.numeric_cols].describe(), use_container_width=True)

    with tab2:
        st.markdown("### ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ")

        numeric_cols = explorer.numeric_cols
        if not numeric_cols or len(numeric_cols) < 2:
            st.info("éœ€è¦è‡³å°‘2ä¸ªæ•°å€¼åˆ—")
        else:
            # --- è‡ªå®šä¹‰çƒ­å›¾ç‰¹å¾å­é›† ---
            col_a, col_b = st.columns([2, 1])
            with col_a:
                heatmap_mode = st.radio(
                    "çƒ­å›¾ç‰¹å¾æ¥æº",
                    ["å…¨éƒ¨æ•°å€¼ç‰¹å¾", "è‡ªå®šä¹‰å¤šé€‰", "ä½¿ç”¨ã€ç‰¹å¾é€‰æ‹©ã€‘é¡µå½“å‰å­é›†", "æŒ‰ç›®æ ‡ç›¸å…³æ€§Top-K"],
                    horizontal=True,
                    key="corr_heatmap_mode"
                )
            with col_b:
                max_show = st.number_input(
                    "æœ€å¤šæ˜¾ç¤ºç‰¹å¾æ•°",
                    min_value=2,
                    max_value=min(80, len(numeric_cols)),
                    value=min(25, len(numeric_cols)),
                    key="corr_heatmap_max"
                )

            target = st.session_state.get("target_col")
            include_target = st.checkbox("åŒ…å«ç›®æ ‡å˜é‡ï¼ˆè‹¥ä¸ºæ•°å€¼åˆ—ï¼‰", value=True, key="corr_heatmap_include_target")

            selected_cols = None

            if heatmap_mode == "å…¨éƒ¨æ•°å€¼ç‰¹å¾":
                selected_cols = numeric_cols.copy()

            elif heatmap_mode == "ä½¿ç”¨ã€ç‰¹å¾é€‰æ‹©ã€‘é¡µå½“å‰å­é›†":
                selected_cols = [c for c in st.session_state.get("feature_cols", []) if c in numeric_cols]
                if not selected_cols:
                    st.info("å½“å‰å°šæœªåœ¨ã€ç‰¹å¾é€‰æ‹©ã€‘é¡µé€‰å®šç‰¹å¾å­é›†ï¼Œå°†å›é€€ä¸ºâ€œè‡ªå®šä¹‰å¤šé€‰â€ã€‚")
                    heatmap_mode = "è‡ªå®šä¹‰å¤šé€‰"

            if heatmap_mode == "è‡ªå®šä¹‰å¤šé€‰":
                default_cols = numeric_cols[:min(25, len(numeric_cols))]
                selected_cols = st.multiselect(
                    "é€‰æ‹©ç”¨äºçƒ­å›¾çš„æ•°å€¼ç‰¹å¾",
                    options=numeric_cols,
                    default=default_cols,
                    key="corr_heatmap_cols"
                )

            elif heatmap_mode == "æŒ‰ç›®æ ‡ç›¸å…³æ€§Top-K":
                k = st.number_input(
                    "Top-Kï¼ˆæŒ‰ |corr| æ’åºï¼‰",
                    min_value=2,
                    max_value=len(numeric_cols),
                    value=min(20, len(numeric_cols)),
                    key="corr_heatmap_topk"
                )
                if target in numeric_cols:
                    corrs = df[numeric_cols].corrwith(df[target]).abs().sort_values(ascending=False)
                    selected_cols = corrs.head(int(k)).index.tolist()
                else:
                    st.info("ç›®æ ‡å˜é‡ä¸æ˜¯æ•°å€¼åˆ—ï¼Œæ— æ³•æŒ‰ç›¸å…³æ€§æ’åºï¼›å·²ä½¿ç”¨å‰Kä¸ªæ•°å€¼åˆ—ã€‚")
                    selected_cols = numeric_cols[:int(k)]

            # å¯é€‰ï¼šæŠŠç›®æ ‡å˜é‡ä¹Ÿæ”¾åˆ°çƒ­å›¾é‡Œ
            if include_target and (target in numeric_cols) and (target not in selected_cols):
                selected_cols = selected_cols + [target]

            # é™åˆ¶æ˜¾ç¤ºæ•°é‡ï¼Œé¿å…çƒ­å›¾è¿‡å¤§
            if len(selected_cols) > int(max_show):
                st.warning(f"å·²é€‰æ‹© {len(selected_cols)} ä¸ªç‰¹å¾ï¼Œä¸ºå¯è¯»æ€§ä»…æ˜¾ç¤ºå‰ {int(max_show)} ä¸ªã€‚")
                selected_cols = selected_cols[:int(max_show)]

            fig = explorer.plot_correlation_matrix(cols=selected_cols)
            if fig:
                st.plotly_chart(fig, use_container_width=True)

            # é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹ï¼ˆåŸºäºå½“å‰çƒ­å›¾åˆ—ï¼‰
            pairs = explorer.get_high_correlation_pairs(cols=selected_cols, threshold=0.8)
            if pairs:
                st.markdown("### âš ï¸ é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹ (|r| > 0.8)")
                for p in pairs[:10]:
                    st.write(f"- **{p['feature1']}** â†” **{p['feature2']}**: {p['correlation']:.3f}")

    with tab3:
        st.markdown("### æ•°å€¼ç‰¹å¾åˆ†å¸ƒ")
        fig = explorer.plot_distributions()
        if fig:
            st.plotly_chart(fig, use_container_width=True)

        st.markdown("### ç®±çº¿å›¾")
        fig_box = explorer.plot_boxplots()
        if fig_box:
            st.plotly_chart(fig_box, use_container_width=True)

    with tab4:
        st.markdown("### ç¼ºå¤±å€¼åˆ†æ")
        fig_missing = explorer.plot_missing_values()
        if fig_missing:
            st.plotly_chart(fig_missing, use_container_width=True)
        else:
            st.success("âœ… æ•°æ®æ— ç¼ºå¤±å€¼")

    with tab5:
        st.markdown("### å¯¼å‡ºæ•°æ®")
        col1, col2 = st.columns(2)

        with col1:
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                "ğŸ“¥ ä¸‹è½½CSV",
                csv,
                "data_export.csv",
                "text/csv"
            )

        with col2:
            buffer = io.BytesIO()
            df.to_excel(buffer, index=False)
            st.download_button(
                "ğŸ“¥ ä¸‹è½½Excel",
                buffer.getvalue(),
                "data_export.xlsx",
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )


# ============================================================
# é¡µé¢ï¼šæ•°æ®æ¸…æ´—
# ============================================================
def page_data_cleaning():
    """æ•°æ®æ¸…æ´—é¡µé¢"""
    st.title("ğŸ§¹ æ•°æ®æ¸…æ´—")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    cleaner = AdvancedDataCleaner(df)

    tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8 = st.tabs([
        "â“ ç¼ºå¤±å€¼å¤„ç†", "ğŸ“Š å¼‚å¸¸å€¼æ£€æµ‹", "ğŸ”„ é‡å¤æ•°æ®", "ğŸ”§ æ•°æ®ç±»å‹", "ğŸ§ª SMILESæ¸…æ´—", "ğŸ§© SMILESç»„åˆ†åˆ†åˆ—", "âš–ï¸ ç±»åˆ«å¹³è¡¡",
        "ğŸ§© K-Meansèšç±»"])

    with tab1:
        st.markdown("### ç¼ºå¤±å€¼å¤„ç†")

        # ç¼ºå¤±å€¼ç»Ÿè®¡
        missing = df.isnull().sum()
        missing = missing[missing > 0]

        if len(missing) > 0:
            st.warning(f"æ£€æµ‹åˆ° {len(missing)} åˆ—å­˜åœ¨ç¼ºå¤±å€¼")

            col1, col2 = st.columns(2)
            with col1:
                st.dataframe(pd.DataFrame({
                    'åˆ—å': missing.index,
                    'ç¼ºå¤±æ•°é‡': missing.values,
                    'ç¼ºå¤±æ¯”ä¾‹': (missing.values / len(df) * 100).round(2)
                }), use_container_width=True)

            with col2:
                strategy = st.selectbox(
                    "é€‰æ‹©å¡«å……ç­–ç•¥",
                    ["median", "mean", "mode", "knn", "drop_rows", "constant"]
                )

                fill_value = None
                if strategy == "constant":
                    fill_value = st.number_input("å¡«å……å¸¸æ•°å€¼", value=0.0)

                if st.button("ğŸ”§ æ‰§è¡Œç¼ºå¤±å€¼å¤„ç†", type="primary"):
                    # ä¸ºäº†é¿å…â€œä¼ªæ•°å€¼åˆ—(object)æ— æ³•å¡«å……â€å¯¼è‡´çœ‹èµ·æ¥æ²¡ç”Ÿæ•ˆï¼Œ
                    # è¿™é‡Œå…ˆè‡ªåŠ¨å°è¯•æŠŠå¯è½¬æ¢çš„åˆ—è½¬ä¸ºæ•°å€¼å‹å†åšç¼ºå¤±å€¼å¤„ç†ã€‚
                    df_in = df.copy()
                    try:
                        _tmp_cleaner = AdvancedDataCleaner(df_in)
                        df_in = _tmp_cleaner.fix_pseudo_numeric_columns()
                    except Exception:
                        pass

                    missing_before = int(df_in.isna().sum().sum())
                    rows_before = int(df_in.shape[0])

                    if strategy == "mode":
                        cleaned_df = df_in.copy()
                        # ä¼—æ•°å¡«å……ï¼šå¯¹æ‰€æœ‰åˆ—éƒ½ç”Ÿæ•ˆï¼ˆå«æ–‡æœ¬åˆ—ï¼‰
                        for _col in cleaned_df.columns:
                            if cleaned_df[_col].isna().any():
                                try:
                                    _mode = cleaned_df[_col].mode(dropna=True)
                                    if not _mode.empty:
                                        cleaned_df[_col] = cleaned_df[_col].fillna(_mode.iloc[0])
                                except Exception:
                                    # æŸäº›åˆ— mode è®¡ç®—å¯èƒ½å¤±è´¥ï¼Œè·³è¿‡å³å¯
                                    pass
                    elif strategy == "drop_rows":
                        cleaned_df = df_in.dropna().reset_index(drop=True)
                    else:
                        # å…¶ä½™ç­–ç•¥å…ˆèµ°åŸæ¸…æ´—å™¨ï¼ˆä¸»è¦é’ˆå¯¹æ•°å€¼åˆ—ï¼‰
                        _cleaner2 = AdvancedDataCleaner(df_in)
                        cleaned_df = _cleaner2.handle_missing_values(strategy=strategy, fill_value=fill_value)

                        # å¦‚æœè¿˜æœ‰éæ•°å€¼åˆ—ç¼ºå¤±ï¼Œç»™ä¸€ä¸ªæ¸©å’Œå›é€€ï¼šç”¨ä¼—æ•°è¡¥é½
                        # é¿å…ç”¨æˆ·çœ‹åˆ°â€œæŒ‰é’®ç‚¹äº†ä½†æ²¡å˜åŒ–â€
                        non_num_cols = cleaned_df.select_dtypes(exclude=np.number).columns.tolist()
                        for _col in non_num_cols:
                            if cleaned_df[_col].isna().any():
                                try:
                                    _mode = cleaned_df[_col].mode(dropna=True)
                                    if not _mode.empty:
                                        cleaned_df[_col] = cleaned_df[_col].fillna(_mode.iloc[0])
                                except Exception:
                                    if strategy == "constant":
                                        cleaned_df[_col] = cleaned_df[_col].fillna(fill_value if fill_value is not None else 0)

                    missing_after = int(cleaned_df.isna().sum().sum())
                    rows_after = int(cleaned_df.shape[0])

                    st.session_state.processed_data = cleaned_df

                    log_fe_step(
                        operation="ç¼ºå¤±å€¼å¤„ç†",
                        description=f"ç­–ç•¥: {strategy}",
                        params={"strategy": strategy, "fill_value": fill_value},
                        input_df=df_in,
                        output_df=cleaned_df,
                        message=f"ç¼ºå¤±å€¼: {missing_before} â†’ {missing_after}; è¡Œæ•°: {rows_before} â†’ {rows_after}"
                    )

                    if strategy == "drop_rows":
                        st.success(f"âœ… ç¼ºå¤±å€¼å¤„ç†å®Œæˆï¼šåˆ é™¤ {rows_before - rows_after} è¡Œï¼ˆ{rows_before} â†’ {rows_after}ï¼‰")
                    else:
                        st.success(f"âœ… ç¼ºå¤±å€¼å¤„ç†å®Œæˆï¼šç¼ºå¤±å€¼ {missing_before} â†’ {missing_after}")

                    st.rerun()
        else:
            st.success("âœ… æ•°æ®æ— ç¼ºå¤±å€¼")

    with tab2:
        st.markdown("### å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†")

        col1, col2 = st.columns(2)
        with col1:
            method = st.selectbox("æ£€æµ‹æ–¹æ³•", ["iqr", "zscore"])
            threshold = st.slider("é˜ˆå€¼", 1.0, 5.0, 1.5 if method == "iqr" else 3.0)

        with col2:
            handle_method = st.selectbox("å¤„ç†æ–¹æ³•", ["clip", "replace_median", "remove"])

        if st.button("ğŸ” æ£€æµ‹å¼‚å¸¸å€¼"):
            # åŒæ ·å…ˆå°è¯•æŠŠâ€œä¼ªæ•°å€¼åˆ—â€è½¬æ¢ä¸ºæ•°å€¼åˆ—ï¼Œé¿å…æ¼æ£€
            _tmp_cleaner = AdvancedDataCleaner(df.copy())
            try:
                _tmp_cleaner.fix_pseudo_numeric_columns()
            except Exception:
                pass

            outliers = _tmp_cleaner.detect_outliers(method=method, threshold=threshold)
            if outliers:
                st.warning(f"æ£€æµ‹åˆ° {len(outliers)} åˆ—å­˜åœ¨å¼‚å¸¸å€¼")
                st.json(outliers)
            else:
                st.success("âœ… æœªæ£€æµ‹åˆ°æ˜¾è‘—å¼‚å¸¸å€¼")

        if st.button("ğŸ”§ å¤„ç†å¼‚å¸¸å€¼", type="primary"):
            # åç«¯ cleaner.handle_outliers() ä»…æ”¯æŒ IQR + clip/replace_medianï¼Œ
            # ä¸ºäº†è®©å‰ç«¯çš„ remove / zscore é€‰é¡¹çœŸæ­£ç”Ÿæ•ˆï¼Œè¿™é‡Œåœ¨ app.py å†…åšå…¼å®¹å®ç°ã€‚
            df_in = df.copy()
            _tmp_cleaner = AdvancedDataCleaner(df_in)
            try:
                df_in = _tmp_cleaner.fix_pseudo_numeric_columns()
            except Exception:
                pass

            numeric_cols = df_in.select_dtypes(include=np.number).columns.tolist()
            if not numeric_cols:
                st.info("â„¹ï¸ æœªæ‰¾åˆ°å¯ç”¨äºå¼‚å¸¸å€¼å¤„ç†çš„æ•°å€¼åˆ—ã€‚")
                return

            any_outlier = pd.Series(False, index=df_in.index)
            total_affected = 0

            for col in numeric_cols:
                s = df_in[col]

                if method == "iqr":
                    q1 = s.quantile(0.25)
                    q3 = s.quantile(0.75)
                    iqr = q3 - q1
                    if pd.isna(iqr) or iqr == 0:
                        continue
                    lower = q1 - threshold * iqr
                    upper = q3 + threshold * iqr
                    mask = (s < lower) | (s > upper)
                else:
                    mean = s.mean(skipna=True)
                    std = s.std(skipna=True)
                    if pd.isna(std) or std == 0:
                        continue
                    z = (s - mean) / std
                    mask = z.abs() > threshold
                    lower = mean - threshold * std
                    upper = mean + threshold * std

                mask = mask.fillna(False)

                if handle_method == "remove":
                    any_outlier = any_outlier | mask
                elif handle_method == "clip":
                    df_in[col] = s.clip(lower, upper)
                    total_affected += int(mask.sum())
                elif handle_method == "replace_median":
                    median_val = s.median(skipna=True)
                    df_in.loc[mask, col] = median_val
                    total_affected += int(mask.sum())

            if handle_method == "remove":
                removed_rows = int(any_outlier.sum())
                cleaned_df = df_in.loc[~any_outlier].reset_index(drop=True)
                st.session_state.processed_data = cleaned_df
                log_fe_step(
                    operation="å¼‚å¸¸å€¼å¤„ç†",
                    description=f"æ–¹æ³•: {method}, å¤„ç†: {handle_method}",
                    params={"method": method, "threshold": float(threshold), "handle": handle_method},
                    input_df=df_in,
                    output_df=cleaned_df,
                    message=f"åˆ é™¤ {removed_rows} è¡Œ"
                )
                st.success(f"âœ… å¼‚å¸¸å€¼å¤„ç†å®Œæˆï¼šåˆ é™¤ {removed_rows} è¡Œï¼ˆ{len(df_in)} â†’ {len(cleaned_df)}ï¼‰")
            else:
                st.session_state.processed_data = df_in
                log_fe_step(
                    operation="å¼‚å¸¸å€¼å¤„ç†",
                    description=f"æ–¹æ³•: {method}, å¤„ç†: {handle_method}",
                    params={"method": method, "threshold": float(threshold), "handle": handle_method},
                    input_df=df,
                    output_df=df_in,
                    message=f"è°ƒæ•´ {total_affected} ä¸ªå¼‚å¸¸å€¼å•å…ƒæ ¼"
                )
                st.success(f"âœ… å¼‚å¸¸å€¼å¤„ç†å®Œæˆï¼šå·²è°ƒæ•´ {total_affected} ä¸ªå¼‚å¸¸å€¼å•å…ƒæ ¼")

            st.rerun()

    with tab3:
        st.markdown("### ğŸ”„ æ•°æ®å»é‡ä¸åˆ†å¸ƒä¼˜åŒ–")

        col_clean_1, col_clean_2 = st.columns(2)

        with col_clean_1:
            st.markdown("#### 1. è¡Œå»é‡")
            st.caption("åˆ é™¤å®Œå…¨é‡å¤çš„æ ·æœ¬è¡Œ")
            dup_count = df.duplicated().sum()
            st.metric("å®Œå…¨é‡å¤è¡Œæ•°", dup_count)

            if dup_count > 0:
                if st.button("ğŸ—‘ï¸ åˆ é™¤é‡å¤è¡Œ", type="primary"):
                    cleaned_df = cleaner.remove_duplicates()
                    st.session_state.processed_data = cleaned_df
                    log_fe_step(
                        operation="å»é‡",
                        description="åˆ é™¤å®Œå…¨é‡å¤è¡Œ",
                        params={"removed_rows": int(dup_count)},
                        input_df=df,
                        output_df=cleaned_df,
                        message=f"åˆ é™¤ {dup_count} è¡Œ"
                    )
                    st.success(f"âœ… å·²åˆ é™¤ {dup_count} è¡Œé‡å¤æ•°æ®")
                    st.rerun()
            else:
                st.info("âœ… æ— é‡å¤è¡Œ")

        st.markdown("---")

        with col_clean_2:
            st.markdown("#### 2. ç‰¹å¾åˆ†å¸ƒä¼˜åŒ– (é’ˆå¯¹æ•°å€¼)")
            st.caption("é™ä½æŸä¸€ç‰¹å¾ä¸­ä¼—æ•°ï¼ˆå‡ºç°æœ€å¤šçš„å€¼ï¼‰çš„æ¯”ä¾‹ï¼Œå¹³è¡¡æ•°æ®åˆ†å¸ƒ")

            # æ£€æµ‹é˜ˆå€¼è®¾ç½®
            rep_threshold = st.slider("é«˜é‡å¤ç‡æ£€æµ‹é˜ˆå€¼", 0.5, 0.99, 0.8, 0.05,
                                      help="æ£€æµ‹ä¼—æ•°å æ¯”è¶…è¿‡æ­¤æ¯”ä¾‹çš„ç‰¹å¾")

            high_rep_cols = cleaner.detect_high_repetition_columns(rep_threshold)

            if high_rep_cols:
                st.warning(f"âš ï¸ æ£€æµ‹åˆ° {len(high_rep_cols)} ä¸ªç‰¹å¾å­˜åœ¨é«˜é‡å¤å€¼")

                # æ˜¾ç¤ºè¯¦æƒ…
                rep_data = []
                for col, info in high_rep_cols.items():
                    rep_data.append({
                        "ç‰¹å¾": col,
                        "ä¼—æ•°": str(info['most_frequent_value']),
                        "å½“å‰å æ¯”": f"{info['frequency'] * 100:.1f}%"
                    })
                st.dataframe(pd.DataFrame(rep_data), use_container_width=True, hide_index=True)

                # æ“ä½œåŒº
                st.markdown("##### ğŸ”§ æ‰§è¡Œä¼˜åŒ–")
                target_col = st.selectbox("é€‰æ‹©è¦ä¼˜åŒ–çš„ç‰¹å¾", list(high_rep_cols.keys()))

                # æ™ºèƒ½è®¡ç®—æ»‘å—èŒƒå›´ï¼šä¸èƒ½æ¯”å½“å‰å æ¯”è¿˜é«˜ï¼Œä¹Ÿä¸èƒ½å¤ªä½ï¼ˆå¦‚0%ï¼‰
                current_freq = high_rep_cols[target_col]['frequency']
                target_rate = st.slider(
                    f"ç›®æ ‡å æ¯” (é’ˆå¯¹ {target_col})",
                    0.1, float(current_freq), 0.5, 0.05,
                    help="é€šè¿‡éšæœºåˆ é™¤åŒ…å«ä¼—æ•°çš„æ ·æœ¬ï¼Œä½¿å…¶å æ¯”é™ä½åˆ°æ­¤å€¼"
                )

                if st.button(f"ğŸ“‰ é™ä½ '{target_col}' çš„é‡å¤ç‡", type="primary"):
                    original_len = len(df)
                    cleaned_df = cleaner.reduce_feature_repetition(target_col, target_rate)
                    new_len = len(cleaned_df)
                    st.session_state.processed_data = cleaned_df
                    log_fe_step(
                        operation="åˆ†å¸ƒä¼˜åŒ–",
                        description=f"é™ä½é‡å¤ç‡: {target_col}",
                        params={"feature": target_col, "target_rate": float(target_rate)},
                        input_df=df,
                        output_df=cleaned_df,
                        message=f"è¡Œæ•°: {original_len} â†’ {new_len}"
                    )

                    st.success(f"âœ… ä¼˜åŒ–å®Œæˆï¼åˆ é™¤äº† {original_len - new_len} ä¸ªæ ·æœ¬")
                    st.info(f"ğŸ“Š å½“å‰è¡Œæ•°: {new_len}ï¼Œ'{target_col}' çš„ä¼—æ•°å æ¯”å·²è°ƒæ•´è‡³ {target_rate * 100:.1f}%")
                    st.rerun()
            else:
                st.success("âœ… æœªæ£€æµ‹åˆ°é«˜é‡å¤ç‡ç‰¹å¾")

        st.markdown("---")
        st.markdown("#### 3. ğŸ§ª æŒ‰é…æ–¹/é”®èšåˆé‡å¤è®°å½•ï¼ˆæ¨èï¼šTg / åŠ›å­¦æ€§è´¨ï¼‰")
        st.caption("åŒä¸€é…æ–¹(æˆ–åŒä¸€æµ‹è¯•å£å¾„)çš„é‡å¤æµ‹é‡å¾€å¾€ä¼šå¼•å…¥æ ‡ç­¾å™ªå£°ã€‚å¯¹ target åšç¨³å¥èšåˆï¼ˆå¦‚ medianï¼‰å¯æ˜¾è‘—æå‡æ³›åŒ–ç¨³å®šæ€§ã€‚")

        all_cols = df.columns.tolist()

        # é»˜è®¤èšåˆé”®ï¼šresin_smiles + curing_agent_smiles (+ tg_method)
        default_keys = []
        for k in ["resin_smiles", "curing_agent_smiles", "tg_method"]:
            if k in all_cols:
                default_keys.append(k)

        keys = st.multiselect(
            "é€‰æ‹©èšåˆé”®ï¼ˆGroup Byï¼‰",
            options=all_cols,
            default=default_keys,
            help="å»ºè®®ï¼šresin_smiles + curing_agent_smilesï¼›å¦‚æœå­˜åœ¨ tg_method ä¸”ç›®æ ‡ä¸º Tgï¼Œå»ºè®®ä¹ŸåŠ å…¥ tg_method ä»¥ç»Ÿä¸€å£å¾„ã€‚"
        )

        # é»˜è®¤ç›®æ ‡ï¼šä¼˜å…ˆç”¨å·²é€‰ targetï¼Œå…¶æ¬¡ tg_c
        default_target = st.session_state.get("target_col") if st.session_state.get("target_col") in all_cols else ("tg_c" if "tg_c" in all_cols else all_cols[0])
        target_col_for_agg = st.selectbox("é€‰æ‹©éœ€è¦èšåˆçš„ç›®æ ‡åˆ—", options=all_cols, index=all_cols.index(default_target))

        agg_method = st.selectbox("èšåˆæ–¹å¼", options=["median", "mean", "min", "max"], index=0)
        dropna_target = st.checkbox("åˆ é™¤èšåˆåç›®æ ‡ä»ä¸ºç©º(NaN)çš„ç»„", value=True)

        if keys:
            try:
                n_unique = df[keys].drop_duplicates().shape[0]
                dup_like = len(df) - n_unique
                c1, c2, c3 = st.columns(3)
                c1.metric("å½“å‰æ ·æœ¬æ•°", len(df))
                c2.metric("æŒ‰é”®å”¯ä¸€ç»„æ•°", n_unique)
                c3.metric("å¯åˆå¹¶çš„é‡å¤è®°å½•", dup_like)
            except Exception:
                pass

        if st.button("ğŸ”„ æ‰§è¡Œèšåˆï¼ˆç”Ÿæˆ *_rep_n / *_rep_stdï¼‰", type="primary"):
            if not keys:
                st.error("è¯·è‡³å°‘é€‰æ‹© 1 ä¸ªèšåˆé”®")
            else:
                try:
                    new_df = cleaner.aggregate_by_keys(keys=keys, target_col=target_col_for_agg, agg=agg_method, dropna_target=dropna_target)
                    st.session_state.processed_data = new_df
                    log_fe_step(
                        operation="é‡å¤è®°å½•èšåˆ",
                        description=f"keys={keys} / target={target_col_for_agg} / agg={agg_method}",
                        params={"keys": keys, "target": target_col_for_agg, "agg": agg_method, "dropna_target": bool(dropna_target)},
                        input_df=df,
                        output_df=new_df,
                        message=f"è¡Œæ•°: {len(df)} â†’ {len(new_df)}"
                    )
                    st.success(f"âœ… èšåˆå®Œæˆï¼š{len(df)} è¡Œ â†’ {len(new_df)} è¡Œ")
                    st.info("å·²ç”Ÿæˆé‡å¤ç»Ÿè®¡åˆ—ï¼štg_rep_n / tg_rep_stdï¼ˆæˆ– <target>_rep_n / <target>_rep_stdï¼‰")
                    st.rerun()
                except Exception as e:
                    st.error(f"âŒ èšåˆå¤±è´¥: {e}")

    with tab4:
        st.markdown("### æ•°æ®ç±»å‹è¯Šæ–­")

        pseudo_numeric = cleaner.detect_pseudo_numeric_columns()

        if pseudo_numeric:
            st.warning(f"æ£€æµ‹åˆ° {len(pseudo_numeric)} ä¸ªä¼ªæ•°å€¼åˆ—")
            st.json(pseudo_numeric)

            if st.button("ğŸ”§ ä¿®å¤ä¼ªæ•°å€¼åˆ—", type="primary"):
                cleaned_df = cleaner.fix_pseudo_numeric_columns()
                st.session_state.processed_data = cleaned_df
                log_fe_step(
                    operation="æ•°æ®ç±»å‹ä¿®å¤",
                    description="ä¿®å¤ä¼ªæ•°å€¼åˆ—",
                    input_df=df,
                    output_df=cleaned_df,
                    message="å·²å°†å¯è½¬æ¢çš„ object åˆ—è½¬æ¢ä¸ºæ•°å€¼ç±»å‹"
                )
                st.success("âœ… æ•°æ®ç±»å‹ä¿®å¤å®Œæˆ")
        else:
            st.success("âœ… æ•°æ®ç±»å‹æ­£å¸¸")

        st.markdown("---")
        st.markdown("### ğŸ”¤ One-Hot ç¼–ç ï¼ˆæŠŠç±»åˆ«åˆ—è½¬æˆæ•°å€¼ç‰¹å¾ï¼‰")
        st.caption("é€‚åˆï¼štg_methodã€æ ‘è„‚ä½“ç³»ç±»å‹ç­‰ç±»åˆ«ä¿¡æ¯ã€‚å¦‚æœä½ å¸Œæœ›ä¸€ä¸ªæ¨¡å‹è¦†ç›–å¤šå£å¾„ï¼Œå¯å°† tg_method one-hot ååŠ å…¥ç‰¹å¾ã€‚")

        cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        if cat_cols:
            default_encode = ["tg_method"] if "tg_method" in cat_cols else []
            encode_cols = st.multiselect("é€‰æ‹©è¦ç¼–ç çš„åˆ—", options=cat_cols, default=default_encode)
            drop_first = st.checkbox("drop_firstï¼ˆå¯é€‰ï¼šé¿å…å®Œå…¨å…±çº¿ï¼‰", value=False)

            if st.button("ğŸ”¤ æ‰§è¡Œ One-Hot ç¼–ç ", type="primary"):
                if not encode_cols:
                    st.error("è¯·è‡³å°‘é€‰æ‹© 1 ä¸ªè¦ç¼–ç çš„åˆ—")
                else:
                    try:
                        new_df = cleaner.one_hot_encode(encode_cols, drop_first=drop_first, dummy_na=False)
                        st.session_state.processed_data = new_df
                        log_fe_step(
                            operation="One-Hot ç¼–ç ",
                            description=f"ç¼–ç åˆ—: {encode_cols}",
                            params={"cols": encode_cols, "drop_first": bool(drop_first)},
                            input_df=df,
                            output_df=new_df,
                            message=f"åˆ—æ•°: {df.shape[1]} â†’ {new_df.shape[1]}"
                        )
                        st.success(f"âœ… One-Hot ç¼–ç å®Œæˆï¼šåˆ—æ•° {df.shape[1]} â†’ {new_df.shape[1]}")
                        st.rerun()
                    except Exception as e:
                        st.error(f"âŒ One-Hot ç¼–ç å¤±è´¥: {e}")
        else:
            st.info("æœªæ£€æµ‹åˆ°å¯ç¼–ç çš„ç±»åˆ«åˆ—")

    with tab5:
        st.markdown("### ğŸ§ª SMILES å­—ç¬¦ä¸²æ¸…æ´—ä¸ä¿®å¤")
        st.info(
            "ğŸ’¡ é’ˆå¯¹åŸå§‹æ•°æ®ä¸­çš„ä¸è§„èŒƒ SMILESï¼ˆå¦‚åŒ…å«å¼•å·ã€éæ ‡å‡†å­—ç¬¦ã€é”™è¯¯çš„ç«‹ä½“åŒ–å­¦æ ‡è®°ç­‰ï¼‰è¿›è¡Œæ¸…æ´—å’Œæ™ºèƒ½ä¿®å¤ã€‚è¿™èƒ½æ˜¾è‘—æé«˜åç»­ç‰¹å¾æå–çš„æˆåŠŸç‡ã€‚")

        # 1. ç­›é€‰å¯èƒ½çš„ SMILES åˆ— (æ–‡æœ¬åˆ—)
        obj_cols = df.select_dtypes(include=['object']).columns.tolist()
        # ç®€å•å¯å‘å¼ï¼šé»˜è®¤é€‰ä¸­åˆ—ååŒ…å« 'smi' çš„åˆ—
        default_candidates = [c for c in obj_cols if 'smi' in c.lower()]

        cols_to_clean = st.multiselect(
            "é€‰æ‹©è¦æ¸…æ´—çš„ SMILES åˆ—",
            options=obj_cols,
            default=default_candidates,
            help="é€‰ä¸­åˆ—ä¸­çš„æ— æ•ˆå­—ç¬¦ä¸²å°†è¢«å°è¯•ä¿®å¤ï¼›æ— æ³•ä¿®å¤çš„å°†è¢«ç½®ä¸º NaNã€‚"
        )

        col_c1, col_c2 = st.columns(2)
        with col_c1:
            strategy = st.selectbox(
                "æ¸…æ´—/ä¿®å¤ç­–ç•¥",
                options=['standard', 'repair', 'strict'],
                index=1,
                format_func=lambda x: {
                    'standard': 'æ ‡å‡†æ¨¡å¼ (åŸºç¡€æ¸…æ´— + RDKit Canonical)',
                    'repair': 'æ™ºèƒ½ä¿®å¤ (æ¨èï¼šå»é™¤ç«‹ä½“æ ‡è®° / æå–æœ€å¤§ç‰‡æ®µ / å»é™¤ç›)',
                    'strict': 'ä¸¥æ ¼æ¨¡å¼ (ä»»ä½•è§£æå¤±è´¥å‡ç½® NaN)'
                }[x],
                help="æ™ºèƒ½ä¿®å¤æ¨¡å¼ä¼šå°è¯•å¤„ç† 'Salt.Component' å†™æ³•ï¼Œæˆ–å»é™¤å¯¼è‡´è§£æå¤±è´¥çš„æ‰‹æ€§æ ‡è®°ã€‚"
            )
        with col_c2:
            drop_invalid = st.checkbox(
                "åˆ é™¤æ¸…æ´—åä»æ— æ•ˆ(NaN)çš„æ ·æœ¬è¡Œ",
                value=False,
                help="å¦‚æœå‹¾é€‰ï¼Œé‚£äº›ç»è¿‡ä¿®å¤ä»æ— æ³•è§£æä¸ºåˆ†å­çš„è¡Œå°†è¢«ç›´æ¥åˆ é™¤ã€‚"
            )

        # ==== æ¸…æ´—ç»“æœé¢„è§ˆï¼ˆæ”¯æŒå¤šåˆ—ï¼‰ ====
        if st.session_state.get("smiles_clean_preview") is not None:
            st.markdown("#### ğŸ‘€ æ¸…æ´—å‰åé¢„è§ˆï¼ˆæœ€å¤š50è¡Œï¼‰")
            st.dataframe(st.session_state["smiles_clean_preview"], use_container_width=True)
            if st.button("ğŸ§¹ æ¸…é™¤é¢„è§ˆ", key="clear_smiles_preview"):
                st.session_state["smiles_clean_preview"] = None
                st.session_state["smiles_clean_cols"] = None
                st.rerun()

        st.markdown("---")

        if st.button("ğŸ§ª æ‰§è¡Œæ¸…æ´—ä¸ä¿®å¤", type="primary"):
            if not cols_to_clean:
                st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹©ä¸€åˆ—è¿›è¡Œæ¸…æ´—")
            else:
                try:
                    # è°ƒç”¨åç«¯ AdvancedDataCleaner.clean_smiles_columns
                    # æ³¨æ„ï¼šè¿™ä¾èµ–äºæ‚¨ä¹‹å‰åœ¨ core/data_processor.py ä¸­æ·»åŠ çš„æ–¹æ³•
                    if not hasattr(cleaner, 'clean_smiles_columns'):
                        st.error("âŒ åç«¯ä»£ç æœªæ›´æ–°ï¼šæœªåœ¨ AdvancedDataCleaner ä¸­æ‰¾åˆ° `clean_smiles_columns` æ–¹æ³•ã€‚")
                    else:
                        df_before = df[cols_to_clean].copy()
                        new_df = cleaner.clean_smiles_columns(
                            columns=cols_to_clean,
                            strategy=strategy,
                            drop_invalid=drop_invalid
                        )
                        st.session_state.processed_data = new_df

                        # ä¿å­˜æ¸…æ´—å‰åå¯¹æ¯”é¢„è§ˆï¼ˆå¤šåˆ—æ”¯æŒï¼‰ï¼Œä¾› rerun åå±•ç¤º
                        try:
                            _preview = pd.DataFrame(index=new_df.index)
                            for _c in cols_to_clean:
                                _preview[f"{_c} (before)"] = df_before.get(_c)
                                _preview[f"{_c} (after)"] = new_df.get(_c)
                            st.session_state["smiles_clean_preview"] = _preview.head(50)
                            st.session_state["smiles_clean_cols"] = cols_to_clean
                        except Exception:
                            # é¢„è§ˆå¤±è´¥ä¸å½±å“ä¸»æµç¨‹
                            pass


                        st.success("âœ… æ¸…æ´—å®Œæˆï¼")

                        # æ˜¾ç¤ºæ—¥å¿—æ‘˜è¦
                        logs = [x for x in cleaner.cleaning_log if x.get('action') == 'clean_smiles']
                        if logs:
                            st.markdown("#### ğŸ“Š æ¸…æ´—ç»“æœç»Ÿè®¡")
                            log_data = []
                            for l in logs:
                                log_data.append({
                                    "åˆ—å": l['column'],
                                    "åŸå§‹æœ‰æ•ˆæ•°": l['valid_before'],
                                    "ä¿®å¤åæœ‰æ•ˆæ•°": l['valid_after'],
                                    "æœ€ç»ˆæ— æ•ˆæ•°": l['lost_samples']
                                })
                            st.dataframe(pd.DataFrame(log_data), use_container_width=True)

                        if drop_invalid:
                            dropped_logs = [x for x in cleaner.cleaning_log if
                                            x.get('action') == 'drop_invalid_smiles_rows']
                            if dropped_logs:
                                count = dropped_logs[-1]['rows_dropped']
                                st.warning(f"ğŸ—‘ï¸ å·²åˆ é™¤ {count} è¡Œæ— æ•ˆæ ·æœ¬")

                        st.rerun()

                except Exception as e:
                    st.error(f"âŒ æ‰§è¡Œå¤±è´¥: {str(e)}")
                    st.code(traceback.format_exc())

    # ================= [é¡ºå»¶] åŸ Tab 5 -> Tab 6: SMILESç»„åˆ†åˆ†åˆ— =================
    with tab6:
        # (è¿™é‡Œæ˜¯åŸæ¥çš„ "with tab5:" çš„å†…å®¹ï¼Œä¸åšä¿®æ”¹ï¼Œç›´æ¥ç²˜è´´è¿‡æ¥)
        st.markdown("### ğŸ§© SMILESç»„åˆ†è‡ªåŠ¨åˆ†åˆ—ï¼ˆæ ‘è„‚/å›ºåŒ–å‰‚/æ”¹æ€§å‰‚ï¼‰")
        # ... (åŸ tab5 ä»£ç å†…å®¹) ...
        # (è¯·ç¡®ä¿è¿™é‡Œçš„ä»£ç é€»è¾‘ä¸åŸæ–‡ä»¶ä¸€è‡´ï¼Œåªæ˜¯ç¼©è¿›åœ¨ with tab6 ä¸‹)
        st.info("ğŸ’¡ å°†å•å…ƒæ ¼å†…çš„å¤šç»„åˆ† SMILESï¼ˆå¦‚ 'A;B' æˆ– 'A + B' æˆ– 'A.B'ï¼‰è‡ªåŠ¨æ‹†åˆ†åˆ°å¤šåˆ—...")

        from core.smiles_utils import split_smiles_column, build_formulation_key
        import re

        text_cols_local = df.select_dtypes(include=['object', 'category']).columns.tolist()
        smiles_cols = [c for c in text_cols_local if 'smiles' in c.lower()]
        candidate_cols = smiles_cols if smiles_cols else text_cols_local

        if not candidate_cols:
            st.warning("âš ï¸ æœªæ£€æµ‹åˆ°å¯åˆ†åˆ—çš„æ–‡æœ¬åˆ—ï¼ˆobject/categoryï¼‰ã€‚")
        else:
            # ... (ä¿ç•™åŸæœ‰çš„åˆ†åˆ—é€»è¾‘ä»£ç ) ...
            # ä¸ºèŠ‚çœç¯‡å¹…ï¼Œæ­¤å¤„çœç•¥ä¸­é—´æœªä¿®æ”¹ä»£ç ï¼Œè¯·ä¿ç•™åŸ app.py ä¸­è¯¥éƒ¨åˆ†é€»è¾‘
            # ...
            # ...
            # ç›´åˆ°åŸ tab5 ç»“æŸ
            pass

            # (ä»¥ä¸‹æ˜¯åŸåˆ†åˆ—é€»è¾‘çš„ UI ç»„ä»¶ï¼Œéœ€ç¡®ä¿å®ƒä»¬ç°åœ¨ä½äº tab6 ä¸‹)
            # é»˜è®¤ä¼˜å…ˆï¼šresin_smiles / curing_agent_smiles
            default_cols = []
            for cand in ["resin_smiles", "curing_agent_smiles", "hardener_smiles", "curing_agent",
                         "curing_agent_smiles"]:
                if cand in candidate_cols:
                    default_cols.append(cand)
            if not default_cols:
                default_cols = [candidate_cols[0]]

            cols_to_split = st.multiselect(
                "é€‰æ‹©è¦åˆ†åˆ—çš„åˆ—",
                options=candidate_cols,
                default=default_cols,
                help="å»ºè®®è‡³å°‘é€‰æ‹© resin_smiles ä¸ curing_agent_smiles ä¸¤åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚",
                key="split_cols_multiselect"  # åŠ ä¸ª key é˜²æ­¢å†²çª
            )

            col_s1, col_s2, col_s3 = st.columns(3)
            with col_s1:
                max_components = st.slider("æœ€å¤§åˆ†åˆ—ç»„åˆ†æ•°", 1, 12, 6, help="æ¯åˆ—æœ€å¤šæ‹†æˆå¤šå°‘ä¸ªç»„åˆ†ï¼ˆ*_1~*_kï¼‰")
            with col_s2:
                canonicalize = st.checkbox("RDKit canonical åŒ–ç»„åˆ†ï¼ˆæ¨èï¼‰", value=True, key="split_canon")
            with col_s3:
                keep_original = st.checkbox("ä¿ç•™åŸå§‹åˆ—", value=True, key="split_keep")

            add_key = st.checkbox("ç”Ÿæˆ *_key é…æ–¹é”®ï¼ˆæ’åºå»é‡å '.' æ‹¼æ¥ï¼‰", value=True, key="split_add_key")
            add_n = st.checkbox("ç”Ÿæˆ *_n_components ç»„åˆ†æ•°åˆ—", value=True, key="split_add_n")

            if st.button("ğŸ§© æ‰§è¡Œåˆ†åˆ—", type="primary"):
                new_df = df.copy()
                created_cols = []

                for c in cols_to_split:
                    new_df, new_cols = split_smiles_column(
                        new_df,
                        column=c,
                        max_components=max_components,
                        canonicalize=canonicalize,
                        add_key=add_key,
                        add_n_components=add_n,
                        keep_original=keep_original,
                        prefix=None
                    )
                    created_cols.extend(new_cols)

                if add_key:
                    resin_key = None
                    hard_key = None
                    for c in cols_to_split:
                        if resin_key is None and "resin" in c.lower():
                            if f"{c}_key" in new_df.columns:
                                resin_key = f"{c}_key"
                        if hard_key is None and ("curing" in c.lower() or "hardener" in c.lower()):
                            if f"{c}_key" in new_df.columns:
                                hard_key = f"{c}_key"
                    if resin_key and hard_key:
                        new_df = build_formulation_key(
                            new_df,
                            resin_key_col=resin_key,
                            hardener_key_col=hard_key,
                            new_col="formulation_key"
                        )
                        created_cols.append("formulation_key")

                st.session_state.processed_data = new_df
                st.success(f"âœ… åˆ†åˆ—å®Œæˆï¼šæ–°å¢ {len(created_cols)} åˆ—")
                if created_cols:
                    st.caption("æ–°å¢åˆ—ç¤ºä¾‹ï¼ˆå‰ 20 ä¸ªï¼‰ï¼š " + ", ".join(created_cols[:20]) + (
                        " ..." if len(created_cols) > 20 else ""))
                st.rerun()

            st.markdown("---")
            st.markdown("#### ğŸ” åˆ†åˆ—åçš„ç±»åˆ«åˆ†å¸ƒå¿«é€Ÿä½“æ£€")

            preview_cols = [c for c in df.columns if c.endswith("_key") or re.search(r"_\d+$", c)]
            if preview_cols:
                prev_col = st.selectbox("é€‰æ‹©è¦æŸ¥çœ‹åˆ†å¸ƒçš„åˆ—", options=preview_cols, key="split_view_col")
                vc = df[prev_col].value_counts(dropna=False)
                if len(vc) > 0:
                    col_m1, col_m2, col_m3 = st.columns(3)
                    col_m1.metric("å”¯ä¸€ç±»åˆ«æ•°", int(len(vc)))
                    col_m2.metric("æœ€å¤§æ ·æœ¬æ•°", int(vc.max()))
                    col_m3.metric("ä¸­ä½æ•°æ ·æœ¬æ•°", int(vc.median()))
                    st.bar_chart(vc.head(10))

                    st.markdown("##### âš–ï¸ ä¸€é”®ç±»åˆ«ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰")
                    default_cap = int(max(1, vc.median()))
                    cap = st.slider(
                        "æ¯ä¸ªç±»åˆ«æœ€å¤§æ ·æœ¬æ•°",
                        min_value=1,
                        max_value=int(vc.max()),
                        value=default_cap,
                        help="å°†è¶…é«˜é¢‘çš„å•ä½“/é…æ–¹ä¸‹é‡‡æ ·åˆ°æŒ‡å®šä¸Šé™ï¼Œå‡å°‘æ•°æ®ä¸­â€œå•ç§åˆ†å­å•ä½“è¿‡å¤šâ€çš„åç½®ã€‚",
                        key="split_cap_slider"
                    )
                    if st.button("âš–ï¸ ç«‹å³å¯¹è¯¥åˆ—æ‰§è¡Œå¹³è¡¡", key=f"quick_balance_{prev_col}"):
                        cleaner_tmp = AdvancedDataCleaner(df)
                        balanced_df = cleaner_tmp.balance_category_counts(prev_col, max_samples=int(cap))
                        st.session_state.processed_data = balanced_df
                        st.success(f"âœ… å·²å¯¹ {prev_col} æ‰§è¡Œç±»åˆ«å¹³è¡¡ï¼ˆmax_samples={int(cap)}ï¼‰")
                        st.rerun()
            else:
                st.info("å½“å‰æ•°æ®è¿˜æ²¡æœ‰ *_key æˆ– *_æ•°å­— çš„åˆ†åˆ—åˆ—ã€‚ä½ å¯ä»¥å…ˆç‚¹å‡»ä¸Šæ–¹æŒ‰é’®æ‰§è¡Œåˆ†åˆ—ã€‚")

    # ================= [é¡ºå»¶] åŸ Tab 6 -> Tab 7: ç±»åˆ«å¹³è¡¡ =================
    with tab7:
        # (è¿™é‡Œæ˜¯åŸæ¥çš„ "with tab6:" çš„å†…å®¹ï¼Œä¸åšä¿®æ”¹ï¼Œç›´æ¥ç²˜è´´è¿‡æ¥)
        st.markdown("### âš–ï¸ ç±»åˆ«å¹³è¡¡ (é’ˆå¯¹åŒ–å­¦ç»“æ„)")
        # ... (åŸ tab6 ä»£ç å†…å®¹) ...
        # (ç¡®ä¿ç¼©è¿›æ­£ç¡®)
        st.info("ğŸ’¡ è§£å†³ç‰¹å®šå•ä½“/åˆ†å­é‡å¤æ¬¡æ•°è¿‡å¤šçš„é—®é¢˜...")

        text_cols = df.select_dtypes(include=['object']).columns.tolist()
        if text_cols:
            cat_col = st.selectbox("é€‰æ‹©è¦å¹³è¡¡çš„ç±»åˆ«åˆ— (é€šå¸¸æ˜¯SMILES)", text_cols, key="bal_col_select")

            counts = df[cat_col].value_counts()
            n_unique = len(counts)

            col1, col2, col3 = st.columns(3)
            col1.metric("å”¯ä¸€ç±»åˆ«æ•°", n_unique)
            col2.metric("æœ€å¤§æ ·æœ¬æ•°", counts.max())
            col3.metric("ä¸­ä½æ•°æ ·æœ¬æ•°", int(counts.median()))

            st.markdown("#### Top 10 å‡ºç°æœ€é¢‘ç¹çš„åˆ†å­")
            st.bar_chart(counts.head(10))

            st.markdown("#### ğŸ”§ å¹³è¡¡è®¾ç½®")

            limit_val = st.slider(
                "æ¯ä¸ªç±»åˆ«çš„æœ€å¤§æ ·æœ¬æ•° (Max Samples per Category)",
                min_value=1,
                max_value=int(counts.max()),
                value=int(counts.median()) if n_unique > 0 else 10,
                key="bal_slider"
            )

            if st.button(f"âš–ï¸ æ‰§è¡Œå¹³è¡¡ (é™åˆ¶ä¸º {limit_val} ä¸ª)", type="primary", key="bal_btn"):
                old_len = len(df)
                cleaned_df = cleaner.balance_category_counts(cat_col, max_samples=limit_val)
                new_len = len(cleaned_df)

                st.session_state.processed_data = cleaned_df

                st.success(f"âœ… å¹³è¡¡å®Œæˆï¼")
                st.info(f"ğŸ“Š æ€»æ ·æœ¬æ•°ä» {old_len} å‡å°‘åˆ° {new_len} (åˆ é™¤äº† {old_len - new_len} ä¸ªè¿‡åº¦é‡å¤æ ·æœ¬)")
                st.rerun()
        else:
            st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬åˆ—ï¼Œæ— æ³•æ‰§è¡Œç±»åˆ«å¹³è¡¡")


    with tab8:
        st.markdown("### ğŸ§© K-Means æ™ºèƒ½èšç±» (æ–‡çŒ®æ ¸å¿ƒç­–ç•¥)")
        st.info(
            "ğŸ’¡ æ–‡çŒ® [Polymer 256 (2022) 125216] æŒ‡å‡ºï¼Œåˆ©ç”¨ K-Means å°†ç¯æ°§æ ‘è„‚ä½“ç³»åˆ†ä¸º 11 ä¸ªç°‡ï¼Œå¯å°† RÂ² æå‡è‡³ 0.99ã€‚æ­¤åŠŸèƒ½å°†ç”Ÿæˆ 'Cluster_Label' åˆ—ä½œä¸ºæ–°ç‰¹å¾ã€‚")

        # é€‰æ‹©ç”¨äºèšç±»çš„ç‰¹å¾ï¼ˆé€šå¸¸æ˜¯åˆ†å­æè¿°ç¬¦ + æ¸©åº¦ï¼‰
        num_cols = df.select_dtypes(include=np.number).columns.tolist()
        cluster_features = st.multiselect("é€‰æ‹©ç”¨äºèšç±»çš„ç‰¹å¾", num_cols,
                                          default=num_cols[:5] if len(num_cols) > 5 else num_cols)

        col_k1, col_k2 = st.columns(2)
        with col_k1:
            auto_k = st.checkbox("è‡ªåŠ¨æœç´¢æœ€ä½³ç°‡æ•°é‡ (Silhouette Score)", value=True)
        with col_k2:
            n_clusters = st.slider("æ‰‹åŠ¨æŒ‡å®šç°‡æ•°é‡", 2, 20, 11, disabled=auto_k)

        if st.button("ğŸš€ æ‰§è¡Œ K-Means èšç±»", type="primary"):
            cleaned_df, final_k = cleaner.apply_kmeans_clustering(
                feature_cols=cluster_features,
                n_clusters=None if auto_k else n_clusters,
                auto_tune=auto_k
            )
            st.session_state.processed_data = cleaned_df
            st.success(f"âœ… èšç±»å®Œæˆï¼æœ€ä½³ç°‡æ•°é‡: {final_k}")
            st.rerun()

# ============================================================
# é¡µé¢ï¼šæ•°æ®å¢å¼º
# ============================================================
def page_data_enhancement():
    """æ•°æ®å¢å¼ºé¡µé¢"""
    st.title("âœ¨ æ•°æ®å¢å¼º")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    enhancer = DataEnhancer(df)

    tab1, tab2 = st.tabs(["ğŸ”® KNNæ™ºèƒ½å¡«å……", "ğŸ§¬ VAEç”Ÿæˆå¼å¢å¼º"])

    with tab1:
        st.markdown("### KNNæ™ºèƒ½å¡«å……")
        st.info("ä½¿ç”¨Kè¿‘é‚»ç®—æ³•é¢„æµ‹å¹¶å¡«å……ç¼ºå¤±å€¼ï¼Œæ¯”ç®€å•çš„å‡å€¼/ä¸­ä½æ•°å¡«å……æ›´å‡†ç¡®")

        n_neighbors = st.slider("Kå€¼ï¼ˆè¿‘é‚»æ•°é‡ï¼‰", 1, 20, 5)

        if st.button("ğŸ”§ æ‰§è¡ŒKNNå¡«å……", type="primary"):
            with st.spinner("æ­£åœ¨æ‰§è¡ŒKNNå¡«å……..."):
                filled_df = enhancer.knn_impute(n_neighbors=n_neighbors)
                st.session_state.processed_data = filled_df
                st.success("âœ… KNNå¡«å……å®Œæˆ")

                # å¯¹æ¯”
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("åŸå§‹ç¼ºå¤±å€¼", df.isnull().sum().sum())
                with col2:
                    st.metric("å¤„ç†åç¼ºå¤±å€¼", filled_df.isnull().sum().sum())

    with tab2:
        st.markdown("### VAEç”Ÿæˆå¼æ•°æ®å¢å¼º")
        st.info("ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨(VAE)å­¦ä¹ æ•°æ®åˆ†å¸ƒï¼Œç”Ÿæˆé«˜ä¿çœŸè™šæ‹Ÿæ•°æ®ç‚¹")

        col1, col2 = st.columns(2)
        with col1:
            n_samples = st.number_input("ç”Ÿæˆæ ·æœ¬æ•°é‡", 10, 1000, 100)
            latent_dim = st.slider("æ½œåœ¨ç©ºé—´ç»´åº¦", 4, 64, 16)
            h_dim = st.slider("éšè—å±‚ç»´åº¦", 32, 256, 128)

        with col2:
            epochs = st.slider("è®­ç»ƒè½®æ•°", 10, 500, 100)
            batch_size = st.selectbox("æ‰¹å¤§å°", [16, 32, 64, 128], index=1)
            learning_rate = st.number_input("å­¦ä¹ ç‡", 0.0001, 0.1, 0.001, format="%.4f")

        if st.button("ğŸš€ ç”Ÿæˆå¢å¼ºæ•°æ®", type="primary"):
            try:
                with st.spinner("æ­£åœ¨è®­ç»ƒVAEæ¨¡å‹..."):
                    progress_bar = st.progress(0)

                    generated_df, fig = enhancer.generate_with_vae(
                        n_samples=n_samples,
                        latent_dim=latent_dim,
                        h_dim=h_dim,
                        epochs=epochs,
                        batch_size=batch_size,
                        lr=learning_rate
                    )

                    progress_bar.progress(100)

                st.success(f"âœ… æˆåŠŸç”Ÿæˆ {len(generated_df)} ä¸ªæ ·æœ¬")

                # PCAå¯è§†åŒ–
                st.markdown("### ğŸ“Š PCAå¯è§†åŒ–å¯¹æ¯”")
                st.plotly_chart(fig, use_container_width=True)

                # åˆå¹¶é€‰é¡¹
                if st.checkbox("å°†ç”Ÿæˆæ•°æ®åˆå¹¶åˆ°åŸå§‹æ•°æ®"):
                    merged_df = pd.concat([df, generated_df], ignore_index=True)
                    st.session_state.processed_data = merged_df
                    st.success(f"âœ… åˆå¹¶åæ•°æ®: {merged_df.shape}")

            except Exception as e:
                st.error(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")


# ============================================================
# é¡µé¢ï¼šåˆ†å­ç‰¹å¾æå–ï¼ˆå®Œæ•´5ç§æ–¹æ³•ï¼‰
# ============================================================
def page_molecular_features():
    """åˆ†å­ç‰¹å¾æå–é¡µé¢ - å®Œæ•´è¿˜åŸ5ç§æ–¹æ³• + åˆ†å­æŒ‡çº¹ (é€‚é…åŒç»„åˆ†)"""
    st.title("ğŸ§¬ åˆ†å­ç‰¹å¾æå–")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    # ä¼˜å…ˆä½¿ç”¨å¤„ç†åçš„æ•°æ®
    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data

    # æ£€æµ‹SMILESåˆ—
    text_cols = df.select_dtypes(include=['object']).columns.tolist()
    smiles_candidates = [col for col in text_cols if 'smiles' in col.lower() or 'smi' in col.lower()]

    if not text_cols:
        st.warning("âš ï¸ æ•°æ®ä¸­æœªæ£€æµ‹åˆ°æ–‡æœ¬åˆ—ï¼Œæ— æ³•æå–åˆ†å­ç‰¹å¾")
        return

    st.markdown("### ğŸ”¬ SMILESåˆ—é€‰æ‹©")

    # Render operation log panel
    oplog_render()

    # -----------------------------
    # å¤šç»„åˆ†/æ··åˆç‰© SMILES å¤„ç†
    # è¯´æ˜ï¼š
    # 1) å•åˆ—é‡Œå¯èƒ½ç”¨ ";" ç­‰åˆ†éš”ç¬¦è¡¨ç¤ºå¤šä¸ªç»„åˆ†ï¼ˆRDKit ä¸èƒ½ç›´æ¥è§£æ ";"ï¼Œä½†èƒ½è§£æ "."ï¼‰
    # 2) ä¹Ÿå¯èƒ½æ¯ä¸ªç»„åˆ†å•ç‹¬å ä¸€åˆ—ï¼ˆå¦‚ resin_smiles_1, resin_smiles_2 ...ï¼‰
    # è¿™é‡ŒæŠŠå¤šä¸ªç»„åˆ†ç»Ÿä¸€è½¬æ¢ä¸ºâ€œå¤šç‰‡æ®µ SMILESâ€ï¼ˆç”¨ "." è¿æ¥ï¼‰ï¼Œå†äº¤ç»™ RDKit/æŒ‡çº¹æå–å™¨ã€‚
    # -----------------------------
    import re

    def _split_smiles_cell(x):
        """æŠŠå•å…ƒæ ¼é‡Œçš„ SMILES æ‹†æˆç»„åˆ†åˆ—è¡¨ã€‚

        ä»…æŠŠå¸¸è§â€œåˆ—è¡¨åˆ†éš”ç¬¦â€å½“ä½œç»„åˆ†è¾¹ç•Œï¼š;ã€ï¼›ã€|ã€ä»¥åŠå¸¦ç©ºæ ¼çš„ +
        æ³¨æ„ï¼šä¸æŠŠâ€œ/â€å½“åˆ†éš”ç¬¦ï¼ˆå®ƒæ˜¯ SMILES ç«‹ä½“åŒ–å­¦çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚
        """
        if x is None or (isinstance(x, float) and np.isnan(x)):
            return []
        s = str(x).strip()
        if not s or s.lower() == 'nan':
            return []
        # ç»Ÿä¸€ä¸­æ–‡åˆ†å·
        s = s.replace('ï¼›', ';')

        # å…ˆæŒ‰ ; æˆ– | åˆ†å‰²
        parts = re.split(r"\s*[;|]\s*", s)

        # å†æŒ‰â€œå¸¦ç©ºæ ¼çš„ +â€åˆ†å‰²ï¼ˆé¿å…è¯¯ä¼¤ [N+] è¿™ç±»å¸¦ç”µè·å†™æ³•ï¼‰
        final = []
        for p in parts:
            final.extend(re.split(r"\s+\+\s+", p))

        # æ¸…ç†ç©ºä¸²
        final = [p.strip() for p in final if p and p.strip()]
        return final

    def _combine_components(df_in: pd.DataFrame, cols: list[str]):
        """æŠŠå¤šåˆ—/å•åˆ— SMILES åˆå¹¶æˆå¤šç‰‡æ®µ SMILESï¼Œå¹¶è¿”å›(åˆå¹¶åçš„Series, ç»„åˆ†æ•°é‡Series)"""
        if not cols:
            return pd.Series([np.nan] * len(df_in)), pd.Series([0] * len(df_in))

        combined = []
        counts = []
        for _, row in df_in[cols].iterrows():
            comps = []
            for c in cols:
                comps.extend(_split_smiles_cell(row[c]))
            counts.append(len(comps))
            combined.append('.'.join(comps) if comps else np.nan)
        return pd.Series(combined), pd.Series(counts)

    col1, col2 = st.columns(2)
    with col1:
        default_idx = 0
        if smiles_candidates:
            default_idx = text_cols.index(smiles_candidates[0])

        # è¿™é‡Œæ˜ç¡®è¿™æ˜¯ç¬¬ä¸€ç»„åˆ†ï¼ˆé€šå¸¸æ˜¯æ ‘è„‚ï¼‰
        smiles_col = st.selectbox(
            "é€‰æ‹©åŒ…å«SMILESçš„åˆ— (æ ‘è„‚/ä¸»ä½“)",
            text_cols,
            index=default_idx
        )
<<<<<<< HEAD
        st.session_state.selected_smiles_col = smiles_col
=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4

    with col2:
        st.markdown("**ç¤ºä¾‹SMILES:**")
        samples = df[smiles_col].dropna().head(3).tolist()
        for s in samples:
            st.code(s[:50] + "..." if len(str(s)) > 50 else s)

    # --- å¤šç»„åˆ†è®¾ç½®ï¼ˆæ ‘è„‚ä¾§ï¼‰ ---
    st.markdown("#### ğŸ§© å¤šç»„åˆ†/æ··åˆç‰©è®¾ç½® (å¯é€‰)")
    resin_mix_mode = st.checkbox(
        "æ ‘è„‚ä¸ºå¤šç»„åˆ†ï¼ˆæˆ–å•å…ƒæ ¼å†…åŒ…å«å¤šä¸ªSMILESï¼‰",
        value=False,
        help="å¦‚æœä½ çš„æ ‘è„‚åˆ—é‡Œå‡ºç° 'A;B' è¿™ç§å†™æ³•ï¼Œæˆ–æœ‰ resin_smiles_1/resin_smiles_2 è¿™ç§å¤šåˆ—ç»„åˆ†ï¼Œè¯·å¼€å¯ã€‚"
    )

    resin_component_cols = [smiles_col]
    resin_mix_layout = "å•åˆ—"  # ä»…ç”¨äº UI è®°å½•
    add_component_count_features = False

    if resin_mix_mode:
        resin_mix_layout = st.radio(
            "æ ‘è„‚ç»„åˆ†åœ¨è¡¨æ ¼ä¸­çš„ç»„ç»‡æ–¹å¼",
            ["å•åˆ—ï¼ˆåŒä¸€å•å…ƒæ ¼ç”¨åˆ†éš”ç¬¦è¡¨ç¤ºå¤šä¸ªç»„åˆ†ï¼Œå¦‚ A;Bï¼‰", "å¤šåˆ—ï¼ˆæ¯åˆ—ä¸€ä¸ªç»„åˆ†ï¼Œå¦‚ resin_smiles_1/resin_smiles_2â€¦ï¼‰"],
            index=0
        )

        if resin_mix_layout.startswith("å¤šåˆ—"):
            # è‡ªåŠ¨æ¨èï¼šä¸æ‰€é€‰åˆ—åŒå‰ç¼€ã€ä¸”ä»¥ _æ•°å­— ç»“å°¾çš„åˆ—
            pattern = re.compile(rf"^{re.escape(smiles_col)}_\d+$")
            auto_cols = [c for c in text_cols if pattern.match(c)]
            # æŒ‰æœ«å°¾æ•°å­—æ’åº
            def _tail_num(colname: str):
                try:
                    return int(colname.split('_')[-1])
                except:
                    return 0
            auto_cols = sorted(auto_cols, key=_tail_num)
            resin_component_cols = st.multiselect(
                "é€‰æ‹©æ ‘è„‚ç»„åˆ†åˆ—",
                options=text_cols,
                default=auto_cols if auto_cols else [smiles_col],
                help="ç³»ç»Ÿä¼šæŠŠè¿™äº›åˆ—çš„æ‰€æœ‰éç©ºç»„åˆ†åˆå¹¶ä¸ºä¸€ä¸ªå¤šç‰‡æ®µSMILESï¼ˆç”¨ '.' è¿æ¥ï¼‰"
            )
        else:
            st.caption("å°†è‡ªåŠ¨æŠŠ ';'ã€'ï¼›'ã€'|'ã€ä»¥åŠå¸¦ç©ºæ ¼çš„ ' + ' è½¬æ¢ä¸ºå¤šç»„åˆ†åˆ†éš”ï¼Œå¹¶ç”¨ '.' è¿æ¥ã€‚")

        add_component_count_features = st.checkbox(
            "é¢å¤–åŠ å…¥ç»„åˆ†æ•°é‡ç‰¹å¾ï¼ˆresin_n_components / hardener_n_componentsï¼‰",
            value=True,
            help="å¯¹å¾ˆå¤šæ··é…ä½“ç³»ï¼Œç»„åˆ†æ•°é‡æœ¬èº«ä¹Ÿä¼šå½±å“æ€§èƒ½ï¼›æ­¤é€‰é¡¹ä¼šæŠŠç»„åˆ†æ•°ä½œä¸ºé¢å¤–æ•°å€¼ç‰¹å¾å¹¶å…¥æ•°æ®é›†ã€‚"
        )

    st.markdown("---")

    # ğŸ”¥ æ ¸å¿ƒåŠŸèƒ½ï¼š5ç§æå–æ–¹æ³•é€‰æ‹©
    st.markdown("### ğŸ› ï¸ æå–æ–¹æ³•é€‰æ‹©")

    extraction_method = st.radio(
        "é€‰æ‹©åˆ†å­ç‰¹å¾æå–æ–¹æ³•",
        [
            "ğŸ‘† åˆ†å­æŒ‡çº¹ (MACCS/Morgan)",
            "ğŸ”¹ RDKit æ ‡å‡†ç‰ˆ (æ¨èæ–°æ‰‹)",
            "ğŸš€ RDKit å¹¶è¡Œç‰ˆ (å¤§æ•°æ®é›†)",
            "ğŸ’¾ RDKit å†…å­˜ä¼˜åŒ–ç‰ˆ (ä½å†…å­˜)",
            "ğŸ”¬ Mordred æè¿°ç¬¦ (1600+ç‰¹å¾)",
            "ğŸ§Š 3Dæ„è±¡æè¿°ç¬¦ (RDKit3D+Coulomb) ",
            "ğŸ§© TDAæ‹“æ‰‘ç‰¹å¾ (æŒç»­åŒè°ƒPH) ",
            "ğŸ§  é¢„è®­ç»ƒSMILES Transformer Embedding (ChemBERTaç­‰)",
            "ğŸ•¸ï¸ å›¾ç¥ç»ç½‘ç»œç‰¹å¾ (æ‹“æ‰‘ç»“æ„)",
            "âš›ï¸ MLåŠ›åœºç‰¹å¾ (ANIèƒ½é‡/åŠ›)",
            "âš—ï¸ ç¯æ°§æ ‘è„‚ååº”ç‰¹å¾ (åŸºäºé¢†åŸŸçŸ¥è¯†)",
            "ğŸ“‘ FGD å®˜èƒ½å›¢åŒºåˆ†",
        ],
        help="ä¸åŒæ–¹æ³•é€‚ç”¨äºä¸åŒåœºæ™¯"
    )


    # Log selected extraction method
    oplog(f"Selected molecular feature method: {extraction_method}")

    # UI å˜é‡åˆå§‹åŒ–
    fp_type = "MACCS"
    fp_bits = 2048
    fp_radius = 2
    hardener_col = None
    hardener_component_cols = None
    hardener_fusion_mode = "ä»…ç”¨äºæŒ‡çº¹/ååº”ç‰¹å¾ï¼ˆå½“å‰é»˜è®¤ï¼‰"  # åˆå§‹åŒ–å›ºåŒ–å‰‚åˆ—å˜é‡
    phr_col = None

    # --- [ä¿®å¤] åœ¨è¿™é‡Œè¡¥ä¸Š Transformer å˜é‡çš„é»˜è®¤åˆå§‹åŒ– ---
    lm_model_name = "seyonec/ChemBERTa-zinc-base-v1"
    lm_pooling = "cls"
    lm_max_length = 128
    lm_batch_size = 16

    # [æ–°å¢] TDA å‚æ•°é»˜è®¤å€¼
    tda_maxdim = 2
    tda_use_pim = False
    tda_pim_pixels = 10
    tda_pim_spread = 1.0

    # [æ–°å¢] MLåŠ›åœº(ANI) å‚æ•°é»˜è®¤å€¼
    ani_batch_size = 64
    ani_cpu_workers = max(1, (os.cpu_count() or 1) - 1) if os.name != 'nt' else 1

<<<<<<< HEAD
    # [æ–°å¢] æŒ‡çº¹é»˜è®¤å‚æ•°
    fp_use_chirality = False
    fp_use_features = False

    # [æ–°å¢] Mordred é»˜è®¤å‚æ•°
    mordred_batch_size = 1000
    mordred_ignore_3d = True

    # [æ–°å¢] 3D æè¿°ç¬¦é»˜è®¤å‚æ•°
    keep_all_rows_3d = True
    rdkit3d_coulomb_top_k = 10
    rdkit3d_n_jobs = None


=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4
    # ============== [ä¿®æ”¹] æŒ‡çº¹å‚æ•°è®¾ç½® ==============
    if "åˆ†å­æŒ‡çº¹" in extraction_method:
        st.info("ğŸ’¡ æç¤ºï¼šå¯¹äºç¯æ°§æ ‘è„‚ä½“ç³»ï¼Œå»ºè®®åŒæ—¶é€‰æ‹©æ ‘è„‚å’Œå›ºåŒ–å‰‚åˆ—ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æ‹¼æ¥ä¸¤è€…çš„æŒ‡çº¹ä»¥æè¿°å®Œæ•´ç½‘ç»œç»“æ„ã€‚")

        col_fp1, col_fp2, col_fp3 = st.columns(3)
        with col_fp1:
            fp_type = st.selectbox("æŒ‡çº¹ç±»å‹", ["MACCS", "Morgan"])

<<<<<<< HEAD
        
            drop_all_zero_bits = st.checkbox("ç§»é™¤å…¨ä¸º0çš„æŒ‡çº¹ä½ï¼ˆä¸æ¨èï¼šä¼šé€ æˆåˆ—ç¼ºå¤±ï¼Œå½±å“æ¨¡å‹å¯¼å…¥/å¤ç”¨ï¼‰", value=False)
=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4
        if fp_type == "Morgan":
            with col_fp2:
                fp_radius = st.selectbox("åŠå¾„ (Radius)", [2, 3, 4], index=0)
            with col_fp3:
                fp_bits = st.selectbox("ä½é•¿ (Bits)", [1024, 2048, 4096], index=1)

<<<<<<< HEAD
            # [æ–°å¢] Morgan é¢å¤–å‚æ•°
            col_fpa, col_fpb = st.columns(2)
            with col_fpa:
                fp_use_chirality = st.checkbox("åŒ…å«æ‰‹æ€§ (useChirality)", value=False,
                                              help="å¯ç”¨åä¼šæŠŠæ‰‹æ€§ä¿¡æ¯ç¼–ç åˆ°æŒ‡çº¹ä¸­ï¼ˆå¯èƒ½æå‡å¯¹æ‰‹æ€§æ•æ„Ÿä½“ç³»çš„æ•ˆæœï¼‰")
            with col_fpb:
                fp_use_features = st.checkbox("ä½¿ç”¨ Feature Morgan (FCFP, useFeatures)", value=False,
                                             help="å¯ç”¨åä½¿ç”¨ feature-based Morgan æŒ‡çº¹ï¼ˆæ›´åå‘å®˜èƒ½å›¢/è¯æ•ˆå›¢é£æ ¼ï¼‰")

=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4

        # ---- é¢„è®­ç»ƒ SMILES Transformer Embedding å‚æ•°ï¼ˆå¯é€‰ï¼‰----
        lm_model_name = "seyonec/ChemBERTa-zinc-base-v1"
        lm_pooling = "cls"
        lm_max_length = 128
        lm_batch_size = 16

        if "Transformer Embedding" in extraction_method:
            st.markdown("#### ğŸ§  é¢„è®­ç»ƒ Transformer è®¾ç½®")
            st.info("ğŸ’¡ å°†è°ƒç”¨ HuggingFace `transformers` åº“ã€‚é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹æƒé‡ï¼ˆéœ€è”ç½‘ï¼‰ã€‚")

            # 1. æ¨¡å‹åç§°è¾“å…¥æ¡†
            lm_model_name = st.text_input(
                "HuggingFace æ¨¡å‹åç§° (Model ID)",
                value=lm_model_name,  # ä½¿ç”¨é»˜è®¤å€¼åˆå§‹åŒ–
                help="ä¾‹å¦‚: 'seyonec/ChemBERTa-zinc-base-v1' æˆ– 'DeepChem/ChemBERTa-77M-MTR'"
            )

            col_lm1, col_lm2, col_lm3 = st.columns(3)

            # 2. æ± åŒ–ç­–ç•¥
            with col_lm1:
                lm_pooling = st.selectbox(
                    "æ± åŒ–ç­–ç•¥ (Pooling)",
                    ["cls", "mean"],
                    index=["cls", "mean"].index(lm_pooling) if lm_pooling in ["cls", "mean"] else 0,
                    help="CLS: å–é¦–ä¸ªtokenå‘é‡; Mean: å–æ‰€æœ‰tokenå‡å€¼"
                )

            # 3. æœ€å¤§é•¿åº¦
            with col_lm2:
                lm_max_length = st.selectbox(
                    "æœ€å¤§åºåˆ—é•¿åº¦ (Max Length)",
                    [64, 128, 256, 512],
                    index=[64, 128, 256, 512].index(lm_max_length) if lm_max_length in [64, 128, 256, 512] else 1
                )

            # 4. æ‰¹å¤§å°
            with col_lm3:
                lm_batch_size = st.selectbox(
                    "æ‰¹å¤„ç†å¤§å° (Batch Size)",
                    [8, 16, 32, 64, 128],
                    index=[8, 16, 32, 64, 128].index(lm_batch_size) if lm_batch_size in [8, 16, 32, 64, 128] else 1,
                    help="æ˜¾å­˜è¶Šå°ï¼Œè¯·è°ƒå°æ­¤æ•°å€¼"
                )

        # [æ–°å¢] åŒç»„åˆ†é€‰æ‹© UI
        st.markdown("#### åŒç»„åˆ†è®¾ç½® (æ¨è)")

        # åˆå§‹åŒ–å˜é‡
        hardener_component_cols = []

        col_h1, col_h2 = st.columns(2)
        with col_h1:
            # 1. åŸºç¡€å•åˆ—é€‰æ‹©
            candidate_cols = ["æ—  (ä»…æå–å•åˆ—)"] + [c for c in text_cols if c != smiles_col]
            hardener_col_opt = st.selectbox("é€‰æ‹©ã€å›ºåŒ–å‰‚ã€‘ä¸»åˆ—", candidate_cols)

            if hardener_col_opt != "æ—  (ä»…æå–å•åˆ—)":
                hardener_col = hardener_col_opt
                hardener_component_cols = [hardener_col]
            else:
                hardener_col = None

        # [æ–°å¢] å›ºåŒ–å‰‚å¤šç»„åˆ†å¤é€‰æ¡†
        if hardener_col:
            hardener_mix_mode = st.checkbox(
                "å›ºåŒ–å‰‚ä¸ºå¤šç»„åˆ†ï¼ˆå¤šåˆ—å¤é…ï¼‰",
                value=False,
                help="å¦‚æœä½ çš„é…æ–¹åŒ…å«å¤šç§å›ºåŒ–å‰‚ï¼ˆå¦‚ hardener_1, hardener_2ï¼‰ï¼Œè¯·å‹¾é€‰æ­¤é¡¹è¿›è¡Œå¤šåˆ—é€‰æ‹©ã€‚"
            )

            if hardener_mix_mode:
                # è‡ªåŠ¨æ­£åˆ™åŒ¹é…æ¨è
                pattern_h = re.compile(rf"^{re.escape(hardener_col)}_\d+$")
                auto_h = [c for c in text_cols if pattern_h.match(c)]

                # å…è®¸ç”¨æˆ·å¤šé€‰
                hardener_component_cols = st.multiselect(
                    "é€‰æ‹©æ‰€æœ‰å›ºåŒ–å‰‚ç»„åˆ†åˆ—",
                    options=text_cols,
                    default=auto_h if auto_h else [hardener_col],
                    help="ç³»ç»Ÿä¼šå°†è¿™äº›åˆ—åˆå¹¶æå–ç‰¹å¾ï¼ˆä¾‹å¦‚ï¼šæŒ‡çº¹å åŠ æˆ–ç»“æ„æ‹¼æ¥ï¼‰"
                )

        with col_h2:
            hardener_fusion_mode = st.selectbox(
                "å›ºåŒ–å‰‚èå…¥æ–¹å¼",
                [
                    "ä»…ç”¨äºæŒ‡çº¹/ååº”ç‰¹å¾ï¼ˆå½“å‰é»˜è®¤ï¼‰",
                    "æ‹¼æ¥SMILESåç”¨äºæ‰€æœ‰åˆ†å­ç‰¹å¾ï¼ˆResin.Hardenerï¼‰"
                ],
                index=0,
                help="é€‰æ‹©ç¬¬äºŒé¡¹åï¼ŒRDKit/Mordred/3D/ANI/Transformer ç­‰æ–¹æ³•å°†å¯¹æ‹¼æ¥åçš„ SMILES æå–ç‰¹å¾ã€‚"
            )

    # ============== [UI] ç¯æ°§æ ‘è„‚ç‰¹å¾å‚æ•° ==============
    if "ç¯æ°§æ ‘è„‚ååº”ç‰¹å¾" in extraction_method:
        st.info("ğŸ’¡ è¯¥æ–¹æ³•éœ€è¦åŒæ—¶æä¾›ã€æ ‘è„‚ã€‘å’Œã€å›ºåŒ–å‰‚ã€‘çš„SMILESç»“æ„ã€‚")
        col_h, col_p = st.columns(2)
        with col_h:
            candidate_cols = [c for c in text_cols if c != smiles_col]

            # âœ… æ”¯æŒå¤šé€‰ï¼šå…è®¸é€‰æ‹©å¤šä¸ªã€å›ºåŒ–å‰‚ã€‘SMILESåˆ—ï¼ˆä¾‹å¦‚ hardener_smiles_1/2/3ï¼‰
            # ä½¿ç”¨ key äº¤ç»™ Streamlit ç®¡ç†çŠ¶æ€ï¼Œé¿å…â€œéœ€è¦ç‚¹ä¸¤æ¬¡æ‰èƒ½é€‰ä¸Šâ€çš„äº¤äº’é—®é¢˜
            if "epoxy_hardener_cols" not in st.session_state:
                st.session_state["epoxy_hardener_cols"] = ([candidate_cols[0]] if candidate_cols else [])

            hardener_cols = st.multiselect(
                "é€‰æ‹©ã€å›ºåŒ–å‰‚ã€‘SMILESåˆ—",
                options=candidate_cols,
                key="epoxy_hardener_cols",
                help="å¯å¤šé€‰ï¼šç³»ç»Ÿä¼šæŠŠæ‰€é€‰å›ºåŒ–å‰‚SMILESåˆ—åˆå¹¶ç”¨äºç¯æ°§æ ‘è„‚ååº”ç‰¹å¾æå–"
            )

            # ä¸ºå…¼å®¹åç»­é€»è¾‘ï¼šhardener_col ä¿ç•™ç¬¬ä¸€ä¸ªé€‰æ‹©ï¼›hardener_component_cols ä¸ºå…¨éƒ¨é€‰æ‹©
            hardener_col = hardener_cols[0] if hardener_cols else None
            hardener_component_cols = hardener_cols if hardener_cols else None

        with col_p:
            num_cols = df.select_dtypes(include=np.number).columns.tolist()
            phr_col = st.selectbox("é€‰æ‹©ã€é…æ¯”ã€‘åˆ— (å¯é€‰)", ["æ—  (å‡è®¾ç†æƒ³é…æ¯”)"] + num_cols)

            stoich_mode = "theoretical"
            if phr_col and phr_col != "æ—  (å‡è®¾ç†æƒ³é…æ¯”)":
                stoich_mode = st.selectbox(
                    "é…æ¯”å«ä¹‰",
                    ["Resin/Hardener (æ€»è´¨é‡æ¯”, R/H)", "PHR (Hardener per 100 Resin)"],
                    index=0,
                    help="å¦‚æœä½ çš„é…æ¯”åˆ—æ˜¯â€˜æ ‘è„‚æ€»é‡/å›ºåŒ–å‰‚æ€»é‡(R/H)â€™ï¼Œé€‰ç¬¬ä¸€é¡¹ï¼›å¦‚æœæ˜¯ä¼ ç»Ÿ PHRï¼ˆæ¯100ä»½æ ‘è„‚å¯¹åº”å›ºåŒ–å‰‚ä»½æ•°ï¼‰ï¼Œé€‰ç¬¬äºŒé¡¹ã€‚"
                )


    
    # ============== [æ–°å¢ UI] MLåŠ›åœº(ANI) å‚æ•° ==============
<<<<<<< HEAD
    
    # ============== [æ–°å¢ UI] 3D æ„è±¡æè¿°ç¬¦ å‚æ•° ==============
    if "3Dæ„è±¡" in extraction_method:
        st.markdown("#### ğŸ§Š 3D æ„è±¡æè¿°ç¬¦å‚æ•°")
        st.info("å°†ç”Ÿæˆ 3D æ„è±¡å¹¶è®¡ç®— Coulomb Matrix / 3D æè¿°ç¬¦ã€‚è€—æ—¶è¾ƒé«˜ã€‚")
        col_3d1, col_3d2 = st.columns(2)
        with col_3d1:
            rdkit3d_coulomb_top_k = st.selectbox("Coulomb Top-K", [5, 10, 15, 20, 30, 50], index=[5, 10, 15, 20, 30, 50].index(int(rdkit3d_coulomb_top_k)) if int(rdkit3d_coulomb_top_k) in [5, 10, 15, 20, 30, 50] else 1)
        with col_3d2:
            max_workers = max(1, (os.cpu_count() or 1) - 1) if os.name != 'nt' else 1
            if max_workers <= 1:
                rdkit3d_n_jobs = 1
                st.info("âš ï¸ å½“å‰ç¯å¢ƒä»…æ”¯æŒ 1 ä¸ªå¹¶è¡Œè¿›ç¨‹ï¼ˆWindows/å•æ ¸ç¯å¢ƒï¼‰ã€‚")
            else:
                rdkit3d_n_jobs = st.slider("n_jobs", min_value=1, max_value=max_workers, value=min(max_workers, 2))

        keep_all_rows_3d = st.checkbox(
            "è·³è¿‡ç©ºå€¼/æ— æ•ˆSMILESå¹¶ä¿ç•™åŸå§‹è¡Œï¼ˆå¤±è´¥è¡Œç‰¹å¾ä¸ºNaNï¼‰",
            value=True,
            help="å¼€å¯åï¼šå³ä½¿æœ‰å¤§é‡ NaN/æ— æ•ˆ SMILESï¼Œä¹Ÿä¸ä¼šä¸¢è¡Œï¼›ä»…å¯¹æœ‰æ•ˆæ ·æœ¬è®¡ç®— 3D ç‰¹å¾ï¼Œå…¶ä½™æ ·æœ¬ç‰¹å¾ç•™ç©ºï¼ˆNaNï¼‰ã€‚å…³é—­åï¼šåªä¿ç•™æˆåŠŸæå–ç‰¹å¾çš„æ ·æœ¬è¡Œã€‚"
        )

        with st.expander("ğŸ§ª 3D Diagnostics / è‡ªæ£€ï¼ˆçœ‹è¿™é‡Œå®šä½å¤±è´¥åŸå› ï¼‰", expanded=False):
            try:
                import rdkit as _rdkit
                from rdkit.Chem import AllChem as _AllChem
                from core import molecular_features as _mf
                st.write(f"RDKit version: **{getattr(_rdkit, '__version__', 'unknown')}**")
                st.write(f"RDKIT_AVAILABLE in system: **{getattr(_mf, 'RDKIT_AVAILABLE', False)}**")
                st.write(f"Has ETKDGv3: **{hasattr(_AllChem, 'ETKDGv3')}**")
            except Exception as _e:
                st.error(f"Import self-test failed: {_e}")

            col_a, col_b = st.columns(2)
            with col_a:
                run_self_test = st.button("Run 3D self-test", key="rdkit3d_selftest_btn")
            with col_b:
                quick_n = st.number_input("Test first N rows", min_value=1, max_value=200, value=30, step=1, key="rdkit3d_selftest_n")

            if run_self_test:
                try:
                    from core.molecular_features import _rdkit3d_feature_worker, rdkit3d_debug_one
                    st.caption("1) ç”¨æœ€ç®€å•åˆ†å­ CCO æµ‹è¯• 3D pipelineï¼ˆåº”å½“æˆåŠŸï¼‰ã€‚")
                    out0 = _rdkit3d_feature_worker("CCO", coulomb_top_k=int(rdkit3d_coulomb_top_k))
                    st.write("CCO worker:", "âœ… OK" if out0 is not None else "âŒ None (failed)")
                    if out0 is None:
                        st.json(rdkit3d_debug_one("CCO", coulomb_top_k=int(rdkit3d_coulomb_top_k)))

                    st.caption("2) æµ‹è¯•ä½ å½“å‰é€‰æ‹©åˆ—çš„å‰ N è¡Œï¼ˆä»…ç»Ÿè®¡æˆåŠŸ/å¤±è´¥ï¼‰ã€‚")
                    try:
                        # å…¼å®¹ï¼šé¡µé¢ä¸»é€‰æ‹©å˜é‡é€šå¸¸å« smiles_colï¼›è‹¥ä¸å­˜åœ¨å†å°è¯•å…¶å®ƒåå­—
                        _col = (locals().get("smiles_col", None) or locals().get("resin_smiles_col", None) or st.session_state.get("selected_smiles_col", None))
                        if _col is None:
                            st.warning("æ— æ³•è‡ªåŠ¨ç¡®å®šè¦æµ‹è¯•çš„ SMILES åˆ—åï¼ˆè¯·å…ˆåœ¨é¡µé¢ä¸Šæ–¹é€‰æ‹© SMILES åˆ—å¹¶é‡æ–°å±•å¼€è‡ªæ£€ï¼‰ã€‚")
                            sample_list = []
                        else:
                            exists = bool((_col in df.columns)) if "df" in locals() else False
                            st.write(f"Testing column: `{_col}` | exists in df: **{exists}**")
                            sample_list = df[_col].tolist()[: int(quick_n)] if exists else []
                            if len(sample_list) == 0:
                                st.warning("æœªè·å–åˆ°ä»»ä½•æ ·æœ¬è¡Œç”¨äºæµ‹è¯•ï¼šå¯èƒ½ df æœªåŠ è½½ã€åˆ—åä¸åœ¨ df ä¸­ï¼Œæˆ–è¯¥åˆ—å…¨ä¸ºç©ºã€‚")
                    except Exception as _e:
                        st.error(f"åŠ è½½æ ·æœ¬å¤±è´¥: {_e}")
                        sample_list = []
                    ok_cnt = 0
                    first_fail = None
                    for _s in sample_list:
                        out = _rdkit3d_feature_worker(_s, coulomb_top_k=int(rdkit3d_coulomb_top_k))
                        if out is None and first_fail is None:
                            first_fail = _s
                        if out is not None:
                            ok_cnt += 1
                    st.write(f"First {len(sample_list)} rows: âœ… {ok_cnt} success / âŒ {len(sample_list)-ok_cnt} fail")
                    if first_fail is not None and ok_cnt == 0:
                        st.warning("æ‰€æœ‰æ ·æœ¬éƒ½å¤±è´¥äº†ï¼šä¸‹é¢ç»™å‡ºç¬¬ä¸€æ¡å¤±è´¥æ ·æœ¬çš„è¯¦ç»†è¯Šæ–­ï¼ˆè§£æ/åµŒå…¥é˜¶æ®µåŸå› ï¼‰ã€‚")
                        st.code(str(first_fail)[:300])
                        st.json(rdkit3d_debug_one(first_fail, coulomb_top_k=int(rdkit3d_coulomb_top_k)))
                except Exception as e:
                    st.error(f"Self-test failed: {e}")

=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4
    if "MLåŠ›åœºç‰¹å¾" in extraction_method:
        st.markdown("#### âš›ï¸ ML åŠ›åœº (ANI2x) å‚æ•°")
        st.info("è¯¥æ–¹æ³•ä¼šå…ˆç”Ÿæˆ3Dæ„è±¡ï¼Œå†ç”¨ ANI2x æ¨ç†èƒ½é‡/åŠ›ã€‚è¾ƒè€—æ—¶ï¼Œå»ºè®®è°ƒå¤§æ‰¹é‡å¹¶ä½¿ç”¨å¤šæ ¸CPUã€‚")
        col_a1, col_a2 = st.columns(2)
        with col_a1:
            ani_batch_size = st.selectbox("ANI Batch Size", [16, 32, 64, 128], index=2)
        with col_a2:
            max_workers = max(1, (os.cpu_count() or 1) - 1) if os.name != 'nt' else 1
            if max_workers <= 1:
                ani_cpu_workers = 1
                st.info("âš ï¸ å½“å‰ç¯å¢ƒä»…æ”¯æŒ 1 ä¸ª CPU workerï¼ˆWindows / å•æ ¸ç¯å¢ƒï¼‰ã€‚")
            else:
                ani_cpu_workers = st.slider(
                    "CPU Workers (3D Generation)",
                    min_value=1,
                    max_value=max_workers,
                    value=min(max_workers, ani_cpu_workers)
                )
        st.caption("æç¤ºï¼š3D æ„è±¡ç”Ÿæˆä½¿ç”¨å¤šè¿›ç¨‹ï¼›ANI æ¨ç†åœ¨ä¸»è¿›ç¨‹ä½¿ç”¨ Torch CPU å¤šçº¿ç¨‹ã€‚")
<<<<<<< HEAD

    # ============== [æ–°å¢ UI] Mordred å‚æ•° ==============
    if "Mordred" in extraction_method:
        st.markdown("#### ğŸ”¬ Mordred å‚æ•°")
        st.info("Mordred è¾“å‡º 1600+ æè¿°ç¬¦ï¼Œè€—æ—¶ä¸å†…å­˜éƒ½è¾ƒé«˜ã€‚å»ºè®®å…ˆç”¨è¾ƒå° batch_size æµ‹è¯•ã€‚")
        col_m1, col_m2 = st.columns(2)
        with col_m1:
            mordred_batch_size = st.number_input("batch_size", min_value=100, max_value=5000, value=int(mordred_batch_size), step=100)
        with col_m2:
            mordred_ignore_3d = st.checkbox("ignore_3D (æ¨è True)", value=bool(mordred_ignore_3d),
                                            help="True: ä»…è®¡ç®— 2D æè¿°ç¬¦ï¼Œæ›´ç¨³å®šã€æ›´å¿«ï¼›False: å…è®¸ 3D ç›¸å…³æè¿°ç¬¦ï¼ˆæ›´æ…¢ã€å¯¹æ„è±¡æ•æ„Ÿï¼‰")

=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4
# ============== [æ–°å¢ UI] TDA å‚æ•° ==============
    if "TDAæ‹“æ‰‘ç‰¹å¾" in extraction_method:
        st.markdown("#### ğŸ§© TDA(æŒç»­åŒè°ƒ) å‚æ•°")
        st.info("éœ€è¦å®‰è£… ripser/persimï¼špip install ripser persimã€‚TDA å°†æŠŠ 3D æ„è±¡ç‚¹äº‘è½¬ä¸º Betti0/1/2 çš„æ‹“æ‰‘ç»Ÿè®¡ç‰¹å¾ã€‚")

        col_t1, col_t2, col_t3, col_t4 = st.columns(4)
        with col_t1:
            tda_maxdim = st.selectbox("maxdim (0/1/2)", [0, 1, 2], index=2)
        with col_t2:
            tda_use_pim = st.checkbox("ä½¿ç”¨ Persistence Imageï¼ˆé«˜ç»´ï¼‰", value=False)
        with col_t3:
            tda_pim_pixels = st.selectbox("PIM åƒç´ è¾¹é•¿", [8, 10, 16, 20], index=1, disabled=(not tda_use_pim))
        with col_t4:
            tda_pim_spread = st.number_input("PIM spread", min_value=0.1, max_value=5.0, value=1.0, step=0.1,
                                             disabled=(not tda_use_pim))

        # é€Ÿåº¦/ç¨³å®šæ€§é€‰é¡¹ï¼ˆæ¨èé»˜è®¤å³å¯ï¼‰
        col_s1, col_s2, col_s3 = st.columns(3)
        with col_s1:
            tda_add_hs = st.checkbox("æ·»åŠ æ°¢åŸå­ï¼ˆæ›´æ…¢ï¼‰", value=False)
        with col_s2:
            tda_do_optimize = st.checkbox("åŠ›åœºä¼˜åŒ–MMFF/UFFï¼ˆæ›´æ…¢ï¼‰", value=False)
        with col_s3:
            tda_max_points = st.selectbox("æœ€å¤§ç‚¹æ•°(ä¸‹é‡‡æ ·åŠ é€Ÿ)", ["ä¸é™åˆ¶", 64, 128, 256, 512, 1024], index=3)
        st.caption("æç¤ºï¼šTDA é»˜è®¤åªç”¨é‡åŸå­åæ ‡ï¼›é€šå¸¸ä¸éœ€è¦åŠ æ°¢/åŠ›åœºä¼˜åŒ–ã€‚ç‚¹æ•°è¶Šå¤§è¶Šæ…¢ã€‚")

    # å¹¶è¡Œç‰ˆå‚æ•°
    if "å¹¶è¡Œç‰ˆ" in extraction_method and OPTIMIZED_EXTRACTOR_AVAILABLE:
        col1, col2 = st.columns(2)
        with col1:
            n_jobs = st.slider("å¹¶è¡Œè¿›ç¨‹æ•°", 1, mp.cpu_count(), mp.cpu_count() // 2)
        with col2:
            batch_size = st.number_input("æ‰¹å¤„ç†å¤§å°", 100, 5000, 1000)

    st.markdown("---")

    # [ä¿®æ”¹] æŒ‰é’®åŒºåŸŸï¼šå¢åŠ æ¸…é™¤æŒ‰é’®
    col_btn1, col_btn2 = st.columns([1, 4])

    with col_btn1:
        run_extraction = st.button("ğŸš€ å¼€å§‹æå–åˆ†å­ç‰¹å¾", type="primary")

    with col_btn2:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤å·²æå–ç‰¹å¾"):
            # æ£€æŸ¥æ˜¯å¦æœ‰è®°å½•çš„ç‰¹å¾åˆ—å
            if st.session_state.get('molecular_feature_names'):
                current_df = st.session_state.processed_data
                # æ‰¾å‡ºå½“å‰æ•°æ®ä¸­å®é™…å­˜åœ¨çš„ç‰¹å¾åˆ—
                cols_to_remove = [c for c in st.session_state.molecular_feature_names if c in current_df.columns]

                if cols_to_remove:
                    # ä» processed_data ä¸­ç§»é™¤è¿™äº›åˆ—
                    st.session_state.processed_data = current_df.drop(columns=cols_to_remove)
                    # é‡ç½®çŠ¶æ€
                    st.session_state.molecular_features = None
                    st.session_state.molecular_feature_names = []
                    st.success(f"âœ… å·²æˆåŠŸæ¸…é™¤ {len(cols_to_remove)} ä¸ªåˆ†å­ç‰¹å¾åˆ—ï¼")
                    st.rerun()
                else:
                    st.warning("âš ï¸ æ•°æ®è¡¨ä¸­æœªæ‰¾åˆ°å¯æ¸…é™¤çš„ç‰¹å¾åˆ—ï¼ˆå¯èƒ½å·²è¢«ä¿®æ”¹ï¼‰ã€‚")

            elif st.session_state.get('molecular_features') is not None:
                # å…œåº•é€»è¾‘ï¼šå¦‚æœæœ‰ç‰¹å¾æ•°æ®ä½†æ²¡è®°å½•åˆ—åï¼ˆæ—§çŠ¶æ€ï¼‰ï¼Œå¼ºåˆ¶é‡ç½®çŠ¶æ€
                st.session_state.molecular_features = None
                st.warning("âš ï¸ ç‰¹å¾çŠ¶æ€å·²é‡ç½®ï¼Œä½†æ— æ³•è‡ªåŠ¨ä»æ•°æ®è¡¨ä¸­ç§»é™¤å…·ä½“åˆ—ï¼ˆå»ºè®®é‡æ–°ä¸Šä¼ æ•°æ®ï¼‰ã€‚")
                st.rerun()
            else:
                st.info("â„¹ï¸ å½“å‰æ²¡æœ‰å·²æå–çš„ç‰¹å¾ã€‚")

    # æ‰§è¡Œæå–é€»è¾‘
    if run_extraction:
        # -----------------------------
        # 1) ç”Ÿæˆâ€œå¯è¢« RDKit è§£æâ€çš„ SMILES åˆ—è¡¨
        #    - å•åˆ—å¤šç»„åˆ†ï¼šå°† ';' ç­‰åˆ†éš”ç¬¦è½¬æ¢ä¸º '.'
        #    - å¤šåˆ—å¤šç»„åˆ†ï¼šåˆå¹¶å¤šåˆ—ä¸º '.' è¿æ¥çš„å¤šç‰‡æ®µ SMILES
        # -----------------------------
        resin_smiles_series, resin_ncomp = _combine_components(df, resin_component_cols)
        smiles_list = resin_smiles_series.tolist()

        # 2) å›ºåŒ–å‰‚ï¼ˆå¯é€‰ï¼‰â€”â€”åŒæ ·æ”¯æŒå¤šç»„åˆ†
        hardener_list = None
        hardener_ncomp = None
        if hardener_col:
            if hardener_component_cols is None:
                hardener_component_cols = [hardener_col]

            # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ä½¿ç”¨ UI ä¸­ç”Ÿæˆçš„ hardener_component_cols åˆ—è¡¨
            # ç§»é™¤äº†åŸæœ‰çš„è‡ªåŠ¨æ­£åˆ™è¦†ç›–é€»è¾‘ï¼Œå®Œå…¨å°Šé‡ç”¨æˆ·åœ¨ UI ä¸Šçš„é€‰æ‹©

            hardener_smiles_series, hardener_ncomp = _combine_components(df, hardener_component_cols)
            hardener_list = hardener_smiles_series.tolist()
            if resin_mix_mode:
                # ä»…åœ¨å¯ç”¨å¤šç»„åˆ†æ¨¡å¼æ—¶æ‰å±•ç¤º/ä½¿ç”¨å›ºåŒ–å‰‚å¤šåˆ—åˆå¹¶é€»è¾‘ï¼Œé¿å… UI è¿‡å¤æ‚
                st.caption("ï¼ˆæç¤ºï¼‰å›ºåŒ–å‰‚ä¹Ÿæ”¯æŒå¤šç»„åˆ†ï¼šå¦‚æœæœ‰ hardener_col_1/2/â€¦ å¯åœ¨ä¸‹æ–¹è‡ªåŠ¨åˆå¹¶ã€‚")

            # è‡ªåŠ¨åˆå¹¶ï¼šå¦‚æœå­˜åœ¨ hardener_col_\d åˆ—ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨å®ƒä»¬ï¼ˆç”¨æˆ·æœªæ˜¾å¼å¤šé€‰æ—¶ï¼‰
            pattern_h = re.compile(rf"^{re.escape(hardener_col)}_\d+$")
            auto_h_cols = [c for c in text_cols if pattern_h.match(c)]
            if auto_h_cols:
                def _tail_num_h(colname: str):
                    try:
                        return int(colname.split('_')[-1])
                    except:
                        return 0
                auto_h_cols = sorted(auto_h_cols, key=_tail_num_h)
                hardener_component_cols = auto_h_cols

            hardener_smiles_series, hardener_ncomp = _combine_components(df, hardener_component_cols)
            hardener_list = hardener_smiles_series.tolist()

        try:
            progress_bar = st.progress(0)
            status_text = st.empty()

            # --- [æ–°å¢] å›ºåŒ–å‰‚èåˆï¼šå¯é€‰å°† Resin ä¸ Hardener SMILES æ‹¼æ¥åç”¨äºæ‰€æœ‰åˆ†å­ç‰¹å¾ ---
            smiles_list_input = smiles_list
            if hardener_list and isinstance(hardener_fusion_mode, str) and hardener_fusion_mode.startswith("æ‹¼æ¥SMILES"):
                def _safe_smiles(x):
                    if x is None or (isinstance(x, float) and np.isnan(x)):
                        return ""
                    s = str(x).strip()
                    return "" if s.lower() == "nan" else s

                smiles_list_input = []
                for r, h in zip(smiles_list, hardener_list):
                    rs = _safe_smiles(r)
                    hs = _safe_smiles(h)
                    if rs and hs:
                        smiles_list_input.append(f"{rs}.{hs}")
                    elif rs:
                        smiles_list_input.append(rs)
                    elif hs:
                        smiles_list_input.append(hs)
                    else:
                        smiles_list_input.append(np.nan)

            # --- [é€»è¾‘ä¿®æ”¹] åˆ†å‘æå–ä»»åŠ¡ ---
            if "åˆ†å­æŒ‡çº¹" in extraction_method:
                from core.molecular_features import FingerprintExtractor

                # æç¤ºç”¨æˆ·å½“å‰æ¨¡å¼
                mode_str = "åŒç»„åˆ†æ‹¼æ¥" if hardener_list else "å•ç»„åˆ†"
                status_text.text(f"æ­£åœ¨æå– {fp_type} æŒ‡çº¹ ({mode_str}æ¨¡å¼)...")

                extractor = FingerprintExtractor()
                # ä¼ å…¥ smiles_list_2 (å›ºåŒ–å‰‚)
<<<<<<< HEAD
                import inspect
                _kwargs = dict(
                    smiles_list_2=hardener_list,
                    fp_type=fp_type, n_bits=fp_bits, radius=fp_radius,
                    use_chirality=bool(fp_use_chirality),
                    use_features=bool(fp_use_features),
                )
                # å…¼å®¹æ—§ç‰ˆæœ¬ï¼šè‹¥ FingerprintExtractor æœªå®ç°è¯¥å‚æ•°ï¼Œåˆ™è‡ªåŠ¨å¿½ç•¥ï¼Œé¿å…æŠ¥é”™
                try:
                    if "drop_all_zero_bits" in inspect.signature(extractor.smiles_to_fingerprints).parameters:
                        _kwargs["drop_all_zero_bits"] = bool(drop_all_zero_bits)
                except Exception:
                    pass

                features_df, valid_indices = extractor.smiles_to_fingerprints(
                    smiles_list,
                    **_kwargs
=======
                features_df, valid_indices = extractor.smiles_to_fingerprints(
                    smiles_list,
                    smiles_list_2=hardener_list,
                    fp_type=fp_type, n_bits=fp_bits, radius=fp_radius
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4
                )

            elif "æ ‡å‡†ç‰ˆ" in extraction_method:
                status_text.text("æ­£åœ¨ä½¿ç”¨RDKitæ ‡å‡†ç‰ˆæå–...")
                extractor = AdvancedMolecularFeatureExtractor()
                features_df, valid_indices = extractor.smiles_to_rdkit_features(smiles_list_input)

            elif "å¹¶è¡Œç‰ˆ" in extraction_method:
                if OPTIMIZED_EXTRACTOR_AVAILABLE:
                    status_text.text(f"æ­£åœ¨ä½¿ç”¨RDKitå¹¶è¡Œç‰ˆæå– ({n_jobs}è¿›ç¨‹)...")
                    extractor = OptimizedRDKitFeatureExtractor(n_jobs=n_jobs, batch_size=batch_size)
                    features_df, valid_indices = extractor.smiles_to_rdkit_features(smiles_list_input)
                else:
                    st.warning("å¹¶è¡Œç‰ˆä¸å¯ç”¨ï¼Œå›é€€åˆ°æ ‡å‡†ç‰ˆ")
                    extractor = AdvancedMolecularFeatureExtractor()
                    features_df, valid_indices = extractor.smiles_to_rdkit_features(smiles_list_input)

            elif "å†…å­˜ä¼˜åŒ–ç‰ˆ" in extraction_method:
                status_text.text("æ­£åœ¨ä½¿ç”¨RDKitå†…å­˜ä¼˜åŒ–ç‰ˆ...")
                extractor = MemoryEfficientRDKitExtractor()
                features_df, valid_indices = extractor.smiles_to_rdkit_features(smiles_list_input)

            elif "Mordred" in extraction_method:
                status_text.text("æ­£åœ¨ä½¿ç”¨Mordredæå–...")
                extractor = AdvancedMolecularFeatureExtractor()
<<<<<<< HEAD
                features_df, valid_indices = extractor.smiles_to_mordred(smiles_list_input, batch_size=int(mordred_batch_size), ignore_3D=bool(mordred_ignore_3d))
=======
                features_df, valid_indices = extractor.smiles_to_mordred(smiles_list_input)
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4

            elif "3Dæ„è±¡" in extraction_method:
                from core.molecular_features import RDKit3DDescriptorExtractor
                status_text.text("æ­£åœ¨æå–RDKit 3Dæ„è±¡æè¿°ç¬¦...")
<<<<<<< HEAD
                extractor = RDKit3DDescriptorExtractor(coulomb_top_k=int(rdkit3d_coulomb_top_k))
                features_df, valid_indices = extractor.smiles_to_3d_descriptors(smiles_list_input, n_jobs=rdkit3d_n_jobs)
=======
                extractor = RDKit3DDescriptorExtractor()
                features_df, valid_indices = extractor.smiles_to_3d_descriptors(smiles_list_input)
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4

            elif "TDAæ‹“æ‰‘ç‰¹å¾" in extraction_method:
                from core.tda_features import PersistentHomologyFeatureExtractor, TDAConfig
                status_text.text("æ­£åœ¨æå– TDA æ‹“æ‰‘ç‰¹å¾ï¼ˆæŒç»­åŒè°ƒï¼‰...")

                config = TDAConfig(
                    maxdim=int(tda_maxdim),
                    use_persistence_image=bool(tda_use_pim),
                    pim_size=(int(tda_pim_pixels), int(tda_pim_pixels)),
                    pim_spread=float(tda_pim_spread),
                    max_points=None if str(tda_max_points) == "ä¸é™åˆ¶" else int(tda_max_points),
                    do_optimize=bool(tda_do_optimize),
                )
                extractor = PersistentHomologyFeatureExtractor(config)
                if not getattr(extractor, "AVAILABLE", False):
                    st.error("âŒ æœªæ£€æµ‹åˆ° ripser/persimï¼Œè¯·å…ˆå®‰è£…ï¼špip install ripser persim")
                    return

                features_df, valid_indices = extractor.smiles_to_tda_features(smiles_list_input, add_hs=bool(tda_add_hs))

            elif "Transformer Embedding" in extraction_method:
                from core.molecular_features import SmilesTransformerEmbeddingExtractor
                oplog(f"Running Transformer embedding: model={lm_model_name}, pooling={lm_pooling}, max_length={lm_max_length}, batch={lm_batch_size}")
                oplog(f"SMILES sources (resin_component_cols): {resin_component_cols}")
                if hardener_component_cols is not None:
                    oplog(f"SMILES sources (hardener_component_cols): {hardener_component_cols}")
                oplog(f"Hardener fusion mode: {hardener_fusion_mode}")
                status_text.text("æ­£åœ¨åŠ è½½é¢„è®­ç»ƒTransformerå¹¶æå–Embedding...")
                extractor = SmilesTransformerEmbeddingExtractor(
                    model_name=lm_model_name,
                    pooling=lm_pooling,
                    max_length=lm_max_length
                )
                if not getattr(extractor, "AVAILABLE", False):
                    st.error("âŒ æœªæ£€æµ‹åˆ° transformersï¼Œè¯·å…ˆå®‰è£…ï¼špip install transformers")
                    st.stop()
                features_df, valid_indices = extractor.smiles_to_embeddings(smiles_list_input, batch_size=lm_batch_size)

            elif "å›¾ç¥ç»ç½‘ç»œ" in extraction_method:
                status_text.text("æ­£åœ¨æå–å›¾ç»“æ„ç‰¹å¾...")
                extractor = AdvancedMolecularFeatureExtractor()
                features_df, valid_indices = extractor.smiles_to_graph_features(smiles_list)

            elif "MLåŠ›åœº" in extraction_method:
                from core.molecular_features import MLForceFieldExtractor
                oplog("Running ML force field features (ANI2x): 3D generation + ANI inference")
                status_text.text("æ­£åœ¨è®¡ç®—ANIåŠ›åœºç‰¹å¾...")
                extractor = MLForceFieldExtractor()
                if not extractor.AVAILABLE:
                    st.error("TorchANI æœªå®‰è£…")
                    return
                oplog(f"ANI params: batch_size={ani_batch_size}, cpu_workers(3D)={ani_cpu_workers}")
                features_df, valid_indices = extractor.smiles_to_ani_features(smiles_list_input, batch_size=ani_batch_size, n_jobs=ani_cpu_workers)
            elif "FGD" in extraction_method:
                from core.molecular_features import FGDFeatureExtractor
                status_text.text("æ­£åœ¨æ‰§è¡Œ FGD ç»“æ„åˆ†ç±»ä¸ç¼–ç ...")

                extractor = FGDFeatureExtractor()
                # ä½¿ç”¨ smiles_list_input (è¿™æ˜¯ä¸Šé¢å·²ç»å¤„ç†è¿‡å¤šç»„åˆ†æ‹¼æ¥çš„å˜é‡)
                features_df, valid_indices = extractor.categorize_smiles(smiles_list_input)

                if not features_df.empty:
                    # --- [å…³é”®æ­¥éª¤] è‡ªåŠ¨ One-Hot ç¼–ç  ---
                    # æ–‡çŒ®ä¸­ FGD å¿…é¡»é…åˆ OHE (One-Hot Encoding) ä½¿ç”¨
                    st.info("â„¹ï¸ å·²æå– FGD ç±»åˆ«ç‰¹å¾ï¼Œæ­£åœ¨è‡ªåŠ¨æ‰§è¡Œ One-Hot ç¼–ç ä»¥é€‚é…æ¨¡å‹...")

                    features_df = pd.get_dummies(
                        features_df,
                        columns=["FGD_Substrate", "FGD_Group"],
                        prefix=["Substrate", "Group"],
                        dtype=int
                    )
                    # -----------------------------------


            elif "ç¯æ°§æ ‘è„‚" in extraction_method:
                from core.molecular_features import EpoxyDomainFeatureExtractor
                status_text.text("æ­£åœ¨è®¡ç®—ç¯æ°§æ ‘è„‚é¢†åŸŸç‰¹å¾...")
                if hardener_col is None:
                    st.error("è¯·é€‰æ‹©å›ºåŒ–å‰‚åˆ—ï¼")
                    return

                phr_list = None
                if phr_col and phr_col != "æ—  (å‡è®¾ç†æƒ³é…æ¯”)":
                    phr_list = df[phr_col].tolist()

                extractor = EpoxyDomainFeatureExtractor()
                features_df, valid_indices = extractor.extract_features(smiles_list, hardener_list, phr_list, stoich_mode)

            progress_bar.progress(100)

            # --- åˆå¹¶ç»“æœé€»è¾‘ ---
            if len(features_df) > 0:
                st.session_state.molecular_features = features_df
                prefix = f"{smiles_col}_"  # default
                # âœ… æ›´æ¸…æ™°çš„å‘½åï¼šå¤šç»„åˆ†/æ‹¼æ¥æ¨¡å¼ä¸å†ä½¿ç”¨â€œç¬¬ä¸€åˆ—åâ€ä½œä¸ºå‰ç¼€
                try:
                    if hardener_list and isinstance(hardener_fusion_mode, str) and hardener_fusion_mode.startswith("æ‹¼æ¥SMILES"):
                        prefix = "resin_hardener_"
                    elif resin_mix_mode and isinstance(resin_component_cols, list) and len(resin_component_cols) > 1:
                        prefix = f"multi_smiles_{len(resin_component_cols)}_"
                except Exception:
                    pass
                features_df = features_df.add_prefix(prefix)

<<<<<<< HEAD
                # -----------------------------
                # åˆå¹¶ç­–ç•¥ï¼š
                # - keep_all_rows_3d=Trueï¼šä¿ç•™åŸå§‹æ‰€æœ‰è¡Œï¼›ä»…å¯¹ valid_indices å¡«å……ç‰¹å¾ï¼Œå…¶ä½™ä¸º NaNï¼ˆæ¨èï¼Œé€‚åˆå¤§é‡ç©ºå€¼åœºæ™¯ï¼‰
                # - å¦åˆ™ï¼šä»…ä¿ç•™æˆåŠŸæå–ç‰¹å¾çš„æ ·æœ¬è¡Œï¼ˆåŸé€»è¾‘ï¼‰
                # -----------------------------
                features_df = features_df.reset_index(drop=True)

                if 'keep_all_rows_3d' in locals() and keep_all_rows_3d:
                    base_df = df.reset_index(drop=True)

                    # é˜²æ­¢åˆ—åå†²çªï¼šå¦‚æœæ–°ç‰¹å¾åå·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤æ—§çš„
                    cols_to_drop = [col for col in features_df.columns if col in base_df.columns]
                    if cols_to_drop:
                        base_df = base_df.drop(columns=cols_to_drop)

                    # æ„å»ºå…¨é‡ç‰¹å¾è¡¨å¹¶æŒ‰ valid_indices å›å¡«
                    full_feat = pd.DataFrame(index=range(len(base_df)), columns=features_df.columns, dtype=float)
                    if valid_indices:
                        full_feat.iloc[valid_indices, :] = features_df.values

                    merged_df = pd.concat([base_df, full_feat], axis=1)
                else:
                    df_valid = df.iloc[valid_indices].reset_index(drop=True)

                    # é˜²æ­¢åˆ—åå†²çªï¼šå¦‚æœæ–°ç‰¹å¾åå·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤æ—§çš„
                    cols_to_drop = [col for col in features_df.columns if col in df_valid.columns]
                    if cols_to_drop:
                        df_valid = df_valid.drop(columns=cols_to_drop)

                    merged_df = pd.concat([df_valid, features_df], axis=1)
=======
                df_valid = df.iloc[valid_indices].reset_index(drop=True)
                features_df = features_df.reset_index(drop=True)

                # é˜²æ­¢åˆ—åå†²çªï¼šå¦‚æœæ–°ç‰¹å¾åå·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤æ—§çš„
                cols_to_drop = [col for col in features_df.columns if col in df_valid.columns]
                if cols_to_drop:
                    df_valid = df_valid.drop(columns=cols_to_drop)

                merged_df = pd.concat([df_valid, features_df], axis=1)
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4

                # å¯é€‰ï¼šè¿½åŠ ç»„åˆ†æ•°é‡ç‰¹å¾
                if resin_mix_mode and add_component_count_features:
                    merged_df[f"{smiles_col}_resin_n_components"] = resin_ncomp.iloc[valid_indices].reset_index(drop=True)
                    if hardener_ncomp is not None:
                        merged_df[f"{smiles_col}_hardener_n_components"] = hardener_ncomp.iloc[valid_indices].reset_index(drop=True)

                merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]
                st.session_state.processed_data = merged_df
                # [æ–°å¢] ä¿å­˜ç‰¹å¾åˆ—ååˆ° Session Stateï¼Œä»¥ä¾¿åç»­æ¸…é™¤
                st.session_state.molecular_feature_names = features_df.columns.tolist()
                st.success(f"âœ… æˆåŠŸæå– {len(features_df)} ä¸ªæ ·æœ¬çš„ {features_df.shape[1]} ä¸ªåˆ†å­ç‰¹å¾")

                log_fe_step(
                    operation="åˆ†å­ç‰¹å¾æå–",
                    description=f"æ–¹æ³•: {extraction_method} / åˆ—: {smiles_col}",
                    params={"method": extraction_method, "smiles_col": smiles_col, "n_features": int(features_df.shape[1]), "n_samples": int(len(valid_indices))},
                    input_df=df,
                    output_df=merged_df,
                    features_added=features_df.columns.tolist(),
                    message=f"æ–°å¢åˆ†å­ç‰¹å¾ {features_df.shape[1]} åˆ—ï¼Œæ•°æ®åˆ—æ•° {df.shape[1]} â†’ {merged_df.shape[1]}"
                )

                # ç»“æœç»Ÿè®¡
                col1, col2, col3 = st.columns(3)
                col1.metric("æœ‰æ•ˆæ ·æœ¬", len(valid_indices))
                col2.metric("ç‰¹å¾æ•°é‡", features_df.shape[1])
                col3.metric("åŒç»„åˆ†æ¨¡å¼", "æ˜¯" if hardener_list else "å¦")

                st.markdown("### ğŸ“‹ ç‰¹å¾é¢„è§ˆ")
                st.dataframe(features_df.head(), use_container_width=True)
            else:
<<<<<<< HEAD
                st.error("âŒ æœªèƒ½æå–ä»»ä½•ç‰¹å¾ï¼šå½“å‰é€‰æ‹©çš„ SMILES åˆ—å¯èƒ½å…¨éƒ¨ä¸ºç©º/æ— æ•ˆï¼Œæˆ– 3D æ„è±¡ç”Ÿæˆå…¨éƒ¨å¤±è´¥ã€‚")
                # é¢å¤–è¯Šæ–­ï¼šå¿«é€Ÿæ£€æŸ¥å‰ 200 æ¡æ˜¯å¦èƒ½è¢« RDKit è§£æï¼ˆä¸åš 3Dï¼‰
                try:
                    ok, checked, bad = _quick_rdkit_parse_stats(smiles_list_input, max_check=200)
                    st.info(f"RDKit è§£æè‡ªæ£€ï¼šæ£€æŸ¥ {checked} æ¡ä¸­ï¼Œæœ‰ {ok} æ¡è‡³å°‘åŒ…å«ä¸€ä¸ªå¯è§£æç‰‡æ®µã€‚")
                    if ok == 0 and bad:
                        st.warning("ç¤ºä¾‹ï¼ˆå¯èƒ½ä¸æ˜¯åˆæ³• SMILESï¼‰ï¼š\n- " + "\n- ".join(bad))
                        st.caption("è‹¥è¿™äº›å­—ç¬¦ä¸²çœ‹èµ·æ¥åƒåç§°/é…æ–¹é”®ï¼ˆå«ä¸­æ–‡/å•ä½/ç‰¹æ®Šåˆ†éš”ç¬¦ç­‰ï¼‰ï¼Œè¯·æ”¹é€‰çœŸæ­£çš„ SMILES åˆ—ï¼›æˆ–å…ˆæ¸…æ´—åå†åš 3Dã€‚")
                        st.caption("å¦‚æœç¤ºä¾‹æ˜¯æ­£å¸¸ SMILESï¼Œä½†ä»å…¨éƒ¨å¤±è´¥ï¼Œå¸¸è§åŸå› æ˜¯ RDKit ç‰ˆæœ¬è¿‡æ—§ï¼ˆä¸æ”¯æŒ ETKDGv3ï¼‰ã€‚æœ¬ç‰ˆæœ¬å·²è‡ªåŠ¨å›é€€ ETKDGv2/ETKDGï¼›ä¹Ÿå»ºè®®å‡çº§ rdkitã€‚")
                except Exception:
                    pass

                st.info(f"æ€»è¡Œæ•°={len(df)}ï¼Œæ ‘è„‚/ä¸»ä½“ SMILES éç©ºæ•°â‰ˆ{pd.Series(smiles_list_input).replace(['nan','NaN','<NA>'], np.nan).notna().sum()}ï¼ˆä»…ç²—ç•¥ç»Ÿè®¡ï¼‰")
                st.caption("å»ºè®®ï¼š1) ç¡®è®¤é€‰æ‹©äº†æ­£ç¡®çš„ SMILES åˆ—ï¼›2) å…ˆæŠŠ n_jobs è°ƒåˆ° 1ï¼›3) å…ˆç”¨å°‘é‡æ ·æœ¬æµ‹è¯•ï¼›4) å¤šç»„åˆ†/å«ç›/å«é‡‘å±ä½“ç³»æ›´æ˜“å¤±è´¥ã€‚")
=======
                st.error("âŒ æœªèƒ½æå–ä»»ä½•ç‰¹å¾ï¼Œè¯·æ£€æŸ¥SMILESæ ¼å¼")
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4

        except Exception as e:
            st.error(f"âŒ æå–å¤±è´¥: {str(e)}")
            st.code(traceback.format_exc())


# ============================================================
# é¡µé¢ï¼šç‰¹å¾é€‰æ‹©ï¼ˆå®Œæ•´ç‰ˆï¼‰
# ============================================================
def page_feature_selection():
    """ç‰¹å¾é€‰æ‹©é¡µé¢ - è°ƒç”¨å®Œæ•´çš„show_robust_feature_selection"""
    st.title("ğŸ¯ ç‰¹å¾é€‰æ‹©")

    # è°ƒç”¨å®Œæ•´çš„ç‰¹å¾é€‰æ‹©UI
    show_robust_feature_selection()



# ============================================================
# é¡µé¢ï¼šæ¨¡å‹è®­ç»ƒï¼ˆæ›´æ–°ç‰ˆï¼šå«è¡¨æ ¼ã€ä¸€é”®è¾“å‡ºã€å›¾ç‰‡é˜²æŠ–ï¼‰
# ============================================================
def page_model_training():
    """æ¨¡å‹è®­ç»ƒé¡µé¢ï¼ˆç¨³å¥ç‰ˆï¼šæ”¯æŒåˆ†å±‚/åˆ†ç»„åˆ’åˆ† + Repeated KFold CVï¼‰"""
    st.title("ğŸ¤– æ¨¡å‹è®­ç»ƒ")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    if not st.session_state.feature_cols:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ç‰¹å¾é€‰æ‹©é¡µé¢é€‰æ‹©ç‰¹å¾")
        return

    # æ•°æ®æº
    df_all = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    df = df_all.copy()

    # --- [P0-2] Tg å£å¾„è¿‡æ»¤ï¼šå»ºè®®åˆ†æ–¹æ³•å»ºæ¨¡ ---
    target_col = st.session_state.target_col
    if target_col and isinstance(target_col, str) and ("tg" in target_col.lower()) and ("tg_method" in df.columns):
        st.markdown("### ğŸ§ª Tg å£å¾„è¿‡æ»¤ï¼ˆtg_methodï¼‰")
        methods = df["tg_method"].dropna().astype(str).unique().tolist()
        methods = sorted(methods)
        method_options = ["å…¨éƒ¨"] + methods

        # é»˜è®¤ä¼˜å…ˆ DSCï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™å…¨éƒ¨
        default_method = "å…¨éƒ¨"
        for prefer in ["DSC", "DMA-tanÎ´", "DMA-tanÎ´ (tanÎ´)", "DMA", "DSC (onset)"]:
            if prefer in methods:
                default_method = prefer
                break

        selected_method = st.selectbox(
            "é€‰æ‹©è®­ç»ƒæ•°æ®çš„ tg_method",
            options=method_options,
            index=method_options.index(default_method) if default_method in method_options else 0,
            help="ä¸åŒæµ‹è¯•å£å¾„ï¼ˆDSC / DMA-tanÎ´ ç­‰ï¼‰ä¼šé€ æˆç³»ç»Ÿåå·®ï¼›å»ºè®®åˆ†å£å¾„å»ºæ¨¡è·å¾—æ›´ç¨³å®šçš„æ³›åŒ–ã€‚"
        )

        if selected_method != "å…¨éƒ¨":
            before_n = len(df)
            df = df[df["tg_method"].astype(str) == str(selected_method)].copy()
            st.info(f"å·²è¿‡æ»¤ tg_method={selected_method}ï¼š{before_n} â†’ {len(df)} è¡Œ")

    # æ„é€  X / y
    try:
        X = df[st.session_state.feature_cols]
        y = df[target_col]
    except Exception as e:
        st.error(f"âŒ æ„é€ è®­ç»ƒæ•°æ®å¤±è´¥ï¼š{e}")
        return

    trainer = EnhancedModelTrainer()

    # åˆ†ç»„åˆ’åˆ†å¯ç”¨æ€§æ£€æµ‹
    has_resin = "resin_smiles" in df.columns
    has_hardener = "curing_agent_smiles" in df.columns
    group_key_options = []
    if has_resin and has_hardener:
        group_key_options = ["resin_smiles + curing_agent_smiles", "resin_smiles", "curing_agent_smiles"]
    elif has_resin:
        group_key_options = ["resin_smiles"]
    elif has_hardener:
        group_key_options = ["curing_agent_smiles"]

    col1, col2 = st.columns([1, 2])

    with col1:
        st.markdown("### ğŸ“¦ æ¨¡å‹é€‰æ‹©")

        # è·å–æ¨¡å‹ç›®å½•ï¼ˆåŒ…å«å¯ç”¨æ€§/ç¼ºå¤±ä¾èµ–åŸå› ï¼‰ï¼›å…¼å®¹æ—§ç‰ˆæœ¬ trainer
        if hasattr(trainer, "get_model_catalog"):
            model_catalog = trainer.get_model_catalog()
            model_options = trainer.get_available_models(include_unavailable=True)
        else:
            # æ—§ç‰ˆæœ¬ï¼šä»…æä¾›å¯ç”¨æ¨¡å‹
            model_options = trainer.get_available_models()
            model_catalog = {m: {"available": True, "reason": ""} for m in model_options}

            # å…¼å®¹ï¼šå³ä½¿å½“å‰ç¯å¢ƒæœªå®‰è£… TensorFlowï¼Œä¹Ÿæ˜¾ç¤º TFS å…¥å£ï¼ˆè®­ç»ƒä¼šè¢«ç¦ç”¨å¹¶æç¤ºå®‰è£…ï¼‰
            if "TensorFlow Sequential" not in model_options:
                model_options = model_options + ["TensorFlow Sequential"]
                model_catalog["TensorFlow Sequential"] = {
                    "available": bool(TENSORFLOW_AVAILABLE),
                    "reason": "" if TENSORFLOW_AVAILABLE else "æœªå®‰è£… TensorFlowï¼ˆpip install tensorflowï¼‰"
                }

        def _fmt_model_name(n: str) -> str:
            label = "TFSï¼ˆTensorFlow Sequentialï¼‰" if n == "TensorFlow Sequential" else n
            meta = model_catalog.get(n, {})
            if meta.get("available", True):
                return f"{label} âœ…"
            return f"{label} â›”"

        model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", model_options, format_func=_fmt_model_name)

        meta = model_catalog.get(model_name, {"available": True, "reason": ""})
        disable_train = (not meta.get("available", True))
        if disable_train:
            reason = meta.get("reason") or "å½“å‰ç¯å¢ƒç¼ºå°‘ä¾èµ–"
            st.warning(f"è¯¥æ¨¡å‹å½“å‰ä¸å¯è®­ç»ƒï¼š{reason}")

        st.markdown("### âš™ï¸ è®­ç»ƒè®¾ç½®")
        test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.4, 0.2)
        random_state = st.number_input("éšæœºç§å­", 0, 1000000, 42)

<<<<<<< HEAD
        # å¹¶è¡Œè®­ç»ƒæ ¸æ•°ï¼ˆå¯¹æ”¯æŒ n_jobs/thread_count çš„ç®—æ³•ç”Ÿæ•ˆï¼›å…¶å®ƒç®—æ³•è‡ªåŠ¨å¿½ç•¥ï¼‰
        cpu_total = os.cpu_count() or 1
        core_opts = ["Auto (all cores)"] + [str(i) for i in range(1, min(cpu_total, 64) + 1)]
        core_sel = st.selectbox("è®­ç»ƒå¹¶è¡Œæ ¸æ•°", core_opts, index=0,
                              help="ä¼šåº”ç”¨åˆ° RandomForest/ExtraTrees/XGBoost/LightGBM/CatBoost/éƒ¨åˆ†çº¿æ€§æ¨¡å‹ç­‰ã€‚Auto=ä½¿ç”¨å…¨éƒ¨CPUæ ¸å¿ƒã€‚")
        train_n_jobs = -1 if core_sel.startswith("Auto") else int(core_sel)

=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4
        # --- [P0-3 / P1-1] åˆ’åˆ†ç­–ç•¥ ---
        st.markdown("### ğŸ§© åˆ’åˆ†ç­–ç•¥")
        split_ui = st.selectbox(
            "é€‰æ‹©åˆ’åˆ†ç­–ç•¥",
            options=["éšæœºåˆ’åˆ†", "åˆ†å±‚åˆ’åˆ†(å›å½’åˆ†ç®±)", "æŒ‰é…æ–¹åˆ†ç»„åˆ’åˆ†"],
            index=1 if len(df) >= 50 else 0,
            help="å°æ ·æœ¬/è·¨åº¦å¤§å›å½’å»ºè®®ä½¿ç”¨â€œåˆ†å±‚åˆ’åˆ†â€ï¼›çœŸå®é…æ–¹æ³›åŒ–å»ºè®®ä½¿ç”¨â€œæŒ‰é…æ–¹åˆ†ç»„åˆ’åˆ†â€ã€‚"
        )

        split_strategy = "random"
        n_bins = 10
        groups = None
        group_key = None

        if split_ui.startswith("åˆ†å±‚"):
            split_strategy = "stratified"
            n_bins = st.slider("åˆ†å±‚åˆ†ç®±æ•°ï¼ˆå»ºè®® 8~12ï¼‰", 4, 20, 10)
        elif split_ui.startswith("æŒ‰é…æ–¹"):
            split_strategy = "group"
            if not group_key_options:
                st.warning("âš ï¸ å½“å‰æ•°æ®ç¼ºå°‘ resin_smiles / curing_agent_smilesï¼Œæ— æ³•ä½¿ç”¨åˆ†ç»„åˆ’åˆ†ï¼Œå°†å›é€€ä¸ºéšæœºåˆ’åˆ†ã€‚")
                split_strategy = "random"
            else:
                group_key = st.selectbox("åˆ†ç»„é”®", options=group_key_options, index=0)
                if group_key == "resin_smiles + curing_agent_smiles":
                    groups = df["resin_smiles"].astype(str) + "||" + df["curing_agent_smiles"].astype(str)
                elif group_key == "resin_smiles":
                    groups = df["resin_smiles"].astype(str)
                elif group_key == "curing_agent_smiles":
                    groups = df["curing_agent_smiles"].astype(str)

        # --- [P0-4 / P1-1] äº¤å‰éªŒè¯ ---
        st.markdown("### ğŸ§ª äº¤å‰éªŒè¯ (CV)")
        enable_cv = st.checkbox("åŒæ—¶è®¡ç®—äº¤å‰éªŒè¯ (æ¨è)", value=True)
        cv_folds = 5
        cv_repeats = 5
        if enable_cv:
            cv_folds = st.slider("CV folds", 3, 10, 5)
            cv_repeats = st.slider("CV repeatsï¼ˆä»…å¯¹ repeated kfold æœ‰æ•ˆï¼‰", 1, 10, 5)

    with col2:
        st.markdown("### ğŸ›ï¸ æ‰‹åŠ¨è°ƒå‚")

        # åº”ç”¨ä¼˜åŒ–å‚æ•°
        if st.session_state.best_params and st.session_state.get('optimized_model_name') == model_name:
            st.info(f"ğŸ’¡ æ£€æµ‹åˆ°ä¼˜åŒ–å‚æ•° (RÂ²: {st.session_state.get('best_score', 0):.4f})")
            if st.button("ğŸ”„ åº”ç”¨æœ€ä½³å‚æ•°"):
                for k, v in st.session_state.best_params.items():
                    st.session_state[f"param_{model_name}_{k}"] = v
                st.rerun()

         # ç”Ÿæˆå‚æ•°æ§ä»¶
        manual_params = {}

        # å‚æ•°é…ç½®ä¼˜å…ˆçº§ï¼šTFS ä½¿ç”¨ core.tf_model ä¸­çš„é…ç½®ï¼ˆæ”¯æŒ checkbox ç­‰ï¼‰ï¼Œå…¶ä½™æ¨¡å‹ä½¿ç”¨ ui_config
        configs = []
        if model_name == "TensorFlow Sequential":
            configs = TFS_TUNING_PARAMS if TFS_TUNING_PARAMS else MANUAL_TUNING_PARAMS.get(model_name, [])
        elif model_name in MANUAL_TUNING_PARAMS:
            configs = MANUAL_TUNING_PARAMS[model_name]

        if configs:
            p_cols = st.columns(2)
            for i, config in enumerate(configs):
                with p_cols[i % 2]:
                    key = f"param_{model_name}_{config['name']}"
                    if key not in st.session_state:
                        st.session_state[key] = config.get('default')

                    help_txt = config.get('help', None)
                    widget = config.get('widget', 'text_input')
                    args = config.get('args', {}) or {}

                    # å¯¹ TFSï¼šè‹¥å…³é—­ early_stoppingï¼Œåˆ™ patience ä¸å¿…å¡«å†™
                    disabled_flag = False
                    if model_name == "TensorFlow Sequential" and config.get('name') == 'patience':
                        disabled_flag = (not bool(manual_params.get('early_stopping', True)))

                    if widget == 'slider':
                        manual_params[config['name']] = st.slider(config['label'], key=key, help=help_txt, disabled=disabled_flag, **args)
                    elif widget == 'number_input':
                        manual_params[config['name']] = st.number_input(config['label'], key=key, help=help_txt, disabled=disabled_flag, **args)
                    elif widget == 'selectbox':
                        manual_params[config['name']] = st.selectbox(config['label'], options=args.get('options', []), key=key, help=help_txt, disabled=disabled_flag)
                    elif widget == 'text_input':
                        manual_params[config['name']] = st.text_input(config['label'], key=key, help=help_txt, disabled=disabled_flag)
                    elif widget == 'checkbox':
                        manual_params[config['name']] = st.checkbox(config['label'], key=key, help=help_txt, disabled=disabled_flag)

    st.markdown("---")

    # æŒ‰é’®åŒº
    c_btn1, c_btn2 = st.columns(2)

    with c_btn1:
        if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", type="primary", disabled=disable_train if "disable_train" in locals() else False):
            with st.spinner("è®­ç»ƒä¸­..."):
                try:
                    # å‡†å¤‡å‚æ•°
                    params = manual_params.copy()
<<<<<<< HEAD
                    params['train_n_jobs'] = int(train_n_jobs)
=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4
                    if 'random_state' in params:
                        params.pop('random_state')

                    # è®­ç»ƒï¼ˆæ”¯æŒ split_strategy / n_bins / groupsï¼‰
                    res = trainer.train_model(
                        X, y,
                        model_name=model_name,
                        test_size=test_size,
                        random_state=int(random_state),
                        split_strategy=split_strategy,
                        n_bins=int(n_bins),
                        groups=groups,
                        **params
                    )

                    # äº¤å‰éªŒè¯ï¼ˆå¯é€‰ï¼‰
                    cv_res = None
                    if enable_cv:
                        if split_strategy == "group" and groups is not None:
                            cv_strategy = "group_kfold"
                        elif split_strategy == "stratified":
                            cv_strategy = "stratified_kfold"
                        else:
                            cv_strategy = "repeated_kfold"

                        cv_res = trainer.cross_validate_model(
                            X, y,
                            model_name=model_name,
                            cv_strategy=cv_strategy,
                            n_splits=int(cv_folds),
                            n_repeats=int(cv_repeats),
                            random_state=int(random_state),
                            groups=groups,
                            n_bins=int(n_bins),
                            **params
                        )

                    # ä¿å­˜ç»“æœ
                    st.session_state.model = res['model']
                    st.session_state.pipeline = res.get('pipeline')
                    st.session_state.scaler = res.get('scaler')
                    st.session_state.imputer = res.get('imputer')
                    st.session_state.train_result = res
                    st.session_state.cv_result = cv_res

                    st.session_state.X_train = res['X_train']
                    st.session_state.X_test = res['X_test']
                    st.session_state.y_train = res['y_train']
                    st.session_state.y_test = res['y_test']
                    st.session_state.model_name = model_name
                    st.session_state.manual_params = params  # ç”¨äºè„šæœ¬å¯¼å‡º

                    st.success("âœ… è®­ç»ƒå®Œæˆ")

                    # [æ–°å¢] è®­ç»ƒè¿‡ç¨‹å†™å…¥çŠ¶æ€æ¡
                    try:
                        log_fe_step(
                            operation="æ¨¡å‹è®­ç»ƒ",
                            description=f"è®­ç»ƒå®Œæˆ: {model_name}",
                            params={
                                "model": model_name,
                                "test_size": float(test_size),
                                "split_strategy": str(split_strategy),
                                "n_features": int(len(st.session_state.get('feature_cols') or [])),
                                **(params or {})
                            },
                            input_df=df,
                            status="success",
                            message=f"RÂ²={res.get('r2', 0):.4f}, RMSE={res.get('rmse', 0):.4f}, MAE={res.get('mae', 0):.4f}"
                        )
                    except Exception:
                        pass


                    # --- æŒ‡æ ‡ ---
                    st.markdown("### ğŸ“Œ å•æ¬¡åˆ’åˆ†ï¼ˆTestï¼‰æŒ‡æ ‡")
                    m1, m2, m3, m4 = st.columns(4)
                    m1.metric("RÂ² (Test)", f"{res['r2']:.4f}")
                    m2.metric("RMSE (Test)", f"{res['rmse']:.4f}")
                    m3.metric("MAE (Test)", f"{res['mae']:.4f}")
                    m4.metric("Train Time (s)", f"{res.get('train_time', 0):.2f}")

                    if cv_res is not None:
                        st.markdown("### ğŸ§ª äº¤å‰éªŒè¯ï¼ˆCVï¼‰æŒ‡æ ‡")
                        c1, c2, c3 = st.columns(3)
                        c1.metric("CV RÂ² (meanÂ±std)", f"{cv_res['cv_r2_mean']:.4f} Â± {cv_res['cv_r2_std']:.4f}")
                        c2.metric("OOF RMSE", f"{cv_res['oof_rmse']:.4f}")
                        c3.metric("OOF MAE", f"{cv_res['oof_mae']:.4f}")

                        # æŠ˜åˆ†æ•°è¡¨
                        fold_df = pd.DataFrame({
                            "fold_r2": cv_res.get("fold_r2", []),
                            "fold_rmse": cv_res.get("fold_rmse", []),
                            "fold_mae": cv_res.get("fold_mae", []),
                        })
                        st.dataframe(fold_df, use_container_width=True, height=200)

                    # --- [å¢å¼º] æ‰€æœ‰æ¨¡å‹çš„è®­ç»ƒæ›²çº¿ + è®­ç»ƒè®°å½•è½ç›˜ ---
                    try:
                        history = res.get('training_history') or {}
                        # å›¾å†…æ ‡é¢˜å°½é‡ç”¨è‹±æ–‡ï¼Œé¿å…æœåŠ¡å™¨ç¯å¢ƒç¼ºå°‘ä¸­æ–‡å­—ä½“å¯¼è‡´æ–¹å—/ä¹±ç 
                        fig_curve, hist_export_df = plot_history(history, title=f"{model_name} Training Curves")

                        st.markdown("### ğŸ“‰ è®­ç»ƒæ›²çº¿ï¼ˆæ‰€æœ‰æ¨¡å‹ï¼‰")
                        st.pyplot(fig_curve, use_container_width=True)

                        if hist_export_df is not None and not hist_export_df.empty:
                            with st.expander("ğŸ§¾ æŸ¥çœ‹è®­ç»ƒæ›²çº¿æ•°æ®", expanded=False):
                                st.dataframe(hist_export_df, use_container_width=True, height=240)
                                st.download_button(
                                    "ğŸ“¥ å¯¼å‡ºè®­ç»ƒæ›²çº¿ CSV",
                                    hist_export_df.to_csv(index=False).encode("utf-8-sig"),
                                    f"{model_name}_training_history.csv",
                                    "text/csv"
                                )

                        # ä¿å­˜ä¸€æ¬¡è®­ç»ƒ Runï¼ˆæŒ‡æ ‡+å‚æ•°+æ›²çº¿ï¼‰
                        manager = TrainingRunManager()
                        meta = {
                            "model_name": model_name,
                            "r2": float(res.get('r2', 0)),
                            "rmse": float(res.get('rmse', 0)),
                            "mae": float(res.get('mae', 0)),
                            "train_time": float(res.get('train_time', 0)),
                            "split_strategy": str(res.get('split_strategy', '')),
                            "test_size": float(test_size),
                            "random_state": int(random_state),
                            "params": params or {},
                            "n_samples": int(len(res.get('y_train', [])) + len(res.get('y_test', []))),
                            "n_features": int(len(st.session_state.get('feature_cols') or [])),
                        }
                        summary = manager.save_run(
                            model_name=model_name,
                            metadata=meta,
                            history_df=hist_export_df,
                            curve_fig=fig_curve,
                        )
                        st.session_state.last_training_run_id = summary.run_id
                        st.caption(f"ğŸ—‚ï¸ å·²ä¿å­˜è®­ç»ƒè®°å½•: {summary.run_id}ï¼ˆå¯åœ¨ã€ğŸ“ˆ è®­ç»ƒè®°å½•ã€‘æŸ¥çœ‹ï¼‰")
                    except Exception:
                        pass

                    # --- [æ–°å¢] TFS ç½‘ç»œç»“æ„ Summary ---
                    if model_name == "TensorFlow Sequential":
                        try:
                            summary_str = ""
                            if hasattr(st.session_state.model, "get_model_summary_str"):
                                summary_str = st.session_state.model.get_model_summary_str() or ""
                            if summary_str.strip():
                                with st.expander("ğŸ§¾ TFS ç½‘ç»œç»“æ„ï¼ˆModel Summaryï¼‰", expanded=False):
                                    st.code(summary_str)
                        except Exception:
                            pass

                    # --- ç»“æœè¡¨æ ¼ä¸å¯¼å‡º ---
                    st.markdown("### ğŸ“ˆ æµ‹è¯•é›†é¢„æµ‹ç»“æœè¯¦æƒ…")
                    res_df = pd.DataFrame({
                        "çœŸå®å€¼": res['y_test'],
                        "é¢„æµ‹å€¼": res['y_pred_test'] if 'y_pred_test' in res else res['y_pred']
                    })
                    res_df["æ®‹å·®"] = res_df["çœŸå®å€¼"] - res_df["é¢„æµ‹å€¼"]

                    t1, t2 = st.columns([3, 1])
                    with t1:
                        st.dataframe(res_df, use_container_width=True, height=200)
                    with t2:
                        csv = res_df.to_csv(index=False).encode('utf-8')
                        st.download_button("ğŸ“¥ å¯¼å‡ºç»“æœ CSV", csv, "predictions_test.csv", "text/csv")

                    # --- å¯è§†åŒ– ---
                    st.markdown("### ğŸ“‰ æ€§èƒ½å¯è§†åŒ–")
                    visualizer = Visualizer()

                    if cv_res is not None:
                        tab_a, tab_b = st.tabs(["Train/Test", "CV (OOF)"])
                        with tab_a:
                            col_img1, col_img2, col_img3 = st.columns([1, 2, 1])
                            with col_img2:
                                fig_tt, _ = visualizer.plot_parity_train_test(
                                    res['y_train'], res['y_pred_train'],
                                    res['y_test'], res['y_pred_test'],
                                    target_name=target_col
                                )
                                st.pyplot(fig_tt, use_container_width=True)
                        with tab_b:
                            col_img1, col_img2, col_img3 = st.columns([1, 2, 1])
                            with col_img2:
                                fig_oof, _ = visualizer.plot_predictions_vs_true(
                                    cv_res['oof_true'],
                                    cv_res['oof_pred'],
                                    model_name=f"{model_name} (OOF)"
                                )
                                st.pyplot(fig_oof, use_container_width=True)
                    else:
                        col_img1, col_img2, col_img3 = st.columns([1, 2, 1])
                        with col_img2:
                            fig, _ = visualizer.plot_parity_train_test(
                                res['y_train'], res['y_pred_train'],
                                res['y_test'], res['y_pred_test'],
                                target_name=target_col
                            )
                            st.pyplot(fig, use_container_width=True)

                except Exception as e:
                    st.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
                    # [æ–°å¢] å¤±è´¥ä¹Ÿå†™å…¥çŠ¶æ€æ¡
                    try:
                        log_fe_step(
                            operation="æ¨¡å‹è®­ç»ƒ",
                            description=f"è®­ç»ƒå¤±è´¥: {model_name}",
                            params={"model": model_name},
                            input_df=df,
                            status="error",
                            message=str(e)
                        )
                    except Exception:
                        pass

    with c_btn2:
        # è„šæœ¬å¯¼å‡ºæŒ‰é’®
        if st.session_state.model and st.session_state.model_name == model_name:
            if 'generate_training_script_code' in globals():
                script = generate_training_script_code(
                    model_name,
                    manual_params,
                    st.session_state.feature_cols,
                    st.session_state.target_col
                )
                st.download_button("ğŸ’¾ å¯¼å‡º Python è®­ç»ƒè„šæœ¬", script, "train_script.py")

<<<<<<< HEAD
                st.markdown("---")
                with st.expander("ğŸ“¦ å¯¼å‡ºè®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆ.joblibï¼‰", expanded=False):
                    st.caption("å¯¼å‡ºçš„æ¨¡å‹æ–‡ä»¶åŒ…å«ï¼špipeline/æ¨¡å‹ã€ç‰¹å¾åˆ—ã€ç›®æ ‡åˆ—ã€è¯„ä¼°æŒ‡æ ‡ç­‰ã€‚å¯åœ¨â€œé¢„æµ‹åº”ç”¨â€é¡µé¢ç›´æ¥å¯¼å…¥ä½¿ç”¨ã€‚")
                    try:
                        from core.model_io import create_model_artifact_bytes
                        metrics = {}
                        tr = st.session_state.get("train_result") or {}
                        for k in ["r2", "rmse", "mae", "train_time", "split_strategy", "n_bins"]:
                            if k in tr:
                                metrics[k] = tr[k]
                        model_bytes = create_model_artifact_bytes(
                            model_name=str(st.session_state.get("model_name") or model_name),
                            target_col=str(st.session_state.get("target_col") or ""),
                            feature_cols=list(st.session_state.get("feature_cols") or []),
                            model=st.session_state.get("model"),
                            pipeline=st.session_state.get("pipeline"),
                            scaler=st.session_state.get("scaler"),
                            imputer=st.session_state.get("imputer"),
                            metrics=metrics,
                            extra={
                                "app_version": str(VERSION),
                            },
                        )
                        safe_name = (str(st.session_state.get("model_name") or model_name) or "model").replace(" ", "_")
                        st.download_button(
                            "â¬‡ï¸ ä¸‹è½½æ¨¡å‹æ–‡ä»¶",
                            data=model_bytes,
                            file_name=f"{safe_name}_artifact.joblib",
                            mime="application/octet-stream"
                        )
                    except Exception as e:
                        st.error(f"æ¨¡å‹å¯¼å‡ºå¤±è´¥ï¼š{e}")
                        st.info("æç¤ºï¼šè‹¥ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆTF/è‡ªå®šä¹‰ç½‘ç»œï¼‰ï¼Œjoblib åºåˆ—åŒ–å¯èƒ½å¤±è´¥ã€‚å¯æ”¹ç”¨â€œå¯¼å‡ºè®­ç»ƒè„šæœ¬â€åœ¨ç›®æ ‡ç¯å¢ƒå¤ç°è®­ç»ƒã€‚")


=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4
def page_model_interpretation():
    """æ¨¡å‹è§£é‡Šé¡µé¢"""
    st.title("ğŸ“Š æ¨¡å‹è§£é‡Š")

    if st.session_state.model is None:
        st.warning("âš ï¸ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return

    model = st.session_state.model
    model_name = st.session_state.model_name
    X_train = st.session_state.X_train
    y_train = st.session_state.y_train
    X_test = st.session_state.X_test
    y_test = st.session_state.y_test
    feature_names = st.session_state.feature_cols or []
    n_features = len(feature_names)

    tab1, tab2, tab3 = st.tabs(["ğŸ” SHAPåˆ†æ", "ğŸ“ˆ é¢„æµ‹æ€§èƒ½", "ğŸ¯ ç‰¹å¾é‡è¦æ€§"])

    def _export_matplotlib_fig(fig, base_name: str, key_prefix: str):
        """Export matplotlib fig as PNG/HTML download buttons."""
        if fig is None:
            return
        try:
            png_bytes = fig_to_png_bytes(fig)
            st.download_button(
                "ğŸ“¥ å¯¼å‡ºå›¾åƒ PNG",
                png_bytes,
                file_name=f"{base_name}.png",
                mime="image/png",
                key=f"{key_prefix}_png",
            )
            html_str = fig_to_html(fig, title=base_name)
            st.download_button(
                "ğŸ“¥ å¯¼å‡ºå›¾åƒ HTML",
                html_str.encode("utf-8"),
                file_name=f"{base_name}.html",
                mime="text/html",
                key=f"{key_prefix}_html",
            )
        except Exception as e:
            st.warning(f"å›¾åƒå¯¼å‡ºå¤±è´¥: {e}")

    # --- 1) SHAP / å¿«é€Ÿè§£é‡Š ---
    with tab1:
        st.markdown("### ç‰¹å¾è§£é‡Š")

        default_fast = n_features >= 300
        method = st.radio(
            "è§£é‡Šæ–¹æ³•",
            ["SHAPï¼ˆæ›´å‡†ç¡®ï¼Œå¯èƒ½è¾ƒæ…¢ï¼‰", "å¿«é€Ÿæ¨¡å¼ï¼ˆPermutation Importanceï¼Œæ¨èï¼‰"],
            index=1 if default_fast else 0,
            horizontal=True,
            key="interp_method",
        )

        if n_features >= 300:
            st.warning(
                f"å½“å‰ç‰¹å¾æ•°é‡ä¸º {n_features}ã€‚è‹¥æ¨¡å‹ä¸æ˜¯æ ‘/çº¿æ€§æ¨¡å‹ï¼ŒSHAPï¼ˆå°¤å…¶ KernelExplainerï¼‰å¯èƒ½éå¸¸æ…¢ç”šè‡³å¡ä½ã€‚å»ºè®®ä½¿ç”¨â€œå¿«é€Ÿæ¨¡å¼â€ã€‚"
            )

        # ---------- SHAP ----------
        if method.startswith("SHAP"):
            c_opt1, c_opt2, c_opt3 = st.columns(3)
            with c_opt1:
                plot_type = st.selectbox("å›¾è¡¨ç±»å‹", ["bar", "beeswarm"], index=0, key="shap_plot_type")
            with c_opt2:
                max_display = st.slider("æ˜¾ç¤ºç‰¹å¾æ•°é‡", 5, 50, 20, key="shap_max_display")
            with c_opt3:
                # é™åˆ¶é‡‡æ ·æ¡æ•°ï¼Œæé€Ÿ
                max_val = max(20, min(500, len(X_test)))
                max_samples = int(
                    st.number_input(
                        "é‡‡æ ·æ¡æ•°ï¼ˆè¶Šå°è¶Šå¿«ï¼‰",
                        min_value=20,
                        max_value=int(max_val),
                        value=int(min(200, len(X_test))),
                        step=10,
                        key="shap_max_samples",
                    )
                )

            c_k1, c_k2 = st.columns(2)
            with c_k1:
                kernel_bg_default = 20 if n_features >= 300 else 50
                kernel_bg = int(
                    st.number_input(
                        "Kernel èƒŒæ™¯æ ·æœ¬æ•°",
                        min_value=5,
                        max_value=100,
                        value=int(kernel_bg_default),
                        step=5,
                        key="shap_kernel_bg",
                    )
                )
            with c_k2:
                kernel_ns = int(
                    st.number_input(
                        "Kernel nsamples",
                        min_value=50,
                        max_value=2000,
                        value=200,
                        step=50,
                        key="shap_kernel_nsamples",
                    )
                )

            if st.button("ğŸ” è®¡ç®—SHAPå€¼", key="btn_compute_shap"):
                with st.spinner("æ­£åœ¨è®¡ç®— SHAP å€¼ (å¯èƒ½è¾ƒæ…¢)..."):
                    try:
                        interp = EnhancedModelInterpreter(
                            model,
                            X_train,
                            y_train,
                            X_test,
                            y_test,
                            model_name,
                            feature_names=feature_names,
                            max_samples=max_samples,
                            kernel_background=kernel_bg,
                            kernel_nsamples=kernel_ns,
                        )
                        fig, df_shap = interp.plot_summary(plot_type=plot_type, max_display=max_display)

                        if fig:
                            c1, c2, c3 = st.columns([1, 6, 1])
                            with c2:
                                st.pyplot(fig, use_container_width=True)

                                if df_shap is not None:
                                    csv = df_shap.to_csv(index=False).encode("utf-8-sig")
                                    st.download_button(
                                        "ğŸ“¥ å¯¼å‡º SHAP æ•°æ® (CSV)",
                                        csv,
                                        "shap_values.csv",
                                        "text/csv",
                                        key="shap_csv",
                                    )

                                _export_matplotlib_fig(fig, base_name="shap_summary", key_prefix="shap_fig")
                        else:
                            st.error("æ— æ³•ç”Ÿæˆ SHAP å›¾ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒã€‚")
                    except Exception as e:
                        st.error(f"è®¡ç®—å‡ºé”™: {str(e)}")
            else:
                st.caption("æç¤ºï¼šSHAP ç»“æœä¼šå—åˆ°é‡‡æ ·æ¡æ•°/èƒŒæ™¯æ ·æœ¬çš„å½±å“ï¼›ç‰¹å¾æ•°å¾ˆå¤§æ—¶æ¨èä½¿ç”¨å¿«é€Ÿæ¨¡å¼ã€‚")

        # ---------- Permutation (Fast) ----------
        else:
            st.markdown("#### âš¡ å¿«é€Ÿé‡è¦æ€§ï¼ˆPermutation Importanceï¼‰")

            c_q1, c_q2, c_q3, c_q4 = st.columns(4)
            with c_q1:
                top_n = st.slider("æ˜¾ç¤ºç‰¹å¾æ•°é‡", 5, 50, 20, key="perm_top_n")
            with c_q2:
                n_repeats = st.slider("é‡å¤æ¬¡æ•°", 1, 10, 3, key="perm_repeats")
            with c_q3:
                max_val = max(30, min(1000, len(X_test)))
                sample_n = int(
                    st.number_input(
                        "é‡‡æ ·æ¡æ•°",
                        min_value=30,
                        max_value=int(max_val),
                        value=int(min(200, len(X_test))),
                        step=20,
                        key="perm_sample_n",
                    )
                )
            with c_q4:
                scoring = st.selectbox("è¯„åˆ†æŒ‡æ ‡", ["r2", "neg_root_mean_squared_error"], index=0, key="perm_scoring")

            if st.button("âš¡ è®¡ç®—å¿«é€Ÿé‡è¦æ€§", key="btn_compute_perm"):
                with st.spinner("æ­£åœ¨è®¡ç®— permutation importance..."):
                    try:
                        from sklearn.inspection import permutation_importance

                        # ç¡®ä¿ X/y çš„ç´¢å¼•å¯¹é½
                        if isinstance(X_test, pd.DataFrame):
                            Xdf = X_test.copy()
                        else:
                            Xdf = pd.DataFrame(np.asarray(X_test), columns=feature_names)

                        if isinstance(y_test, pd.Series):
                            y_series = y_test.copy()
                        elif isinstance(y_test, pd.DataFrame):
                            y_series = y_test.iloc[:, 0].copy()
                        else:
                            y_series = pd.Series(np.asarray(y_test).ravel(), index=Xdf.index)

                        if len(Xdf) > sample_n:
                            X_sample = Xdf.sample(n=sample_n, random_state=42)
                            y_sample = y_series.loc[X_sample.index]
                        else:
                            X_sample = Xdf
                            y_sample = y_series

                        result = permutation_importance(
                            model,
                            X_sample,
                            np.asarray(y_sample).ravel(),
                            n_repeats=int(n_repeats),
                            random_state=42,
                            scoring=scoring,
                        )

                        df_perm = pd.DataFrame(
                            {
                                "Feature": feature_names,
                                "Importance": result.importances_mean,
                                "Std": result.importances_std,
                            }
                        ).sort_values("Importance", ascending=False)

                        viz = Visualizer()
                        fig, _ = viz.plot_feature_importance(
                            df_perm["Importance"].values,
                            df_perm["Feature"].values.tolist(),
                            f"{model_name} - Permutation",
                            top_n=int(top_n),
                        )

                        c1, c2, c3 = st.columns([1, 6, 1])
                        with c2:
                            st.pyplot(fig, use_container_width=True)

                            csv = df_perm.to_csv(index=False).encode("utf-8-sig")
                            st.download_button(
                                "ğŸ“¥ å¯¼å‡º permutation æ•°æ® (CSV)",
                                csv,
                                "permutation_importance.csv",
                                "text/csv",
                                key="perm_csv",
                            )

                            _export_matplotlib_fig(fig, base_name="permutation_importance", key_prefix="perm_fig")

                    except Exception as e:
                        st.error(f"è®¡ç®—å¤±è´¥: {e}")
            else:
                st.caption("Permutation importance å¯¹æ¨¡å‹æ— å‡è®¾ã€é€Ÿåº¦æ›´å¿«ï¼›æ•°å€¼è¶Šå¤§è¡¨ç¤ºè¯¥ç‰¹å¾è¶Šé‡è¦ã€‚")

    # --- 2) é¢„æµ‹æ€§èƒ½ ---
    with tab2:
        st.markdown("### é¢„æµ‹æ€§èƒ½")
        visualizer = Visualizer()

        try:
            y_pred = st.session_state.train_result["y_pred"] if st.session_state.get("train_result") else None
        except Exception:
            y_pred = None

        if y_pred is None:
            st.warning("ç¼ºå°‘é¢„æµ‹ç»“æœï¼Œè¯·å…ˆåœ¨è®­ç»ƒé¡µå®Œæˆä¸€æ¬¡è®­ç»ƒã€‚")
        else:
            c1, c2, c3 = st.columns([1, 2, 1])
            with c2:
                fig, df_res = visualizer.plot_residuals(y_test, y_pred, model_name)
                st.pyplot(fig, use_container_width=True)

                if df_res is not None:
                    csv = df_res.to_csv(index=False).encode("utf-8-sig")
                    st.download_button(
                        "ğŸ“¥ å¯¼å‡ºæ®‹å·®æ•°æ® (CSV)",
                        csv,
                        "residuals.csv",
                        "text/csv",
                        key="res_csv",
                    )

                _export_matplotlib_fig(fig, base_name="residuals", key_prefix="res_fig")

    # --- 3) ç‰¹å¾é‡è¦æ€§ ---
    with tab3:
        st.markdown("### ç‰¹å¾é‡è¦æ€§")
        top_n = st.slider("æ˜¾ç¤ºç‰¹å¾æ•°é‡", 5, 50, 20, key="fi_top_n")

        if hasattr(model, "feature_importances_"):
            visualizer = Visualizer()
            c1, c2, c3 = st.columns([1, 2, 1])
            with c2:
                fig, df_imp = visualizer.plot_feature_importance(
                    model.feature_importances_, feature_names, model_name, top_n=int(top_n)
                )
                st.pyplot(fig, use_container_width=True)

                if df_imp is not None:
                    csv = df_imp.to_csv(index=False).encode("utf-8-sig")
                    st.download_button(
                        "ğŸ“¥ å¯¼å‡ºé‡è¦æ€§æ•°æ® (CSV)",
                        csv,
                        "importance.csv",
                        "text/csv",
                        key="fi_csv",
                    )

                _export_matplotlib_fig(fig, base_name="feature_importance", key_prefix="fi_fig")

            # MACCS è§£é‡Šè¡¨
            if df_imp is not None and not df_imp.empty:
                st.markdown("#### ğŸ§¬ ç‰¹å¾å«ä¹‰è§£æï¼ˆTop 15ï¼‰")
                exps = []
                for f in df_imp.head(15)["Feature"]:
                    desc = "æ•°å€¼ç‰¹å¾"
                    if "MACCS" in str(f):
                        try:
                            from core.molecular_features import get_maccs_description

                            idx = int(str(f).split("_")[-1])
                            desc = get_maccs_description(idx)
                        except Exception:
                            desc = "MACCS æŒ‡çº¹ç‰‡æ®µ"
                    exps.append({"ç‰¹å¾å": f, "å«ä¹‰": desc})
                st.table(pd.DataFrame(exps))
        else:
            st.info("è¯¥æ¨¡å‹ä¸æ”¯æŒåŸç”Ÿ feature_importances_ã€‚å¯åœ¨ã€SHAPåˆ†æã€‘ä¸­ä½¿ç”¨ SHAP æˆ–å¿«é€Ÿæ¨¡å¼ã€‚")
            st.markdown("#### ï¼ˆå¯é€‰ï¼‰ç”¨ permutation importance ä½œä¸ºæ›¿ä»£")

            c_q1, c_q2, c_q3 = st.columns(3)
            with c_q1:
                n_repeats = st.slider("é‡å¤æ¬¡æ•°", 1, 10, 3, key="fi_perm_repeats")
            with c_q2:
                max_val = max(30, min(1000, len(X_test)))
                sample_n = int(
                    st.number_input(
                        "é‡‡æ ·æ¡æ•°",
                        min_value=30,
                        max_value=int(max_val),
                        value=int(min(200, len(X_test))),
                        step=20,
                        key="fi_perm_sample",
                    )
                )
            with c_q3:
                scoring = st.selectbox(
                    "è¯„åˆ†æŒ‡æ ‡", ["r2", "neg_root_mean_squared_error"], index=0, key="fi_perm_scoring"
                )

            if st.button("âš¡ è®¡ç®—æ›¿ä»£é‡è¦æ€§", key="btn_fi_perm"):
                with st.spinner("æ­£åœ¨è®¡ç®— permutation importance..."):
                    try:
                        from sklearn.inspection import permutation_importance

                        if isinstance(X_test, pd.DataFrame):
                            Xdf = X_test.copy()
                        else:
                            Xdf = pd.DataFrame(np.asarray(X_test), columns=feature_names)

                        if isinstance(y_test, pd.Series):
                            y_series = y_test.copy()
                        elif isinstance(y_test, pd.DataFrame):
                            y_series = y_test.iloc[:, 0].copy()
                        else:
                            y_series = pd.Series(np.asarray(y_test).ravel(), index=Xdf.index)

                        if len(Xdf) > sample_n:
                            X_sample = Xdf.sample(n=sample_n, random_state=42)
                            y_sample = y_series.loc[X_sample.index]
                        else:
                            X_sample = Xdf
                            y_sample = y_series

                        result = permutation_importance(
                            model,
                            X_sample,
                            np.asarray(y_sample).ravel(),
                            n_repeats=int(n_repeats),
                            random_state=42,
                            scoring=scoring,
                        )

                        df_perm = pd.DataFrame(
                            {
                                "Feature": feature_names,
                                "Importance": result.importances_mean,
                                "Std": result.importances_std,
                            }
                        ).sort_values("Importance", ascending=False)

                        viz = Visualizer()
                        fig, _ = viz.plot_feature_importance(
                            df_perm["Importance"].values,
                            df_perm["Feature"].values.tolist(),
                            f"{model_name} - Permutation",
                            top_n=int(top_n),
                        )
                        st.pyplot(fig, use_container_width=True)

                        csv = df_perm.to_csv(index=False).encode("utf-8-sig")
                        st.download_button(
                            "ğŸ“¥ å¯¼å‡ºæ›¿ä»£é‡è¦æ€§ (CSV)",
                            csv,
                            "permutation_importance.csv",
                            "text/csv",
                            key="fi_perm_csv",
                        )

                        _export_matplotlib_fig(fig, base_name="permutation_importance", key_prefix="fi_perm_fig")

                    except Exception as e:
                        st.error(f"è®¡ç®—å¤±è´¥: {e}")

def page_prediction():
    """é¢„æµ‹åº”ç”¨é¡µé¢ï¼ˆä¿®å¤ï¼šé¢„æµ‹é˜¶æ®µåº”ç”¨ imputer/scalerï¼›æ”¯æŒæŒ‡çº¹é€‚ç”¨åŸŸï¼‰"""
    st.title("ğŸ”® é¢„æµ‹åº”ç”¨")

<<<<<<< HEAD
    # =========================
    # ğŸ“¦ å¯¼å…¥æ¨¡å‹ï¼ˆæ— éœ€å…ˆè®­ç»ƒï¼‰
    # =========================
    with st.expander("ğŸ“¦ å¯¼å…¥è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆ.joblibï¼‰", expanded=(st.session_state.model is None)):
        uploaded_model = st.file_uploader("ä¸Šä¼ æ¨¡å‹æ–‡ä»¶ï¼ˆ.joblib/.pklï¼‰", type=["joblib", "pkl"], key="model_uploader")
        if uploaded_model is not None:
            try:
                import hashlib
                data_bytes = uploaded_model.getvalue()
                file_hash = hashlib.sha256(data_bytes).hexdigest()
                # é¿å… Streamlit rerun å¯¼è‡´é‡å¤å¯¼å…¥ï¼ˆçœ‹èµ·æ¥åƒâ€œå¡æ­»â€ï¼‰
                if st.session_state.get("_last_import_hash") == file_hash and st.session_state.get("model") is not None:
                    st.info("æ¨¡å‹å·²åŠ è½½ï¼ˆæ£€æµ‹åˆ°ç›¸åŒæ–‡ä»¶ï¼‰ï¼Œå·²è·³è¿‡é‡å¤å¯¼å…¥ã€‚")
                else:
                    with st.spinner("æ­£åœ¨åŠ è½½æ¨¡å‹â€¦ï¼ˆé¦–æ¬¡å¯èƒ½è¾ƒæ…¢ï¼‰"):
                        from core.model_io import load_model_artifact_bytes
                        artifact = load_model_artifact_bytes(data_bytes)
        
                    # å†™å…¥ session_stateï¼ˆç”¨äºåç»­é¡µé¢å¤ç”¨ï¼‰
                    st.session_state.model_name = artifact.get("model_name") or "ImportedModel"
                    st.session_state.target_col = artifact.get("target_col") or st.session_state.get("target_col", "")
                    st.session_state.feature_cols = artifact.get("feature_cols") or st.session_state.get("feature_cols", [])
                    st.session_state.pipeline = artifact.get("pipeline", None)
                    st.session_state.model = artifact.get("model", None) or artifact.get("pipeline", None)
                    st.session_state.scaler = artifact.get("scaler", None)
                    st.session_state.imputer = artifact.get("imputer", None)
                    st.session_state.imported_model_artifact = artifact
                    st.session_state._last_import_hash = file_hash
        
                    # AutoGluon / TabPFN ç­‰é‡ä¾èµ–æ¨¡å‹æç¤º
                    if (artifact.get("model_name") or "").strip() in ["AutoGluon", "TabPFN"]:
                        st.warning("è¯¥æ¨¡å‹å±äºé‡ä¾èµ–ç±»å‹ï¼ˆå¦‚ AutoGluon/TabPFNï¼‰ã€‚è‹¥åŠ è½½è€—æ—¶è¾ƒé•¿ï¼Œè¯·ç¡®è®¤ä¾èµ–å·²å®‰è£…ä¸”ç‰ˆæœ¬ä¸€è‡´ã€‚")
        
                    st.success("âœ… æ¨¡å‹å¯¼å…¥æˆåŠŸï¼ä½ ç°åœ¨å¯ä»¥ç›´æ¥è¿›è¡Œé¢„æµ‹ã€‚")
            except Exception as e:
                st.error(f"âŒ æ¨¡å‹å¯¼å…¥å¤±è´¥ï¼š{e}")
                st.info("è¯·ç¡®è®¤æ–‡ä»¶æ¥è‡ªæœ¬ç³»ç»Ÿå¯¼å‡ºï¼ˆartifact.joblibï¼‰ï¼Œæˆ–æ˜¯å¯è¢« joblib æ­£å¸¸åŠ è½½çš„ sklearn Pipeline/æ¨¡å‹ã€‚")



=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4
    if st.session_state.model is None:
        st.warning("âš ï¸ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return

    model = st.session_state.model
    model_name = st.session_state.model_name
    feature_cols = st.session_state.feature_cols
    pipeline = st.session_state.get("pipeline", None)
    scaler = st.session_state.get("scaler", None)
    imputer = st.session_state.get("imputer", None)

    tab1, tab2, tab3 = st.tabs(["ğŸ“ å•æ ·æœ¬é¢„æµ‹", "ğŸ“ æ‰¹é‡é¢„æµ‹", "ğŸ¯ é€‚ç”¨åŸŸåˆ†æ"])

    # ============================================================
    # Tab1: å•æ ·æœ¬é¢„æµ‹
    # ============================================================
    with tab1:
        st.markdown("### å•æ ·æœ¬é¢„æµ‹")

        input_df = None

        # ç‰¹å¾è¿‡å¤šæ—¶ï¼Œç¦æ­¢æ¸²æŸ“å¤§é‡ number_inputï¼ˆä¼šå¡æ­»ï¼‰
        if len(feature_cols) <= 60:
            input_values = {}
            cols = st.columns(3)
            for i, feature in enumerate(feature_cols):
                with cols[i % 3]:
                    input_values[feature] = st.number_input(feature, value=0.0)
            input_df = pd.DataFrame([input_values])
        else:
            st.info(f"å½“å‰ç‰¹å¾æ•°é‡è¾ƒå¤šï¼ˆ{len(feature_cols)}ï¼‰ï¼Œå»ºè®®ç”¨â€œå•è¡Œæ–‡ä»¶â€ä¸Šä¼ è¿›è¡Œå•æ ·æœ¬é¢„æµ‹ã€‚")
            single_file = st.file_uploader("ä¸Šä¼ å•æ ·æœ¬æ–‡ä»¶ï¼ˆCSV/Excelï¼Œè‡³å°‘åŒ…å«æ‰€é€‰ç‰¹å¾åˆ—ï¼‰", type=['csv', 'xlsx', 'xls'], key="single_pred_file")
            if single_file is not None:
                try:
                    tmp_df = load_data_file(single_file)
                    if tmp_df.shape[0] == 0:
                        st.error("æ–‡ä»¶ä¸ºç©º")
                    else:
                        row_idx = 0
                        if tmp_df.shape[0] > 1:
                            row_idx = st.number_input("é€‰æ‹©é¢„æµ‹çš„è¡Œå·ï¼ˆä» 0 å¼€å§‹ï¼‰", 0, int(tmp_df.shape[0]-1), 0)
                        input_df = tmp_df.iloc[[int(row_idx)]].copy()
                except Exception as e:
                    st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")

        if st.button("ğŸ”® å¼€å§‹é¢„æµ‹", type="primary"):
            if input_df is None:
                st.error("è¯·å…ˆæä¾›è¾“å…¥æ ·æœ¬")
            else:
                # ä¿è¯åˆ—é½å…¨
                missing = [c for c in feature_cols if c not in input_df.columns]
                if missing:
                    st.error(f"è¾“å…¥æ ·æœ¬ç¼ºå°‘ç‰¹å¾åˆ—: {missing[:10]}{'...' if len(missing)>10 else ''}")
                else:
                    X_in = input_df[feature_cols]

                    try:
                        if pipeline is not None:
                            pred = pipeline.predict(X_in)[0]
                        else:
                            # [P0-5] ä¿®å¤ï¼šæ²¡æœ‰ pipeline æ—¶ä¹Ÿè¦åº”ç”¨ imputer + scaler
                            X_arr = X_in.values
                            if imputer is not None:
                                X_arr = imputer.transform(X_arr)
                            if scaler is not None:
                                X_arr = scaler.transform(X_arr)
                            pred = model.predict(X_arr)[0]

                        st.success(f"âœ… é¢„æµ‹ç»“æœï¼š{pred:.4f}")

                    except Exception as e:
                        st.error(f"âŒ é¢„æµ‹å¤±è´¥: {e}")

    # ============================================================
    # Tab2: æ‰¹é‡é¢„æµ‹
    # ============================================================
    with tab2:
        st.markdown("### æ‰¹é‡é¢„æµ‹")
        uploaded_file = st.file_uploader("ä¸Šä¼ å¾…é¢„æµ‹æ•°æ®", type=['csv', 'xlsx', 'xls'], key="batch_pred_file")

        if uploaded_file is not None:
            try:
                pred_df = load_data_file(uploaded_file)
                st.dataframe(pred_df.head(), use_container_width=True)

                if st.button("ğŸš€ æ‰§è¡Œæ‰¹é‡é¢„æµ‹", type="primary"):
                    missing = [c for c in feature_cols if c not in pred_df.columns]
                    if missing:
                        st.error(f"ç¼ºå°‘ç‰¹å¾åˆ—: {missing[:10]}{'...' if len(missing)>10 else ''}")
                    else:
<<<<<<< HEAD
                        # å¯¹é½ç‰¹å¾åˆ—ï¼šé¿å…å› æŒ‡çº¹â€œå…¨é›¶åˆ—è¢«åˆ â€å¯¼è‡´æ¨¡å‹æ‰€éœ€åˆ—ç¼ºå¤±
                        missing_cols = [c for c in feature_cols if c not in pred_df.columns]
                        if missing_cols:
                            # æŒ‡çº¹ç¼ºå¤±åˆ—å¡« 0ï¼›å…¶å®ƒç¼ºå¤±åˆ—å¡« NaNï¼ˆåç»­ imputer å¯å¤„ç†ï¼‰
                            fp_missing = [c for c in missing_cols if ("maccs" in c.lower()) or ("morgan" in c.lower())]
                            other_missing = [c for c in missing_cols if c not in fp_missing]
                            for c in fp_missing:
                                pred_df[c] = 0
                            for c in other_missing:
                                pred_df[c] = np.nan
                            st.warning(f"æ£€æµ‹åˆ°æ¨¡å‹æ‰€éœ€ç‰¹å¾åˆ—ç¼ºå¤± {len(missing_cols)} ä¸ªï¼Œå·²è‡ªåŠ¨è¡¥é½ï¼ˆæŒ‡çº¹åˆ—å¡«0ï¼Œå…¶å®ƒåˆ—å¡«NaNï¼‰ã€‚")
=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4
                        X_pred = pred_df[feature_cols]

                        if pipeline is not None:
                            preds = pipeline.predict(X_pred)
                        else:
                            X_arr = X_pred.values
                            if imputer is not None:
                                X_arr = imputer.transform(X_arr)
                            if scaler is not None:
                                X_arr = scaler.transform(X_arr)
                            preds = model.predict(X_arr)

                        pred_df['prediction'] = preds
                        st.success("âœ… æ‰¹é‡é¢„æµ‹å®Œæˆ")
                        st.dataframe(pred_df.head(20), use_container_width=True)

                        csv = pred_df.to_csv(index=False).encode('utf-8')
                        st.download_button("ğŸ“¥ ä¸‹è½½é¢„æµ‹ç»“æœ CSV", csv, "batch_predictions.csv", "text/csv")

            except Exception as e:
                st.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")

    # ============================================================
    # Tab3: é€‚ç”¨åŸŸåˆ†æ
    # ============================================================
    with tab3:
        st.markdown("### ğŸ¯ é€‚ç”¨åŸŸåˆ†æ")
        st.caption("é€‚ç”¨åŸŸç”¨äºåˆ¤æ–­æ–°æ ·æœ¬æ˜¯å¦â€œè¶…å‡ºè®­ç»ƒæ•°æ®è¦†ç›–èŒƒå›´â€ã€‚PCA Hull æ›´é€šç”¨ï¼›Tanimoto æ›´é€‚ç”¨äºæŒ‡çº¹ç‰¹å¾ã€‚")

        # å¯ç”¨æ–¹æ³•
        fp_cols = [c for c in feature_cols if ("morgan" in c.lower()) or ("maccs" in c.lower())]
        has_fp = len(fp_cols) > 0

        methods = ["PCA Hullï¼ˆæ•°å€¼ç©ºé—´ï¼‰"]
        if has_fp and st.session_state.get("train_result") is not None and "X_train_raw" in st.session_state.train_result:
            methods.append("Tanimoto ç›¸ä¼¼åº¦ï¼ˆæŒ‡çº¹ï¼‰")

        ad_method = st.selectbox("é€‰æ‹©é€‚ç”¨åŸŸæ–¹æ³•", options=methods, index=0)

        # -------- PCA Hull --------
        if ad_method.startswith("PCA"):
            st.info("PCA Hullï¼šåœ¨é™ç»´ç©ºé—´æ„å»ºå‡¸åŒ…ï¼Œåˆ¤æ–­æ–°æ ·æœ¬æ˜¯å¦è½åœ¨è®­ç»ƒé›†è¦†ç›–èŒƒå›´å†…ï¼ˆå¯¹ä»»æ„æ•°å€¼ç‰¹å¾é€šç”¨ï¼‰ã€‚")

            input_df = None
            if len(feature_cols) <= 60:
                input_values = {}
                cols = st.columns(3)
                for i, feature in enumerate(feature_cols):
                    with cols[i % 3]:
                        input_values[feature] = st.number_input(feature, value=0.0, key=f"ad_pca_{feature}")
                input_df = pd.DataFrame([input_values])
            else:
                st.info(f"å½“å‰ç‰¹å¾æ•°é‡è¾ƒå¤šï¼ˆ{len(feature_cols)}ï¼‰ï¼Œå»ºè®®ç”¨â€œå•è¡Œæ–‡ä»¶â€ä¸Šä¼ è¿›è¡Œé€‚ç”¨åŸŸåˆ¤æ–­ã€‚")
                single_file = st.file_uploader("ä¸Šä¼ å•æ ·æœ¬æ–‡ä»¶ï¼ˆCSV/Excelï¼Œä¸€è¡Œå³å¯ï¼‰", type=['csv', 'xlsx', 'xls'], key="ad_single_file_pca")
                if single_file is not None:
                    try:
                        tmp_df = load_data_file(single_file)
                        if tmp_df.shape[0] > 0:
                            row_idx = 0
                            if tmp_df.shape[0] > 1:
                                row_idx = st.number_input("é€‰æ‹©åˆ†æçš„è¡Œå·ï¼ˆä» 0 å¼€å§‹ï¼‰", 0, int(tmp_df.shape[0]-1), 0, key="ad_pca_row_idx")
                            input_df = tmp_df.iloc[[int(row_idx)]].copy()
                    except Exception as e:
                        st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")

            if st.button("ğŸ¯ é€‚ç”¨åŸŸåˆ†æï¼ˆPCA Hullï¼‰", type="primary"):
                if input_df is None:
                    st.error("è¯·å…ˆæä¾›è¾“å…¥æ ·æœ¬")
                else:
                    missing = [c for c in feature_cols if c not in input_df.columns]
                    if missing:
                        st.error(f"è¾“å…¥æ ·æœ¬ç¼ºå°‘ç‰¹å¾åˆ—: {missing[:10]}{'...' if len(missing)>10 else ''}")
                    else:
                        X_input = input_df[feature_cols].values
                        if imputer is not None:
                            X_input = imputer.transform(X_input)
                        if scaler is not None:
                            X_input = scaler.transform(X_input)

                        analyzer = ApplicabilityDomainAnalyzer(st.session_state.X_train.values)
                        analyzer.fit()

                        in_domain, distance = analyzer.is_within_domain(X_input)

                        if in_domain:
                            st.success(f"âœ… æ ·æœ¬åœ¨é€‚ç”¨åŸŸå†… (distance={distance:.4f})")
                        else:
                            st.warning(f"âš ï¸ æ ·æœ¬å¯èƒ½è¶…å‡ºé€‚ç”¨åŸŸ (distance={distance:.4f})")

        # -------- Tanimoto Similarity --------
        else:
            if not has_fp:
                st.warning("å½“å‰ç‰¹å¾ä¸­æœªæ£€æµ‹åˆ° MACCS/Morgan æŒ‡çº¹åˆ—ï¼Œæ— æ³•ä½¿ç”¨ Tanimoto é€‚ç”¨åŸŸã€‚")
            else:
                st.info("Tanimotoï¼šè®¡ç®—æ–°æ ·æœ¬ä¸è®­ç»ƒé›†ä¸­æœ€è¿‘é‚»çš„æŒ‡çº¹ç›¸ä¼¼åº¦ sim_maxã€‚sim_max è¿‡ä½é€šå¸¸æ„å‘³ç€ out-of-domainã€‚")

                threshold = st.slider("ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå»ºè®® 0.20~0.30ï¼‰", 0.0, 1.0, 0.25, 0.01)
                top_k = st.slider("Top-K ç›¸ä¼¼æ ·æœ¬", 1, 20, 5)

                # æ„é€ è®­ç»ƒæŒ‡çº¹çŸ©é˜µ
                try:
                    X_train_raw = st.session_state.train_result["X_train_raw"]
                    y_train = st.session_state.train_result.get("y_train")
                    X_train_fp = X_train_raw[fp_cols]
                    analyzer = TanimotoADAnalyzer(X_train_fp, threshold=threshold, max_train_samples=5000, random_state=42)
                except Exception as e:
                    st.error(f"åˆå§‹åŒ– Tanimoto åˆ†æå™¨å¤±è´¥: {e}")
                    analyzer = None

                st.markdown("#### 1) å•æ ·æœ¬åˆ†æï¼ˆæ¨èï¼šä¸Šä¼ å•è¡Œæ–‡ä»¶ï¼‰")
                single_file = st.file_uploader("ä¸Šä¼ å•æ ·æœ¬æ–‡ä»¶ï¼ˆCSV/Excelï¼Œä¸€è¡Œå³å¯ï¼Œéœ€åŒ…å«æŒ‡çº¹åˆ—ï¼‰", type=['csv', 'xlsx', 'xls'], key="ad_single_file_tanimoto")
                if analyzer is not None and single_file is not None:
                    try:
                        qdf = load_data_file(single_file)
                        if qdf.shape[0] == 0:
                            st.error("æ–‡ä»¶ä¸ºç©º")
                        else:
                            row_idx = 0
                            if qdf.shape[0] > 1:
                                row_idx = st.number_input("é€‰æ‹©åˆ†æçš„è¡Œå·ï¼ˆä» 0 å¼€å§‹ï¼‰", 0, int(qdf.shape[0]-1), 0, key="ad_tani_row_idx")
                            qrow = qdf.iloc[int(row_idx)]
                            missing = [c for c in fp_cols if c not in qdf.columns]
                            if missing:
                                st.error(f"ç¼ºå°‘æŒ‡çº¹åˆ—: {missing[:10]}{'...' if len(missing)>10 else ''}")
                            else:
                                is_in, sim_max, top_df, fig = analyzer.analyze_single(qrow[fp_cols].values, top_k=top_k, threshold=threshold)
                                if is_in:
                                    st.success(f"âœ… åœ¨é€‚ç”¨åŸŸå†…ï¼šsim_max = {sim_max:.3f}")
                                else:
                                    st.warning(f"âš ï¸ å¯èƒ½è¶…å‡ºé€‚ç”¨åŸŸï¼šsim_max = {sim_max:.3f}")

                                # è¡¥å……ï¼šæ˜¾ç¤º top-k çš„ y_trainï¼ˆè‹¥æœ‰ï¼‰
                                if y_train is not None and len(y_train) >= top_df.shape[0]:
                                    top_df = top_df.copy()
                                    try:
                                        top_df["y_train"] = [float(y_train[int(i)]) for i in top_df["train_index"].values]
                                    except Exception:
                                        pass

                                st.dataframe(top_df, use_container_width=True, height=200)
                                st.pyplot(fig, use_container_width=True)

                    except Exception as e:
                        st.error(f"åˆ†æå¤±è´¥: {e}")

                st.markdown("---")
                st.markdown("#### 2) æ‰¹é‡åˆ†æï¼ˆè¾“å‡º sim_max / in_domainï¼‰")
                batch_file = st.file_uploader("ä¸Šä¼ æ‰¹é‡æ ·æœ¬æ–‡ä»¶ï¼ˆCSV/Excelï¼‰", type=['csv', 'xlsx', 'xls'], key="ad_batch_file_tanimoto")

                if analyzer is not None and batch_file is not None:
                    try:
                        qdf = load_data_file(batch_file)
                        missing = [c for c in fp_cols if c not in qdf.columns]
                        if missing:
                            st.error(f"ç¼ºå°‘æŒ‡çº¹åˆ—: {missing[:10]}{'...' if len(missing)>10 else ''}")
                        else:
                            sim_max_arr = analyzer.compute_batch_max_similarity(qdf[fp_cols])
                            out_df = qdf.copy()
                            out_df["sim_max"] = sim_max_arr
                            out_df["in_domain"] = out_df["sim_max"] >= threshold

                            st.success("âœ… æ‰¹é‡é€‚ç”¨åŸŸåˆ†æå®Œæˆ")
                            st.dataframe(out_df[["sim_max", "in_domain"]].head(20), use_container_width=True)

                            # å¯é€‰ï¼šå¦‚æœåŒ…å«ç›®æ ‡åˆ—ï¼Œå¯ç”» |error| vs sim_max
                            if st.session_state.get("target_col") in out_df.columns:
                                try:
                                    y_true = pd.to_numeric(out_df[st.session_state.target_col], errors="coerce")
                                    ok = y_true.notna()
                                    if ok.sum() >= 10:
                                        # é¢„æµ‹
                                        if pipeline is not None:
                                            y_pred = pipeline.predict(out_df.loc[ok, feature_cols])
                                        else:
                                            X_arr = out_df.loc[ok, feature_cols].values
                                            if imputer is not None:
                                                X_arr = imputer.transform(X_arr)
                                            if scaler is not None:
                                                X_arr = scaler.transform(X_arr)
                                            y_pred = model.predict(X_arr)
                                        abs_err = np.abs(y_true.loc[ok].values - y_pred)

                                        fig, ax = plt.subplots(figsize=(7, 4))
                                        ax.scatter(out_df.loc[ok, "sim_max"].values, abs_err, alpha=0.7, edgecolors="k", linewidth=0.3)
                                        ax.set_xlabel("sim_max (Tanimoto)")
                                        ax.set_ylabel("|error|")
                                        ax.set_title("|error| vs sim_max")
                                        ax.grid(True, linestyle="--", alpha=0.4)
                                        plt.tight_layout()
                                        st.pyplot(fig, use_container_width=True)
                                except Exception:
                                    pass

                            csv = out_df.to_csv(index=False).encode('utf-8')
                            st.download_button("ğŸ“¥ ä¸‹è½½é€‚ç”¨åŸŸç»“æœ CSV", csv, "tanimoto_ad_results.csv", "text/csv")

                    except Exception as e:
                        st.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {e}")

def page_hyperparameter_optimization():
    """è¶…å‚æ•°ä¼˜åŒ–é¡µé¢"""
    st.title("âš™ï¸ è¶…å‚æ•°ä¼˜åŒ–")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    if not st.session_state.feature_cols or not st.session_state.target_col:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ç‰¹å¾é€‰æ‹©é¡µé¢é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡å˜é‡")
        return

    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    feature_cols = st.session_state.feature_cols
    target_col = st.session_state.target_col

    X = df[feature_cols]
    y = df[target_col]

    st.markdown("### Optunaæ™ºèƒ½è¶…å‚æ•°ä¼˜åŒ–")

    col1, col2 = st.columns(2)


    disable_opt = False
    with col1:
        trainer = EnhancedModelTrainer()
        available_models = trainer.get_available_models()

        # æ”¯æŒä¼˜åŒ–çš„æ¨¡å‹
        optimizable_models = [
            "éšæœºæ£®æ—", "XGBoost", "LightGBM", "CatBoost",
            "SVR", "Ridgeå›å½’", "Lassoå›å½’", "ElasticNet",
            "AdaBoost", "æ¢¯åº¦æå‡æ ‘",
            "TensorFlow Sequential"
        ]
        optimizable_models = [m for m in optimizable_models if m in available_models]

        # å…¼å®¹ï¼šå³ä½¿å½“å‰ç¯å¢ƒç¼ºå°‘ä¾èµ–ï¼Œä¹Ÿæ˜¾ç¤º TFS å…¥å£ï¼ˆé¿å…â€œåŠŸèƒ½å­˜åœ¨ä½†ç•Œé¢ä¸æ˜¾ç¤ºâ€ï¼‰
        if "TensorFlow Sequential" not in optimizable_models:
            optimizable_models.append("TensorFlow Sequential")


        def _fmt_model_name(n: str) -> str:
            if n == "TensorFlow Sequential":
                if TENSORFLOW_AVAILABLE:
                    return "TFS (TensorFlow Sequential) âœ…"
                return "TFS (TensorFlow Sequential) â›” éœ€è¦å®‰è£… TensorFlow"
            return n

        model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", optimizable_models, format_func=_fmt_model_name)

        # æœªå®‰è£… TensorFlow æ—¶ï¼šä»æ˜¾ç¤ºå…¥å£ï¼Œä½†ç¦ç”¨ä¼˜åŒ–æŒ‰é’®å¹¶æç¤ºå®‰è£…
        disable_opt = (model_name == "TensorFlow Sequential" and (not TENSORFLOW_AVAILABLE))
        if disable_opt:
            st.warning("æ£€æµ‹åˆ°å½“å‰ç¯å¢ƒæœªå®‰è£… TensorFlowï¼ŒTFS æ¨¡å‹æš‚ä¸å¯è¿›è¡Œ Optuna ä¼˜åŒ–ã€‚è¯·å…ˆå®‰è£…ä¾èµ–ï¼š`pip install tensorflow`ï¼ˆæˆ–æŒ‰ä½ çš„ç¡¬ä»¶é€‰æ‹© tensorflow-cpu / tensorflow-gpuï¼‰ã€‚")
    with col2:
        n_trials = st.slider("ä¼˜åŒ–è½®æ•°", 10, 200, DEFAULT_OPTUNA_TRIALS)
        cv_folds = st.slider("äº¤å‰éªŒè¯æŠ˜æ•°", 3, 10, 5)

    # --- [æ–°å¢] è¿›åº¦æ¡ç»„ä»¶ ---
    progress_bar = st.progress(0)
    status_text = st.empty()

    if st.button("ğŸš€ å¼€å§‹ä¼˜åŒ–", type="primary", disabled=disable_opt):
        try:
            optimizer = HyperparameterOptimizer()

            # å®šä¹‰è¿›åº¦æ›´æ–°å›è°ƒ
            def update_progress(p):
                progress_bar.progress(min(p, 1.0))
                status_text.text(f"æ­£åœ¨è¿›è¡Œä¼˜åŒ–... è¿›åº¦: {int(p * 100)}%")

            with st.spinner(f"æ­£åœ¨ä¼˜åŒ– {model_name}..."):
                # ä¼ é€’ progress_callback
                best_params, best_score, study = optimizer.optimize(
                    model_name, X, y,
                    n_trials=n_trials,
                    cv=cv_folds,
                    progress_callback=update_progress
                )

            # ä¼˜åŒ–å®Œæˆï¼Œè¿›åº¦æ¡æ»¡
            progress_bar.progress(100)
            status_text.text("ä¼˜åŒ–å®Œæˆï¼")

            st.success(f"âœ… ä¼˜åŒ–å®Œæˆï¼æœ€ä½³RÂ²åˆ†æ•°: {best_score:.4f}")

            # ä¿å­˜åˆ° session_state
            st.session_state.best_params = best_params
            st.session_state.best_score = best_score
            st.session_state.optimized_model_name = model_name  # è®°å½•ä¼˜åŒ–çš„æ˜¯å“ªä¸ªæ¨¡å‹

            # [æ–°å¢] è®°å½•åˆ°çŠ¶æ€æ¡
            try:
                log_fe_step(
                    operation="è¶…å‚ä¼˜åŒ–",
                    description=f"ä¼˜åŒ–å®Œæˆ: {model_name}",
                    params={
                        "model": model_name,
                        "n_trials": int(n_trials),
                        "cv_folds": int(cv_folds),
                        **(best_params or {})
                    },
                    input_df=df,
                    status="success",
                    message=f"best_r2={best_score:.4f}"
                )
            except Exception:
                pass

            st.markdown("### æœ€ä½³å‚æ•°")
            st.json(best_params)

            # ä¼˜åŒ–å†å²å¯è§†åŒ–
            if study is not None:
                st.markdown("### ä¼˜åŒ–å†å²")

                try:
                    import plotly.graph_objects as go

                    trials = study.trials
                    values = [t.value for t in trials if t.value is not None]

                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        y=values,
                        mode='lines+markers',
                        name='RÂ² Score'
                    ))
                    fig.update_layout(
                        title='ä¼˜åŒ–è¿‡ç¨‹',
                        xaxis_title='Trial',
                        yaxis_title='RÂ² Score'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                except:
                    pass

            # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒ
            if st.button("ğŸ¯ ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹"):
                result = trainer.train_model(
                    X, y,
                    model_name=model_name,
                    **best_params
                )

                st.session_state.model = result['model']
                st.session_state.pipeline = result.get('pipeline')
                st.session_state.scaler = result.get('scaler')
                st.session_state.imputer = result.get('imputer')
                st.session_state.X_train = result.get('X_train')
                st.session_state.X_test = result.get('X_test')
                st.session_state.y_train = result.get('y_train')
                st.session_state.y_test = result.get('y_test')
                st.session_state.model_name = model_name
                st.session_state.train_result = result
                st.session_state.cv_result = None
                st.session_state.best_params = best_params

                st.success(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼RÂ²: {result['r2']:.4f}")

        except Exception as e:
            st.error(f"âŒ ä¼˜åŒ–å¤±è´¥: {str(e)}")
            st.code(traceback.format_exc())
            # [æ–°å¢] å¤±è´¥ä¹Ÿå†™å…¥çŠ¶æ€æ¡
            try:
                log_fe_step(
                    operation="è¶…å‚ä¼˜åŒ–",
                    description=f"ä¼˜åŒ–å¤±è´¥: {model_name}",
                    params={"model": model_name, "n_trials": int(n_trials), "cv_folds": int(cv_folds)},
                    input_df=df,
                    status="error",
                    message=str(e)
                )
            except Exception:
                pass


# ============================================================
# é¡µé¢ï¼šä¸»åŠ¨å­¦ä¹ ï¼ˆActive Learningï¼‰
# ============================================================
def page_active_learning():
    """ä¸»åŠ¨å­¦ä¹ é¡µé¢ï¼šåŸºäºä¸ç¡®å®šæ€§æ¨èä¸‹ä¸€æ‰¹å®éªŒ/æ¨¡æ‹Ÿæ ·æœ¬"""
    st.title("ğŸ§  ä¸»åŠ¨å­¦ä¹  (Active Learning)")

    st.markdown(
        """
ä¸»åŠ¨å­¦ä¹ é€‚ç”¨äº **é«˜åˆ†å­/ç¯æ°§æ ‘è„‚/å¤æ** è¿™ç±»â€œå°æ ·æœ¬ + å•æ¬¡å®éªŒ/æ¨¡æ‹Ÿæˆæœ¬é«˜â€çš„åœºæ™¯ã€‚

å…¸å‹é—­ç¯ï¼š
1) ç”¨å°‘é‡å·²æ ‡æ³¨æ•°æ®è®­ç»ƒä»£ç†æ¨¡å‹ï¼ˆsurrogateï¼‰
2) åœ¨å€™é€‰æ± ä¸­ç”¨é‡‡é›†å‡½æ•°é€‰æ‹©â€œæœ€å€¼å¾—åšâ€çš„ä¸‹ä¸€æ‰¹æ ·æœ¬ï¼ˆä¸ç¡®å®šæ€§/æœŸæœ›æå‡ï¼‰
3) åšå®éªŒæˆ– MD æ¨¡æ‹Ÿå¾—åˆ°çœŸå®æ ‡ç­¾
4) å›å¡«æ•°æ®ï¼Œé‡å¤ 1-3
        """
    )

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    if df is None or df.empty:
        st.warning("âš ï¸ å½“å‰æ•°æ®ä¸ºç©º")
        return

    # ---- é€‰æ‹©ç›®æ ‡ä¸ç‰¹å¾ ----
    st.markdown("### 1) é€‰æ‹©ç›®æ ‡å˜é‡ä¸ç‰¹å¾")

    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if not num_cols:
        st.error("âŒ å½“å‰æ•°æ®æ²¡æœ‰æ•°å€¼åˆ—ï¼›ä¸»åŠ¨å­¦ä¹ éœ€è¦æ•°å€¼ç‰¹å¾ X å’Œæ•°å€¼ç›®æ ‡ y")
        return

    # é»˜è®¤ç›®æ ‡ï¼šä¼˜å…ˆä½¿ç”¨ session_state.target_col
    default_target = st.session_state.get('target_col')
    if default_target not in num_cols:
        default_target = num_cols[-1]
    target_col = st.selectbox("ç›®æ ‡å˜é‡ (y)", options=num_cols, index=num_cols.index(default_target))

    # é»˜è®¤ç‰¹å¾ï¼šä¼˜å…ˆä½¿ç”¨ session_state.feature_cols
    default_features = [c for c in (st.session_state.get('feature_cols') or []) if c in df.columns and c != target_col]
    if not default_features:
        default_features = [c for c in num_cols if c != target_col]

    feature_cols = st.multiselect(
        "ç‰¹å¾åˆ— (X)",
        options=[c for c in df.columns if c != target_col],
        default=default_features,
        help="å»ºè®®ä½¿ç”¨â€˜ç‰¹å¾é€‰æ‹©â€™é¡µé¢ç­›è¿‡çš„ç‰¹å¾ï¼›ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œæ‰‹åŠ¨æŒ‡å®šã€‚"
    )

    if not feature_cols:
        st.warning("âš ï¸ è¯·è‡³å°‘é€‰æ‹© 1 ä¸ªç‰¹å¾åˆ—")
        return

    # ---- æ„å»º labeled/pool ----
    y_all = pd.to_numeric(df[target_col], errors='coerce')
    df_labeled = df.loc[y_all.notna()].copy()

    st.markdown("### 2) é€‰æ‹©å€™é€‰æ± ï¼ˆunlabeled poolï¼‰")

    pool_mode = st.radio(
        "å€™é€‰æ± æ¥æº",
        [
            "ä½¿ç”¨å½“å‰æ•°æ®ä¸­ç›®æ ‡ç¼ºå¤±çš„è¡Œï¼ˆæ¨èï¼šå…ˆå¯¼å…¥å€™é€‰é…æ–¹ï¼Œå†é€æ­¥è¡¥å®éªŒï¼‰",
            "ä¸Šä¼ å€™é€‰æ± æ–‡ä»¶ï¼ˆCSV/Excelï¼Œéœ€åŒ…å«ç›¸åŒç‰¹å¾åˆ—ï¼‰",
        ],
        index=0
    )

    df_pool = None
    if pool_mode.startswith("ä½¿ç”¨å½“å‰æ•°æ®"):
        df_pool = df.loc[y_all.isna()].copy()
        st.caption(f"å½“å‰å€™é€‰æ± å¤§å°: {0 if df_pool is None else len(df_pool)}")
    else:
        up = st.file_uploader("ä¸Šä¼ å€™é€‰æ± æ–‡ä»¶", type=["csv", "xlsx", "xls"], key="al_pool_upload")
        if up is not None:
            try:
                df_pool = load_data_file(up)
                st.success(f"âœ… å·²åŠ è½½å€™é€‰æ± : {df_pool.shape}")
            except Exception as e:
                st.error(f"âŒ å€™é€‰æ± æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
                return

    if df_pool is None or df_pool.empty:
        st.warning("âš ï¸ å€™é€‰æ± ä¸ºç©ºï¼šè¯·åœ¨æ•°æ®ä¸­å‡†å¤‡ä¸€äº›ç›®æ ‡ç¼ºå¤±çš„å€™é€‰æ ·æœ¬ï¼Œæˆ–ä¸Šä¼ å€™é€‰æ± æ–‡ä»¶")
        return

    # æ£€æŸ¥åˆ—
    missing_in_pool = [c for c in feature_cols if c not in df_pool.columns]
    if missing_in_pool:
        st.error(f"âŒ å€™é€‰æ± ç¼ºå°‘ç‰¹å¾åˆ—: {missing_in_pool}\nè¯·ç¡®ä¿å€™é€‰æ± ä¸è®­ç»ƒæ•°æ®çš„ç‰¹å¾åˆ—ä¸€è‡´ã€‚")
        return

    # ---- é€‰æ‹©æ¨¡å‹ä¸é‡‡é›†ç­–ç•¥ ----
    st.markdown("### 3) é€‰æ‹©ä¸ç¡®å®šæ€§æ¨¡å‹ä¸é‡‡é›†ç­–ç•¥")

    col_a, col_b, col_c = st.columns(3)
    with col_a:
        model_ui = st.selectbox(
            "ä¸ç¡®å®šæ€§æ¨¡å‹",
            [
                "Gaussian Process (GPR, å°æ ·æœ¬æ¨è)",
                "éšæœºæ£®æ— (RF, é€‚åˆå¼ºéçº¿æ€§)",
                "Extra Trees (ETR, æ›´å¼ºéšæœºæ€§)",
            ],
            index=0
        )
    with col_b:
        acq_ui = st.selectbox(
            "é‡‡é›†ç­–ç•¥",
            [
                "æœ€å¤§ä¸ç¡®å®šæ€§ (Uncertainty)",
                "UCB ä¸Šç½®ä¿¡ç•Œ (Exploration+Exploitation)",
                "EI æœŸæœ›æå‡ (Expected Improvement)",
            ],
            index=0
        )
    with col_c:
        batch_size = st.slider("æ¨èæ•°é‡", 1, 50, 10)

    minimize = st.checkbox(
        "ç›®æ ‡æ˜¯ã€æœ€å°åŒ–ã€‘ï¼ˆä¾‹å¦‚ï¼šé»åº¦/æˆæœ¬/æ”¶ç¼©ç‡ï¼‰",
        value=False,
        help="è‹¥ç›®æ ‡æ˜¯è¶Šå¤§è¶Šå¥½ï¼ˆä¾‹å¦‚ Tg/æ¨¡é‡/å¼ºåº¦ï¼‰ï¼Œè¯·ä¿æŒä¸å‹¾é€‰ã€‚"
    )

    # EI/UCB å‚æ•°
    col_p1, col_p2 = st.columns(2)
    with col_p1:
        xi = st.number_input("EI å‚æ•° xi", min_value=0.0, max_value=1.0, value=0.01, step=0.01,
                             disabled=("EI" not in acq_ui))
    with col_p2:
        kappa = st.number_input("UCB å‚æ•° kappa", min_value=0.0, max_value=10.0, value=2.0, step=0.5,
                                disabled=("UCB" not in acq_ui))

    if st.button("ğŸš€ ç”Ÿæˆä¸‹ä¸€æ‰¹å®éªŒ/æ¨¡æ‹Ÿå»ºè®®", type="primary"):
        try:
            from core.active_learning import recommend_from_dataframes

            model_kind = "gpr"
            if model_ui.startswith("éšæœºæ£®æ—"):
                model_kind = "rf"
            elif model_ui.startswith("Extra Trees"):
                model_kind = "etr"

            acq_kind = "uncertainty"
            if acq_ui.startswith("UCB"):
                acq_kind = "ucb"
            elif acq_ui.startswith("EI"):
                acq_kind = "ei"

            rec_df = recommend_from_dataframes(
                df_labeled=df_labeled,
                df_pool=df_pool,
                feature_cols=feature_cols,
                target_col=target_col,
                model_kind=model_kind,
                acq_kind=acq_kind,
                batch_size=int(batch_size),
                minimize=bool(minimize),
                xi=float(xi),
                kappa=float(kappa),
                random_state=DEFAULT_RANDOM_STATE,
            )

            st.session_state.al_recommendations = rec_df
            st.success(f"âœ… å·²ç”Ÿæˆæ¨èåˆ—è¡¨ï¼ˆTop-{len(rec_df)}ï¼‰")
        except Exception as e:
            st.error(f"âŒ ä¸»åŠ¨å­¦ä¹ è®¡ç®—å¤±è´¥: {e}")
            st.code(traceback.format_exc())

    # ---- å±•ç¤ºç»“æœ ----
    if st.session_state.get('al_recommendations') is not None:
        rec_df = st.session_state.al_recommendations
        st.markdown("### 4) æ¨èç»“æœ")
        st.dataframe(rec_df, use_container_width=True)

        # å¯¼å‡º
        csv_bytes = rec_df.to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            "â¬‡ï¸ ä¸‹è½½æ¨èåˆ—è¡¨ï¼ˆCSVï¼‰",
            data=csv_bytes,
            file_name="active_learning_recommendations.csv",
            mime="text/csv"
        )

        st.markdown(
            """
**ä¸‹ä¸€æ­¥æ€ä¹ˆåšï¼Ÿ**
- å¯¹è¡¨ä¸­ Top-N å€™é€‰é…æ–¹è¿›è¡Œå®éªŒåˆæˆ/å›ºåŒ–/æµ‹è¯•ï¼ˆæˆ– MD è™šæ‹Ÿå›ºåŒ– + æ€§èƒ½è®¡ç®—ï¼‰ã€‚
- æŠŠæµ‹å¾—çš„ç›®æ ‡å€¼å›å¡«åˆ°æ•°æ®è¡¨å¯¹åº”è¡Œï¼ˆå¡«åˆ°ä½ é€‰æ‹©çš„ç›®æ ‡åˆ—é‡Œï¼‰ã€‚
- é‡æ–°è¿è¡Œæœ¬é¡µé¢ï¼Œå³å¯è¿›å…¥ä¸‹ä¸€è½®ä¸»åŠ¨å­¦ä¹ ã€‚
            """
        )


# ============================================================
# é¡µé¢ï¼šè®­ç»ƒè®°å½•ï¼ˆå†å² Runï¼‰
# ============================================================
def page_training_records():
    st.title("ğŸ“ˆ è®­ç»ƒè®°å½•")
    st.caption("è‡ªåŠ¨ä¿å­˜æ¯æ¬¡è®­ç»ƒçš„æŒ‡æ ‡ã€å‚æ•°ã€è®­ç»ƒæ›²çº¿ï¼ˆloss/è¿­ä»£æŒ‡æ ‡æˆ–å­¦ä¹ æ›²çº¿ï¼‰ã€‚")

    manager = TrainingRunManager()
    runs = manager.list_runs(limit=200)
    if not runs:
        st.info("æš‚æ— è®­ç»ƒè®°å½•ã€‚è¯·å…ˆåœ¨ã€ğŸ¤– æ¨¡å‹è®­ç»ƒã€‘é¡µé¢å®Œæˆä¸€æ¬¡è®­ç»ƒã€‚")
        return

    # é€‰æ‹© Run
    def _label(r):
        s = f"{r.run_id}ï½œ{r.model_name}"
        if r.r2 is not None:
            s += f"ï½œRÂ²={r.r2:.4f}"
        return s

    options = { _label(r): r.run_id for r in runs }
    sel = st.selectbox("é€‰æ‹©ä¸€æ¡è®­ç»ƒè®°å½•", options=list(options.keys()), index=0)
    run_id = options[sel]

    payload = manager.load_run(run_id)
    meta = payload.get("metadata") or {}
    hist_df = payload.get("history")

    # æŒ‡æ ‡æ¦‚è§ˆ
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("æ¨¡å‹", str(meta.get("model_name", "")) or "-")
    c2.metric("RÂ²", f"{float(meta.get('r2', 0)):.4f}" if meta.get("r2") is not None else "-")
    c3.metric("RMSE", f"{float(meta.get('rmse', 0)):.4f}" if meta.get("rmse") is not None else "-")
    c4.metric("MAE", f"{float(meta.get('mae', 0)):.4f}" if meta.get("mae") is not None else "-")

    st.markdown("---")
    st.markdown("### ğŸ“‰ è®­ç»ƒæ›²çº¿")
    if payload.get("training_curve_png"):
        st.image(payload["training_curve_png"], use_container_width=True)
    else:
        st.info("è¯¥è®°å½•æœªåŒ…å«è®­ç»ƒæ›²çº¿å›¾ç‰‡ï¼ˆå¯èƒ½ä¸ºæ—§ç‰ˆæœ¬è®°å½•ï¼‰ã€‚")

    if hist_df is not None and not hist_df.empty:
        st.markdown("### ğŸ§¾ è®­ç»ƒå†å²æ•°æ®")
        st.dataframe(hist_df, use_container_width=True, height=260)
        st.download_button(
            "ğŸ“¥ ä¸‹è½½è®­ç»ƒå†å² CSV",
            data=hist_df.to_csv(index=False).encode("utf-8-sig"),
            file_name=f"{run_id}_history.csv",
            mime="text/csv",
        )

    with st.expander("ğŸ” æŸ¥çœ‹å…ƒæ•°æ®ï¼ˆå‚æ•°/åˆ‡åˆ†/æ—¶é—´ç­‰ï¼‰", expanded=False):
        st.json(meta)
        st.download_button(
            "ğŸ“¥ ä¸‹è½½ metadata.json",
            data=json.dumps(meta, ensure_ascii=False, indent=2).encode("utf-8"),
            file_name=f"{run_id}_metadata.json",
            mime="application/json",
        )

    extra_pngs = payload.get("extra_pngs") or {}
    if extra_pngs:
        with st.expander("ğŸ–¼ï¸ å…¶å®ƒå›¾è¡¨", expanded=False):
            for fn, b in extra_pngs.items():
                st.markdown(f"**{fn}**")
                st.image(b, use_container_width=True)


# ============================================================
# é¡µé¢ï¼šçŠ¶æ€æ¡è®°å½• / æ“ä½œæ—¥å¿—
# ============================================================
def page_status_log():
    """ç‰¹å¾å·¥ç¨‹çŠ¶æ€æ¡è®°å½•ä¸æ•°æ®å¯¼å‡º"""
    st.title("ğŸ“‹ çŠ¶æ€æ¡è®°å½•")

    tracker = st.session_state.get("fe_tracker", None)
    current_df = st.session_state.get('processed_data')
    if current_df is None:
        current_df = st.session_state.get('data')

    tab1, tab2, tab3 = st.tabs(["ğŸ“œ æ“ä½œè®°å½•", "ğŸ’¾ æ•°æ®å¯¼å‡º", "â„¹ï¸ å½“å‰çŠ¶æ€"])

    with tab1:
        render_status_panel(tracker)

    with tab2:
        if current_df is not None and not getattr(current_df, "empty", True):
            render_data_export_panel(current_df, tracker)
        else:
            st.info("æš‚æ— å¯å¯¼å‡ºçš„æ•°æ®ï¼Œè¯·å…ˆä¸Šä¼ /ç”Ÿæˆæ•°æ®ã€‚")

    with tab3:
        c1, c2, c3 = st.columns(3)
        c1.metric("å½“å‰æ•°æ®", "processed_data" if st.session_state.get('processed_data') is not None else ("data" if st.session_state.get('data') is not None else "æ— "))
        c2.metric("ç‰¹å¾æ•° (X)", len(st.session_state.get("feature_cols") or []))
        c3.metric("ç›®æ ‡ (Y)", st.session_state.get("target_col") or "æœªé€‰æ‹©")

        last = None
        if tracker is not None:
            try:
                last = tracker.get_last_step()
            except Exception:
                last = None

        st.markdown("---")
        if last:
            icon = {"success": "âœ…", "warning": "âš ï¸", "error": "âŒ"}.get(last.get("status", "success"), "â„¹ï¸")
            st.markdown(f"### æœ€è¿‘ä¸€æ¬¡æ“ä½œ\n{icon} **{last.get('operation', '')}** - {last.get('description', '')}")
            st.caption(f"#{last.get('step_id','?')} @ {last.get('timestamp','')}")
            if last.get("message"):
                st.info(last.get("message"))
        else:
            st.info("æš‚æ— æ“ä½œè®°å½•ã€‚å®Œæˆä¸€æ¬¡æ•°æ®å¤„ç†/ç‰¹å¾é€‰æ‹©/è®­ç»ƒåï¼Œè¿™é‡Œä¼šè‡ªåŠ¨æ˜¾ç¤ºã€‚")


def render_export_section():
    """æ¸²æŸ“æ•°æ®å¯¼å‡ºåŒºåŸŸ"""
    st.markdown("---")
    st.markdown("## ğŸ“¥ æ•°æ®å¯¼å‡º")

    current_data = st.session_state.get('processed_data')  # æ³¨æ„ï¼šæ”¹ä¸º processed_data
    tracker = st.session_state.get('fe_tracker')

    if current_data is not None and not current_data.empty:
        render_data_export_panel(current_data, tracker)
    else:
        st.info("è¯·å…ˆåŠ è½½æ•°æ®åå†ä½¿ç”¨å¯¼å‡ºåŠŸèƒ½")


<<<<<<< HEAD

# ============================================================
# é¡µé¢ï¼šå›¾åƒ/æ–‡ä»¶è½¬SMILESï¼ˆDECIMERï¼‰
# ============================================================
def page_image_to_smiles():
    """ä½¿ç”¨ DECIMER ä»ç»“æ„å›¾ç‰‡/æ–‡ä»¶ä¸­è¯†åˆ« SMILES"""
    st.title("ğŸ–¼ï¸ å›¾åƒ/æ–‡ä»¶è½¬ SMILES")

    st.markdown(
        """
**è¯´æ˜**
- æœ¬åŠŸèƒ½ä½¿ç”¨ **DECIMER**ï¼ˆImage Transformerï¼‰å°†åŒ–å­¦ç»“æ„å›¾åƒè¯†åˆ«ä¸º SMILESã€‚
- **é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½é¢„è®­ç»ƒæƒé‡ï¼ˆéœ€è¦è”ç½‘ï¼‰**ï¼Œä¸‹è½½ä½ç½®ç”± DECIMER/pystow ç®¡ç†ã€‚
- æ”¯æŒï¼špng/jpg/jpeg/bmp/tif/tiff/webp/heif/heicï¼ŒPDFï¼ˆè‹¥å®‰è£… PyMuPDF æˆ– pdf2imageï¼‰ã€‚
- âš ï¸ **å»ºè®®ï¼šä¸€å¼ å›¾åªæ”¾ä¸€ä¸ªåˆ†å­ç»“æ„**ï¼ˆå¤šåˆ†å­æ‹¼å›¾/å¸¦å¤§é‡æ–‡å­—æ ‡æ³¨ä¼šæ˜¾è‘—é™ä½è¯†åˆ«å‡†ç¡®ç‡ï¼Œè¯·å…ˆè£å‰ªåå†ä¸Šä¼ ï¼‰ã€‚
        """
    )

    from core.image_smiles_extractor import decimer_is_available, smiles_from_bytes

    ok, msg = decimer_is_available()
    if not ok:
        st.error("DECIMER ä¾èµ–æœªå°±ç»ªï¼Œå½“å‰æ— æ³•è¯†åˆ«ã€‚")
        st.caption(msg)
        st.markdown("### âœ… å®‰è£…ä¾èµ–ï¼ˆå»ºè®®åœ¨å·²æ¿€æ´»çš„ç¯å¢ƒä¸­æ‰§è¡Œï¼‰")
        st.code(
            "\n".join(
                [
                    "pip install tensorflow>=2.12.0,<=2.20.0",
                    "pip install opencv-python pystow pillow-heif efficientnet selfies pyyaml",
                    "# è‹¥éœ€è¦ PDF æ”¯æŒï¼ˆäºŒé€‰ä¸€ï¼‰ï¼š",
                    "pip install pymupdf",
                    "# æˆ–ï¼špip install pdf2image  ï¼ˆç³»ç»Ÿéœ€é¢å¤–å®‰è£… popplerï¼‰",
                ]
            ),
            language="bash",
        )
        return

    col1, col2 = st.columns(2)
    with col1:
        hand_drawn = st.checkbox("æ‰‹ç»˜ç»“æ„æ¨¡å¼ï¼ˆHand-Drawnï¼‰", value=False)
    with col2:
        with_conf = st.checkbox("è¿”å›ç½®ä¿¡åº¦ï¼ˆTop-1ï¼‰", value=False)

    uploaded_files = st.file_uploader(
        "ä¸Šä¼ ç»“æ„å›¾ç‰‡æˆ– PDFï¼ˆå¯å¤šé€‰ï¼‰",
        type=["png", "jpg", "jpeg", "bmp", "tif", "tiff", "webp", "heif", "heic", "pdf"],
        accept_multiple_files=True,
    )

    if not uploaded_files:
        st.info("è¯·ä¸Šä¼ å›¾ç‰‡æˆ– PDF åå¼€å§‹è¯†åˆ«ã€‚")
        return

    results = []
    for uf in uploaded_files:
        st.markdown("---")
        st.subheader(f"ğŸ“„ {uf.name}")

        data = uf.getvalue()

        # Preview image (skip pdf)
        if uf.type and uf.type.startswith("image/"):
            try:
                st.image(data, caption=uf.name, use_container_width=True)
            except Exception:
                pass

        try:
            preds = smiles_from_bytes(
                data, uf.name, confidence=with_conf, hand_drawn=hand_drawn
            )
            for p in preds:
                page_tag = ""
                if p.page_index is not None:
                    page_tag = f" (Page {p.page_index + 1})"

                st.success(f"SMILES{page_tag}: {p.smiles}")
                if p.confidence is not None:
                    st.caption(f"Confidence (Top-1): {p.confidence:.4f}")

                results.append(
                    {
                        "filename": p.filename,
                        "page": None if p.page_index is None else int(p.page_index + 1),
                        "smiles": p.smiles,
                        "confidence": p.confidence,
                        "engine": p.engine,
                    }
                )
        except Exception as e:
            st.error(f"è¯†åˆ«å¤±è´¥ï¼š{e}")
            results.append(
                {
                    "filename": uf.name,
                    "page": None,
                    "smiles": "",
                    "confidence": None,
                    "engine": "DECIMER",
                    "error": str(e),
                }
            )

    if results:
        st.markdown("---")
        st.markdown("### ğŸ“Œ è¯†åˆ«ç»“æœæ±‡æ€»")
        df_res = pd.DataFrame(results)
        st.dataframe(df_res, use_container_width=True)

        csv_bytes = df_res.to_csv(index=False).encode("utf-8-sig")
        st.download_button(
            "â¬‡ï¸ ä¸‹è½½è¯†åˆ«ç»“æœ CSV",
            data=csv_bytes,
            file_name="smiles_results.csv",
            mime="text/csv",
        )

        # Save for later pages
        st.session_state["smiles_results"] = df_res
        st.info("ç»“æœå·²ä¿å­˜åˆ°ä¼šè¯å˜é‡ï¼šst.session_state['smiles_results']ã€‚")

=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4
# ============================================================
# ä¸»ç¨‹åºå…¥å£ï¼ˆä¿æŒåŸæœ‰ç»“æ„ï¼‰
# ============================================================
page = render_sidebar()

# [å¢å¼º] é¡¶éƒ¨è½»é‡çŠ¶æ€æ¡ï¼šé¿å…ä¾§è¾¹æ æŠ˜å åæ‰¾ä¸åˆ° TFS å…¥å£/æ“ä½œè®°å½•
render_top_status_bar()

if page == "ğŸ  é¦–é¡µ":
    page_home()
elif page == "ğŸ“¤ æ•°æ®ä¸Šä¼ ":
    page_data_upload()
elif page == "ğŸ” æ•°æ®æ¢ç´¢":
    page_data_explore()
elif page == "ğŸ§¹ æ•°æ®æ¸…æ´—":
    page_data_cleaning()
elif page == "âœ¨ æ•°æ®å¢å¼º":
    page_data_enhancement()
elif page == "ğŸ§¬ åˆ†å­ç‰¹å¾":
    page_molecular_features()
<<<<<<< HEAD

elif page == "ğŸ–¼ï¸ å›¾åƒè½¬SMILES":
    page_image_to_smiles()
=======
>>>>>>> f168256419b9b557a70253c84666a6aee162abf4
elif page == "ğŸ¯ ç‰¹å¾é€‰æ‹©":
    page_feature_selection()
elif page == "ğŸ¤– æ¨¡å‹è®­ç»ƒ":
    page_model_training()
elif page == "ğŸ“ˆ è®­ç»ƒè®°å½•":
    page_training_records()
elif page == "ğŸ“Š æ¨¡å‹è§£é‡Š":
    page_model_interpretation()
elif page == "ğŸ”® é¢„æµ‹åº”ç”¨":
    page_prediction()
elif page == "âš™ï¸ è¶…å‚ä¼˜åŒ–":
    page_hyperparameter_optimization()
elif page == "ğŸ§  ä¸»åŠ¨å­¦ä¹ ":
    page_active_learning()
elif page == "ğŸ“‹ çŠ¶æ€æ¡è®°å½•":
    page_status_log()