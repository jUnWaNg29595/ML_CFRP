# -*- coding: utf-8 -*-
"""
ç¢³çº¤ç»´å¤åˆææ–™æ™ºèƒ½é¢„æµ‹å¹³å° v1.2.9
æ›´æ–°å†…å®¹ï¼š
1. ä¿®å¤SHAPå›¾è¡¨æ˜¾ç¤ºå’Œç‰¹å¾åç¼ºå¤±é—®é¢˜
2. ä¼˜åŒ–æ‰€æœ‰å›¾è¡¨å¸ƒå±€ï¼Œé˜²æ­¢ç¼©æ”¾å˜å½¢
3. ä¸ºæ‰€æœ‰å›¾è¡¨å¢åŠ æ•°æ®å¯¼å‡º(CSV)åŠŸèƒ½
4. å¢åŠ åŒç»„åˆ†åˆ†å­æŒ‡çº¹æ‹¼æ¥åŠŸèƒ½
5. å¢åŠ è®­ç»ƒè„šæœ¬ä¸€é”®å¯¼å‡ºåŠŸèƒ½
"""
try:
    import torchani

    TORCHANI_AVAILABLE = True
except ImportError:
    TORCHANI_AVAILABLE = False
import streamlit as st
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import torch
import traceback
import json
import io
from datetime import datetime
import multiprocessing as mp
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="ç¢³çº¤ç»´å¤åˆææ–™æ™ºèƒ½é¢„æµ‹å¹³å°",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from core.data_processor import AdvancedDataCleaner, SparseDataHandler, DataEnhancer
from core.data_explorer import EnhancedDataExplorer
from core.model_trainer import EnhancedModelTrainer, AutoGluonWrapper  # ç¡®ä¿å¼•å…¥ Wrapper
from core.model_interpreter import ModelInterpreter, EnhancedModelInterpreter
from core.molecular_features import AdvancedMolecularFeatureExtractor, RDKitFeatureExtractor
from core.feature_selector import SmartFeatureSelector, SmartSparseDataSelector, show_robust_feature_selection
from core.optimizer import HyperparameterOptimizer, InverseDesigner, generate_tuning_suggestions
from core.visualizer import Visualizer
from core.applicability_domain import ApplicabilityDomainAnalyzer
from core.ui_config import (
    MANUAL_TUNING_PARAMS,
    MODEL_PARAMETERS,
    DEFAULT_OPTUNA_TRIALS,
    DEFAULT_TEST_SIZE,
    DEFAULT_RANDOM_STATE
)

from config import APP_NAME, VERSION, DATA_DIR
from generate_sample_data import generate_hybrid_dataset, generate_pure_numeric_dataset

# å¯é€‰æ¨¡å—å¯¼å…¥
try:
    from core.molecular_features import OptimizedRDKitFeatureExtractor, MemoryEfficientRDKitExtractor, \
        FingerprintExtractor

    OPTIMIZED_EXTRACTOR_AVAILABLE = True
except ImportError:
    OPTIMIZED_EXTRACTOR_AVAILABLE = False

try:
    from core.graph_utils import GNNFeaturizer, smiles_to_pyg_graph

    GNN_AVAILABLE = True
except ImportError:
    GNN_AVAILABLE = False

try:
    from core.ann_model import ANNRegressor

    ANN_AVAILABLE = True
except ImportError:
    ANN_AVAILABLE = False

try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False


@st.cache_data(ttl=3600, show_spinner="æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶...")
def load_data_file(uploaded_file):
    """å¸¦ç¼“å­˜çš„æ•°æ®åŠ è½½å‡½æ•°ï¼Œé¿å…æ¯æ¬¡äº¤äº’éƒ½é‡æ–°è¯»å–æ–‡ä»¶"""
    # å¿…é¡»é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å¤´ï¼Œå› ä¸ºStreamlitå¯èƒ½ä¼šå¤šæ¬¡è¯»å–åŒä¸€ä¸ªæ–‡ä»¶å¯¹è±¡
    uploaded_file.seek(0)

    if uploaded_file.name.endswith('.csv'):
        return pd.read_csv(uploaded_file)
    else:
        return pd.read_excel(uploaded_file)


# --- [æ–°å¢] ç”Ÿæˆç‹¬ç«‹è®­ç»ƒè„šæœ¬çš„å‡½æ•° ---
def generate_training_script_code(model_name, params, feature_cols, target_col):
    """ç”Ÿæˆç‹¬ç«‹çš„ Python è®­ç»ƒè„šæœ¬"""
    script_template = f'''# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨ç”Ÿæˆçš„æœºå™¨å­¦ä¹ è®­ç»ƒè„šæœ¬
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
æ¨¡å‹ç±»å‹: {model_name}
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
try: from xgboost import XGBRegressor
except ImportError: pass
try: from lightgbm import LGBMRegressor
except ImportError: pass
try: from catboost import CatBoostRegressor
except ImportError: pass

MODEL_NAME = "{model_name}"
FEATURE_COLS = {json.dumps(feature_cols, ensure_ascii=False)}
TARGET_COL = "{target_col}"
HYPERPARAMETERS = {json.dumps(params, indent=4, ensure_ascii=False)}
DATA_PATH = "data.csv" 

def load_and_train():
    print(f"æ­£åœ¨åŠ è½½æ•°æ®: {{DATA_PATH}}...")
    try:
        if DATA_PATH.endswith('.csv'): df = pd.read_csv(DATA_PATH)
        else: df = pd.read_excel(DATA_PATH)
    except FileNotFoundError:
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶")
        return

    X = df[FEATURE_COLS].values
    y = df[TARGET_COL].values
    y = pd.to_numeric(y, errors='coerce')
    mask = ~np.isnan(y)
    X, y = X[mask], y[mask]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    imputer = SimpleImputer(strategy='median')
    X_train = imputer.fit_transform(X_train)
    X_test = imputer.transform(X_test)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    print(f"æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹: {{MODEL_NAME}}...")
    model = None
    if MODEL_NAME == "éšæœºæ£®æ—": model = RandomForestRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "XGBoost": model = XGBRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "LightGBM": model = LGBMRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "CatBoost": model = CatBoostRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "SVR": model = SVR(**HYPERPARAMETERS)
    elif MODEL_NAME == "å†³ç­–æ ‘": model = DecisionTreeRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "æ¢¯åº¦æå‡æ ‘": model = GradientBoostingRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "AdaBoost": model = AdaBoostRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "å¤šå±‚æ„ŸçŸ¥å™¨": model = MLPRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "çº¿æ€§å›å½’": model = LinearRegression(**HYPERPARAMETERS)
    elif MODEL_NAME == "Ridgeå›å½’": model = Ridge(**HYPERPARAMETERS)
    elif MODEL_NAME == "Lassoå›å½’": model = Lasso(**HYPERPARAMETERS)
    elif MODEL_NAME == "ElasticNet": model = ElasticNet(**HYPERPARAMETERS)

    if model:
        print("å¼€å§‹è®­ç»ƒ...")
        model.fit(X_train, y_train)
        print("æ­£åœ¨è¯„ä¼°...")
        y_pred = model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        print(f"è®­ç»ƒå®Œæˆï¼RÂ² Score: {{r2:.4f}}")

if __name__ == "__main__":
    load_and_train()
'''
    return script_template


# --- å…¨å±€å¸¸é‡ ---
USER_DATA_DB = "datasets/user_data.csv"

# --- è‡ªå®šä¹‰ CSS æ ·å¼ ---
# --- è‡ªå®šä¹‰ CSS æ ·å¼ (å«å›¾ç‰‡é˜²æŠ–) ---
CUSTOM_CSS = """
<style>
    :root { --primary-color: #4F46E5; }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 12px; padding: 20px; color: white; text-align: center;
        margin: 8px 0;
    }
    /* å›¾ç‰‡å®¹å™¨é«˜åº¦å›ºå®šï¼Œé˜²æ­¢é¡µé¢æŠ–åŠ¨ */
    div[data-testid="stImage"] { min-height: 400px; display: flex; align-items: center; justify-content: center; }
    .stPlotlyChart { min-height: 400px; }
</style>
"""


st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


# ============================================================
# Session State åˆå§‹åŒ–
# ============================================================
def init_session_state():
    """åˆå§‹åŒ–æ‰€æœ‰session stateå˜é‡"""
    defaults = {
        'data': None,
        'processed_data': None,
        'molecular_features': None,
        'target_col': None,
        'feature_cols': [],
        'model': None,
        'model_name': None,
        'train_result': None,
        'scaler': None,
        'pipeline': None,
        'X_train': None,
        'X_test': None,
        'y_train': None,
        'y_test': None,
        'optimization_history': [],
        'best_params': None,
        'optimized_model_name': None  # æ–°å¢ï¼šè®°å½•ä¼˜åŒ–çš„æ¨¡å‹å
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


init_session_state()


# ============================================================
# ä¾§è¾¹æ æ¸²æŸ“
# ============================================================
def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ å¯¼èˆª"""
    with st.sidebar:
        st.title(f"ğŸ”¬ {APP_NAME}")
        st.caption(f"ç‰ˆæœ¬ {VERSION}")
        st.markdown("---")

        page = st.radio(
            "ğŸ“Œ åŠŸèƒ½å¯¼èˆª",
            [
                "ğŸ  é¦–é¡µ",
                "ğŸ“¤ æ•°æ®ä¸Šä¼ ",
                "ğŸ” æ•°æ®æ¢ç´¢",
                "ğŸ§¹ æ•°æ®æ¸…æ´—",
                "âœ¨ æ•°æ®å¢å¼º",
                "ğŸ§¬ åˆ†å­ç‰¹å¾",
                "ğŸ¯ ç‰¹å¾é€‰æ‹©",
                "ğŸ¤– æ¨¡å‹è®­ç»ƒ",
                "ğŸ“Š æ¨¡å‹è§£é‡Š",
                "ğŸ”® é¢„æµ‹åº”ç”¨",
                "âš™ï¸ è¶…å‚ä¼˜åŒ–",
            ],
            label_visibility="collapsed"
        )

        st.markdown("---")
        st.markdown("### ğŸ“Š æ•°æ®çŠ¶æ€")

        # ä¼˜å…ˆè·å– processed_data (æ¸…æ´—/å¤„ç†åçš„æ•°æ®)
        current_df = st.session_state.get('processed_data')
        original_df = st.session_state.get('data')

        # ç¡®å®šè¦æ˜¾ç¤ºå“ªä¸ªæ•°æ®çš„ä¿¡æ¯
        display_df = current_df if current_df is not None else original_df

        if display_df is not None:
            # 1. æ˜¾ç¤ºè¡Œ/åˆ—æ•°
            status_label = "âœ… å½“å‰æ•°æ® (å·²æ¸…æ´—)" if current_df is not None else "âœ… åŸå§‹æ•°æ®"
            st.success(f"{status_label}\n\n**{display_df.shape[0]} è¡Œ Ã— {display_df.shape[1]} åˆ—**")

            # 2. æ˜¾ç¤ºåˆ†å­ç‰¹å¾çŠ¶æ€
            if st.session_state.get('molecular_features') is not None:
                mf = st.session_state.molecular_features
                st.info(f"ğŸ§¬ åˆ†å­ç‰¹å¾: {mf.shape[1]} ä¸ª")

            # 3. æ˜¾ç¤ºç‰¹å¾é€‰æ‹©çŠ¶æ€
            feature_cols = st.session_state.get('feature_cols')
            target_col = st.session_state.get('target_col')

            if feature_cols:
                st.info(f"ğŸ¯ å·²é€‰ç‰¹å¾ (X): {len(feature_cols)} ä¸ª")

            if target_col:
                st.caption(f"ğŸ¯ ç›®æ ‡å˜é‡ (Y): {target_col}")

        else:
            st.warning("âš ï¸ æœªåŠ è½½æ•°æ®")

        if st.session_state.model is not None:
            st.success(f"ğŸ¤– å·²è®­ç»ƒ: {st.session_state.model_name}")
            # å¦‚æœæœ‰è®­ç»ƒç»“æœï¼Œä¹Ÿå¯ä»¥æ˜¾ç¤ºR2
            if st.session_state.get('train_result'):
                r2 = st.session_state.train_result.get('r2', 0)
                st.caption(f"å½“å‰ RÂ²: {r2:.4f}")

        st.markdown("---")
        st.markdown("### ğŸ”§ ç³»ç»Ÿä¿¡æ¯")
        st.caption(f"CPUæ ¸å¿ƒ: {mp.cpu_count()}")
        if PSUTIL_AVAILABLE:
            mem = psutil.virtual_memory()
            st.caption(f"å†…å­˜ä½¿ç”¨: {mem.percent}%")

        return page


# ============================================================
# é¡µé¢ï¼šé¦–é¡µ
# ============================================================
def page_home():
    """é¦–é¡µ"""
    st.title("ğŸ”¬ ç¢³çº¤ç»´å¤åˆææ–™æ™ºèƒ½é¢„æµ‹å¹³å°")
    st.markdown(f"**ç‰ˆæœ¬ {VERSION}** ")

    st.markdown("---")

    # åŠŸèƒ½å¡ç‰‡
    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("""
        <div class="metric-card">
            <div class="metric-label">æ•°æ®å¤„ç†</div>
            <div class="metric-value">ğŸ“Š</div>
            <p>æ™ºèƒ½æ¸…æ´— Â· VAEå¢å¼º Â· ç±»åˆ«å¹³è¡¡</p>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown("""
        <div class="metric-card metric-card-success">
            <div class="metric-label">åˆ†å­ç‰¹å¾</div>
            <div class="metric-value">ğŸ§¬</div>
            <p>RDKit Â· æŒ‡çº¹(MACCS) Â· å›¾ç‰¹å¾</p>
        </div>
        """, unsafe_allow_html=True)

    with col3:
        st.markdown("""
        <div class="metric-card metric-card-warning">
            <div class="metric-label">æ¨¡å‹è®­ç»ƒ</div>
            <div class="metric-value">ğŸ¤–</div>
            <p>15+æ¨¡å‹ Â· æ‰‹åŠ¨è°ƒå‚ Â· Optunaä¼˜åŒ–</p>
        </div>
        """, unsafe_allow_html=True)

    st.markdown("---")

    # æ ¸å¿ƒåŠŸèƒ½ä»‹ç»
    st.markdown("## ğŸš€ æ ¸å¿ƒåŠŸèƒ½")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        ### ğŸ“Š æ•°æ®å¤„ç†
        - **æ™ºèƒ½æ•°æ®æ¸…æ´—**: ç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼æ£€æµ‹ã€æ•°æ®ç±»å‹ä¿®å¤
        - **VAEæ•°æ®å¢å¼º**: åŸºäºå˜åˆ†è‡ªç¼–ç å™¨çš„è¡¨æ ¼æ•°æ®ç”Ÿæˆ
        - **ç±»åˆ«å¹³è¡¡**: è§£å†³åŒ–å­¦å•ä½“æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜

        ### ğŸ§¬ åˆ†å­ç‰¹å¾æå–
        - **åˆ†å­æŒ‡çº¹**: MACCS Keys, Morgan (ECFP) æŒ‡çº¹
        - **RDKitæ ‡å‡†ç‰ˆ**: 200+åˆ†å­æè¿°ç¬¦
        - **å›¾ç¥ç»ç½‘ç»œç‰¹å¾**: åˆ†å­æ‹“æ‰‘ç»“æ„ç‰¹å¾
        - **MLåŠ›åœºç‰¹å¾**: ANI-2x é«˜ç²¾åº¦èƒ½é‡/åŠ›
        """)

    with col2:
        st.markdown("""
        ### ğŸ¤– æ¨¡å‹è®­ç»ƒ
        - **é›†æˆæ¨¡å‹**: éšæœºæ£®æ—ã€XGBoostã€LightGBMã€CatBoost
        - **AutoML**: AutoGluon è‡ªåŠ¨å»ºæ¨¡
        - **æ‰‹åŠ¨è°ƒå‚**: å¯è§†åŒ–å‚æ•°é…ç½®ç•Œé¢

        ### ğŸ“Š æ¨¡å‹è§£é‡Š
        - **SHAPåˆ†æ**: ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
        - **å­¦ä¹ æ›²çº¿**: æ¨¡å‹æ”¶æ•›åˆ†æ
        - **é€‚ç”¨åŸŸåˆ†æ**: PCAå‡¸åŒ…è¾¹ç•Œæ£€æµ‹
        """)

    st.markdown("---")

    # å¿«é€Ÿå¼€å§‹
    st.markdown("## âš¡ å¿«é€Ÿå¼€å§‹")
    st.info("""
    1. **ä¸Šä¼ æ•°æ®** â†’ æ”¯æŒCSVã€Excelæ ¼å¼
    2. **æ•°æ®æ¸…æ´—** â†’ ä½¿ç”¨â€œç±»åˆ«å¹³è¡¡â€å¤„ç†é«˜é¢‘å•ä½“
    3. **åˆ†å­ç‰¹å¾** â†’ æå–SMILESæŒ‡çº¹æˆ–æè¿°ç¬¦
    4. **ç‰¹å¾é€‰æ‹©** â†’ é€‰æ‹©ç›®æ ‡å˜é‡å’Œè¾“å…¥ç‰¹å¾
    5. **æ¨¡å‹è®­ç»ƒ** â†’ é€‰æ‹©æ¨¡å‹å¹¶è°ƒæ•´å‚æ•°
    6. **æ¨¡å‹è§£é‡Š** â†’ SHAPåˆ†æå’Œæ€§èƒ½è¯„ä¼°
    """)


# ============================================================
# é¡µé¢ï¼šæ•°æ®ä¸Šä¼ 
# ============================================================
def page_data_upload():
    """æ•°æ®ä¸Šä¼ é¡µé¢"""
    st.title("ğŸ“¤ æ•°æ®ä¸Šä¼ ")

    tab1, tab2 = st.tabs(["ğŸ“ ä¸Šä¼ æ–‡ä»¶", "ğŸ“ ç”Ÿæˆç¤ºä¾‹æ•°æ®"])

    with tab1:
        uploaded_file = st.file_uploader(
            "é€‰æ‹©æ•°æ®æ–‡ä»¶",
            type=['csv', 'xlsx', 'xls'],
            help="æ”¯æŒCSVå’ŒExcelæ ¼å¼"
        )

        if uploaded_file is not None:
            try:
                # ä½¿ç”¨ç¼“å­˜å‡½æ•°åŠ è½½æ•°æ®
                df = load_data_file(uploaded_file)

                # å»é‡ååˆ—
                if df.columns.duplicated().any():
                    st.warning("âš ï¸ æ£€æµ‹åˆ°é‡ååˆ—ï¼Œç³»ç»Ÿå·²è‡ªåŠ¨é‡å‘½åå¤„ç†")
                    df = df.loc[:, ~df.columns.duplicated()]

                st.session_state.data = df
                st.session_state.processed_data = df.copy()

                st.success(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—")

                # æ•°æ®é¢„è§ˆ
                st.markdown("### ğŸ“‹ æ•°æ®é¢„è§ˆ")
                st.dataframe(df.head(10), use_container_width=True)

                # åˆ—ä¿¡æ¯
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("#### æ•°å€¼åˆ—")
                    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
                    for col in numeric_cols[:10]:
                        st.markdown(f"<span class='feature-badge'>{col}</span>", unsafe_allow_html=True)
                    if len(numeric_cols) > 10:
                        st.caption(f"... ç­‰å…± {len(numeric_cols)} ä¸ªæ•°å€¼åˆ—")

                with col2:
                    st.markdown("#### æ–‡æœ¬åˆ—")
                    text_cols = df.select_dtypes(include=['object']).columns.tolist()
                    for col in text_cols[:10]:
                        st.markdown(f"<span class='feature-badge'>{col}</span>", unsafe_allow_html=True)
                    if len(text_cols) > 10:
                        st.caption(f"... ç­‰å…± {len(text_cols)} ä¸ªæ–‡æœ¬åˆ—")

            except Exception as e:
                st.error(f"âŒ åŠ è½½å¤±è´¥: {str(e)}")

    with tab2:
        st.markdown("### ç”Ÿæˆç¤ºä¾‹æ•°æ®é›†")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("#### ğŸ§ª æ··åˆæ•°æ®é›†")
            st.caption("åŒ…å«å·¥è‰ºå‚æ•°å’ŒSMILESåˆ†å­ç»“æ„")
            n_samples_hybrid = st.number_input("æ ·æœ¬æ•°é‡", 100, 2000, 500, key="n_hybrid")
            if st.button("ç”Ÿæˆæ··åˆæ•°æ®é›†", type="primary"):
                df = generate_hybrid_dataset(n_samples=n_samples_hybrid)
                st.session_state.data = df
                st.session_state.processed_data = df.copy()
                st.success(f"âœ… å·²ç”Ÿæˆæ··åˆæ•°æ®é›†: {df.shape}")
                st.dataframe(df.head(), use_container_width=True)

        with col2:
            st.markdown("#### ğŸ“Š çº¯æ•°å€¼æ•°æ®é›†")
            st.caption("ä»…åŒ…å«æ•°å€¼å‹å·¥è‰ºå‚æ•°")
            n_samples_numeric = st.number_input("æ ·æœ¬æ•°é‡", 100, 2000, 500, key="n_numeric")
            if st.button("ç”Ÿæˆçº¯æ•°å€¼æ•°æ®é›†", type="primary"):
                df = generate_pure_numeric_dataset(n_samples=n_samples_numeric)
                st.session_state.data = df
                st.session_state.processed_data = df.copy()
                st.success(f"âœ… å·²ç”Ÿæˆçº¯æ•°å€¼æ•°æ®é›†: {df.shape}")
                st.dataframe(df.head(), use_container_width=True)


# ============================================================
# é¡µé¢ï¼šæ•°æ®æ¢ç´¢
# ============================================================
def page_data_explore():
    """æ•°æ®æ¢ç´¢é¡µé¢"""
    st.title("ğŸ” æ•°æ®æ¢ç´¢")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    explorer = EnhancedDataExplorer(df)

    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“Š æè¿°ç»Ÿè®¡", "ğŸ”— ç›¸å…³æ€§åˆ†æ", "ğŸ“ˆ åˆ†å¸ƒå›¾", "â“ ç¼ºå¤±å€¼", "ğŸ’¾ å¯¼å‡º"
    ])

    with tab1:
        stats = explorer.generate_summary_stats()

        col1, col2, col3, col4 = st.columns(4)
        col1.metric("æ€»è¡Œæ•°", stats['basic_info']['total_rows'])
        col2.metric("æ€»åˆ—æ•°", stats['basic_info']['total_columns'])
        col3.metric("æ•°å€¼åˆ—", stats['basic_info']['numeric_columns'])
        col4.metric("ç¼ºå¤±å€¼", stats['basic_info']['missing_values'])

        st.markdown("### æ•°å€¼ç‰¹å¾ç»Ÿè®¡")
        if explorer.numeric_cols:
            st.dataframe(df[explorer.numeric_cols].describe(), use_container_width=True)

    with tab2:
        st.markdown("### ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ")
        fig = explorer.plot_correlation_matrix()
        if fig:
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("éœ€è¦è‡³å°‘2ä¸ªæ•°å€¼åˆ—")

        # é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹
        pairs = explorer.get_high_correlation_pairs(threshold=0.8)
        if pairs:
            st.markdown("### âš ï¸ é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹ (|r| > 0.8)")
            for p in pairs[:10]:
                st.write(f"- **{p['feature1']}** â†” **{p['feature2']}**: {p['correlation']:.3f}")

    with tab3:
        st.markdown("### æ•°å€¼ç‰¹å¾åˆ†å¸ƒ")
        fig = explorer.plot_distributions()
        if fig:
            st.plotly_chart(fig, use_container_width=True)

        st.markdown("### ç®±çº¿å›¾")
        fig_box = explorer.plot_boxplots()
        if fig_box:
            st.plotly_chart(fig_box, use_container_width=True)

    with tab4:
        st.markdown("### ç¼ºå¤±å€¼åˆ†æ")
        fig_missing = explorer.plot_missing_values()
        if fig_missing:
            st.plotly_chart(fig_missing, use_container_width=True)
        else:
            st.success("âœ… æ•°æ®æ— ç¼ºå¤±å€¼")

    with tab5:
        st.markdown("### å¯¼å‡ºæ•°æ®")
        col1, col2 = st.columns(2)

        with col1:
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                "ğŸ“¥ ä¸‹è½½CSV",
                csv,
                "data_export.csv",
                "text/csv"
            )

        with col2:
            buffer = io.BytesIO()
            df.to_excel(buffer, index=False)
            st.download_button(
                "ğŸ“¥ ä¸‹è½½Excel",
                buffer.getvalue(),
                "data_export.xlsx",
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )


# ============================================================
# é¡µé¢ï¼šæ•°æ®æ¸…æ´—
# ============================================================
def page_data_cleaning():
    """æ•°æ®æ¸…æ´—é¡µé¢"""
    st.title("ğŸ§¹ æ•°æ®æ¸…æ´—")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    cleaner = AdvancedDataCleaner(df)

    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "â“ ç¼ºå¤±å€¼å¤„ç†", "ğŸ“Š å¼‚å¸¸å€¼æ£€æµ‹", "ğŸ”„ é‡å¤æ•°æ®", "ğŸ”§ æ•°æ®ç±»å‹", "âš–ï¸ ç±»åˆ«å¹³è¡¡"
    ])

    with tab1:
        st.markdown("### ç¼ºå¤±å€¼å¤„ç†")

        # ç¼ºå¤±å€¼ç»Ÿè®¡
        missing = df.isnull().sum()
        missing = missing[missing > 0]

        if len(missing) > 0:
            st.warning(f"æ£€æµ‹åˆ° {len(missing)} åˆ—å­˜åœ¨ç¼ºå¤±å€¼")

            col1, col2 = st.columns(2)
            with col1:
                st.dataframe(pd.DataFrame({
                    'åˆ—å': missing.index,
                    'ç¼ºå¤±æ•°é‡': missing.values,
                    'ç¼ºå¤±æ¯”ä¾‹': (missing.values / len(df) * 100).round(2)
                }), use_container_width=True)

            with col2:
                strategy = st.selectbox(
                    "é€‰æ‹©å¡«å……ç­–ç•¥",
                    ["median", "mean", "mode", "knn", "drop_rows", "constant"]
                )

                fill_value = None
                if strategy == "constant":
                    fill_value = st.number_input("å¡«å……å¸¸æ•°å€¼", value=0.0)

                if st.button("ğŸ”§ æ‰§è¡Œç¼ºå¤±å€¼å¤„ç†", type="primary"):
                    cleaned_df = cleaner.handle_missing_values(strategy=strategy, fill_value=fill_value)
                    st.session_state.processed_data = cleaned_df
                    st.success("âœ… ç¼ºå¤±å€¼å¤„ç†å®Œæˆ")
                    st.rerun()
        else:
            st.success("âœ… æ•°æ®æ— ç¼ºå¤±å€¼")

    with tab2:
        st.markdown("### å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†")

        col1, col2 = st.columns(2)
        with col1:
            method = st.selectbox("æ£€æµ‹æ–¹æ³•", ["iqr", "zscore"])
            threshold = st.slider("é˜ˆå€¼", 1.0, 5.0, 1.5 if method == "iqr" else 3.0)

        with col2:
            handle_method = st.selectbox("å¤„ç†æ–¹æ³•", ["clip", "replace_median", "remove"])

        if st.button("ğŸ” æ£€æµ‹å¼‚å¸¸å€¼"):
            outliers = cleaner.detect_outliers(method=method, threshold=threshold)
            if outliers:
                st.warning(f"æ£€æµ‹åˆ° {len(outliers)} åˆ—å­˜åœ¨å¼‚å¸¸å€¼")
                st.json(outliers)
            else:
                st.success("âœ… æœªæ£€æµ‹åˆ°æ˜¾è‘—å¼‚å¸¸å€¼")

        if st.button("ğŸ”§ å¤„ç†å¼‚å¸¸å€¼", type="primary"):
            cleaned_df = cleaner.handle_outliers(method=handle_method, threshold=threshold)
            st.session_state.processed_data = cleaned_df
            st.success("âœ… å¼‚å¸¸å€¼å¤„ç†å®Œæˆ")

    with tab3:
        st.markdown("### ğŸ”„ æ•°æ®å»é‡ä¸åˆ†å¸ƒä¼˜åŒ–")

        col_clean_1, col_clean_2 = st.columns(2)

        with col_clean_1:
            st.markdown("#### 1. è¡Œå»é‡")
            st.caption("åˆ é™¤å®Œå…¨é‡å¤çš„æ ·æœ¬è¡Œ")
            dup_count = df.duplicated().sum()
            st.metric("å®Œå…¨é‡å¤è¡Œæ•°", dup_count)

            if dup_count > 0:
                if st.button("ğŸ—‘ï¸ åˆ é™¤é‡å¤è¡Œ", type="primary"):
                    cleaned_df = cleaner.remove_duplicates()
                    st.session_state.processed_data = cleaned_df
                    st.success(f"âœ… å·²åˆ é™¤ {dup_count} è¡Œé‡å¤æ•°æ®")
                    st.rerun()
            else:
                st.info("âœ… æ— é‡å¤è¡Œ")

        st.markdown("---")

        with col_clean_2:
            st.markdown("#### 2. ç‰¹å¾åˆ†å¸ƒä¼˜åŒ– (é’ˆå¯¹æ•°å€¼)")
            st.caption("é™ä½æŸä¸€ç‰¹å¾ä¸­ä¼—æ•°ï¼ˆå‡ºç°æœ€å¤šçš„å€¼ï¼‰çš„æ¯”ä¾‹ï¼Œå¹³è¡¡æ•°æ®åˆ†å¸ƒ")

            # æ£€æµ‹é˜ˆå€¼è®¾ç½®
            rep_threshold = st.slider("é«˜é‡å¤ç‡æ£€æµ‹é˜ˆå€¼", 0.5, 0.99, 0.8, 0.05,
                                      help="æ£€æµ‹ä¼—æ•°å æ¯”è¶…è¿‡æ­¤æ¯”ä¾‹çš„ç‰¹å¾")

            high_rep_cols = cleaner.detect_high_repetition_columns(rep_threshold)

            if high_rep_cols:
                st.warning(f"âš ï¸ æ£€æµ‹åˆ° {len(high_rep_cols)} ä¸ªç‰¹å¾å­˜åœ¨é«˜é‡å¤å€¼")

                # æ˜¾ç¤ºè¯¦æƒ…
                rep_data = []
                for col, info in high_rep_cols.items():
                    rep_data.append({
                        "ç‰¹å¾": col,
                        "ä¼—æ•°": str(info['most_frequent_value']),
                        "å½“å‰å æ¯”": f"{info['frequency'] * 100:.1f}%"
                    })
                st.dataframe(pd.DataFrame(rep_data), use_container_width=True, hide_index=True)

                # æ“ä½œåŒº
                st.markdown("##### ğŸ”§ æ‰§è¡Œä¼˜åŒ–")
                target_col = st.selectbox("é€‰æ‹©è¦ä¼˜åŒ–çš„ç‰¹å¾", list(high_rep_cols.keys()))

                # æ™ºèƒ½è®¡ç®—æ»‘å—èŒƒå›´ï¼šä¸èƒ½æ¯”å½“å‰å æ¯”è¿˜é«˜ï¼Œä¹Ÿä¸èƒ½å¤ªä½ï¼ˆå¦‚0%ï¼‰
                current_freq = high_rep_cols[target_col]['frequency']
                target_rate = st.slider(
                    f"ç›®æ ‡å æ¯” (é’ˆå¯¹ {target_col})",
                    0.1, float(current_freq), 0.5, 0.05,
                    help="é€šè¿‡éšæœºåˆ é™¤åŒ…å«ä¼—æ•°çš„æ ·æœ¬ï¼Œä½¿å…¶å æ¯”é™ä½åˆ°æ­¤å€¼"
                )

                if st.button(f"ğŸ“‰ é™ä½ '{target_col}' çš„é‡å¤ç‡", type="primary"):
                    original_len = len(df)
                    cleaned_df = cleaner.reduce_feature_repetition(target_col, target_rate)
                    new_len = len(cleaned_df)
                    st.session_state.processed_data = cleaned_df

                    st.success(f"âœ… ä¼˜åŒ–å®Œæˆï¼åˆ é™¤äº† {original_len - new_len} ä¸ªæ ·æœ¬")
                    st.info(f"ğŸ“Š å½“å‰è¡Œæ•°: {new_len}ï¼Œ'{target_col}' çš„ä¼—æ•°å æ¯”å·²è°ƒæ•´è‡³ {target_rate * 100:.1f}%")
                    st.rerun()
            else:
                st.success("âœ… æœªæ£€æµ‹åˆ°é«˜é‡å¤ç‡ç‰¹å¾")

    with tab4:
        st.markdown("### æ•°æ®ç±»å‹è¯Šæ–­")

        pseudo_numeric = cleaner.detect_pseudo_numeric_columns()

        if pseudo_numeric:
            st.warning(f"æ£€æµ‹åˆ° {len(pseudo_numeric)} ä¸ªä¼ªæ•°å€¼åˆ—")
            st.json(pseudo_numeric)

            if st.button("ğŸ”§ ä¿®å¤ä¼ªæ•°å€¼åˆ—", type="primary"):
                cleaned_df = cleaner.fix_pseudo_numeric_columns()
                st.session_state.processed_data = cleaned_df
                st.success("âœ… æ•°æ®ç±»å‹ä¿®å¤å®Œæˆ")
        else:
            st.success("âœ… æ•°æ®ç±»å‹æ­£å¸¸")

    with tab5:
        st.markdown("### âš–ï¸ ç±»åˆ«å¹³è¡¡ (é’ˆå¯¹åŒ–å­¦ç»“æ„)")
        st.info(
            "ğŸ’¡ è§£å†³ç‰¹å®šå•ä½“/åˆ†å­é‡å¤æ¬¡æ•°è¿‡å¤šçš„é—®é¢˜ã€‚é€šè¿‡é™åˆ¶æ¯ä¸ªç±»åˆ«çš„æœ€å¤§æ ·æœ¬æ•°ï¼Œå¼ºåˆ¶æ•°æ®åˆ†å¸ƒæ›´å‡åŒ€ï¼Œé¿å…æ¨¡å‹åå‘å¸¸è§åˆ†å­ã€‚")

        # 1. é€‰æ‹©åˆ†ç±»åˆ—
        # é»˜è®¤å°è¯•æ‰¾ 'smiles' ç›¸å…³åˆ—
        text_cols = df.select_dtypes(include=['object']).columns.tolist()
        if text_cols:
            cat_col = st.selectbox("é€‰æ‹©è¦å¹³è¡¡çš„ç±»åˆ«åˆ— (é€šå¸¸æ˜¯SMILES)", text_cols)

            # 2. åˆ†æå½“å‰åˆ†å¸ƒ
            counts = df[cat_col].value_counts()
            n_unique = len(counts)

            col1, col2, col3 = st.columns(3)
            col1.metric("å”¯ä¸€ç±»åˆ«æ•°", n_unique)
            col2.metric("æœ€å¤§æ ·æœ¬æ•°", counts.max())
            col3.metric("ä¸­ä½æ•°æ ·æœ¬æ•°", int(counts.median()))

            st.markdown("#### Top 10 å‡ºç°æœ€é¢‘ç¹çš„åˆ†å­")
            st.bar_chart(counts.head(10))

            # 3. è®¾ç½®å¹³è¡¡å‚æ•°
            st.markdown("#### ğŸ”§ å¹³è¡¡è®¾ç½®")

            limit_val = st.slider(
                "æ¯ä¸ªç±»åˆ«çš„æœ€å¤§æ ·æœ¬æ•° (Max Samples per Category)",
                min_value=1,
                max_value=int(counts.max()),
                value=int(counts.median()) if n_unique > 0 else 10,
                help="å¦‚æœæŸåˆ†å­çš„å‡ºç°æ¬¡æ•°è¶…è¿‡æ­¤å€¼ï¼Œå¤šä½™çš„æ ·æœ¬å°†è¢«éšæœºä¸¢å¼ƒã€‚"
            )

            if st.button(f"âš–ï¸ æ‰§è¡Œå¹³è¡¡ (é™åˆ¶ä¸º {limit_val} ä¸ª)", type="primary"):
                old_len = len(df)
                cleaned_df = cleaner.balance_category_counts(cat_col, max_samples=limit_val)
                new_len = len(cleaned_df)

                st.session_state.processed_data = cleaned_df

                st.success(f"âœ… å¹³è¡¡å®Œæˆï¼")
                st.info(f"ğŸ“Š æ€»æ ·æœ¬æ•°ä» {old_len} å‡å°‘åˆ° {new_len} (åˆ é™¤äº† {old_len - new_len} ä¸ªè¿‡åº¦é‡å¤æ ·æœ¬)")
                st.rerun()
        else:
            st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬åˆ—ï¼Œæ— æ³•æ‰§è¡Œç±»åˆ«å¹³è¡¡")


# ============================================================
# é¡µé¢ï¼šæ•°æ®å¢å¼º
# ============================================================
def page_data_enhancement():
    """æ•°æ®å¢å¼ºé¡µé¢"""
    st.title("âœ¨ æ•°æ®å¢å¼º")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    enhancer = DataEnhancer(df)

    tab1, tab2 = st.tabs(["ğŸ”® KNNæ™ºèƒ½å¡«å……", "ğŸ§¬ VAEç”Ÿæˆå¼å¢å¼º"])

    with tab1:
        st.markdown("### KNNæ™ºèƒ½å¡«å……")
        st.info("ä½¿ç”¨Kè¿‘é‚»ç®—æ³•é¢„æµ‹å¹¶å¡«å……ç¼ºå¤±å€¼ï¼Œæ¯”ç®€å•çš„å‡å€¼/ä¸­ä½æ•°å¡«å……æ›´å‡†ç¡®")

        n_neighbors = st.slider("Kå€¼ï¼ˆè¿‘é‚»æ•°é‡ï¼‰", 1, 20, 5)

        if st.button("ğŸ”§ æ‰§è¡ŒKNNå¡«å……", type="primary"):
            with st.spinner("æ­£åœ¨æ‰§è¡ŒKNNå¡«å……..."):
                filled_df = enhancer.knn_impute(n_neighbors=n_neighbors)
                st.session_state.processed_data = filled_df
                st.success("âœ… KNNå¡«å……å®Œæˆ")

                # å¯¹æ¯”
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("åŸå§‹ç¼ºå¤±å€¼", df.isnull().sum().sum())
                with col2:
                    st.metric("å¤„ç†åç¼ºå¤±å€¼", filled_df.isnull().sum().sum())

    with tab2:
        st.markdown("### VAEç”Ÿæˆå¼æ•°æ®å¢å¼º")
        st.info("ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨(VAE)å­¦ä¹ æ•°æ®åˆ†å¸ƒï¼Œç”Ÿæˆé«˜ä¿çœŸè™šæ‹Ÿæ•°æ®ç‚¹")

        col1, col2 = st.columns(2)
        with col1:
            n_samples = st.number_input("ç”Ÿæˆæ ·æœ¬æ•°é‡", 10, 1000, 100)
            latent_dim = st.slider("æ½œåœ¨ç©ºé—´ç»´åº¦", 4, 64, 16)
            h_dim = st.slider("éšè—å±‚ç»´åº¦", 32, 256, 128)

        with col2:
            epochs = st.slider("è®­ç»ƒè½®æ•°", 10, 500, 100)
            batch_size = st.selectbox("æ‰¹å¤§å°", [16, 32, 64, 128], index=1)
            learning_rate = st.number_input("å­¦ä¹ ç‡", 0.0001, 0.1, 0.001, format="%.4f")

        if st.button("ğŸš€ ç”Ÿæˆå¢å¼ºæ•°æ®", type="primary"):
            try:
                with st.spinner("æ­£åœ¨è®­ç»ƒVAEæ¨¡å‹..."):
                    progress_bar = st.progress(0)

                    generated_df, fig = enhancer.generate_with_vae(
                        n_samples=n_samples,
                        latent_dim=latent_dim,
                        h_dim=h_dim,
                        epochs=epochs,
                        batch_size=batch_size,
                        lr=learning_rate
                    )

                    progress_bar.progress(100)

                st.success(f"âœ… æˆåŠŸç”Ÿæˆ {len(generated_df)} ä¸ªæ ·æœ¬")

                # PCAå¯è§†åŒ–
                st.markdown("### ğŸ“Š PCAå¯è§†åŒ–å¯¹æ¯”")
                st.plotly_chart(fig, use_container_width=True)

                # åˆå¹¶é€‰é¡¹
                if st.checkbox("å°†ç”Ÿæˆæ•°æ®åˆå¹¶åˆ°åŸå§‹æ•°æ®"):
                    merged_df = pd.concat([df, generated_df], ignore_index=True)
                    st.session_state.processed_data = merged_df
                    st.success(f"âœ… åˆå¹¶åæ•°æ®: {merged_df.shape}")

            except Exception as e:
                st.error(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")


# ============================================================
# é¡µé¢ï¼šåˆ†å­ç‰¹å¾æå–ï¼ˆå®Œæ•´5ç§æ–¹æ³•ï¼‰
# ============================================================
def page_molecular_features():
    """åˆ†å­ç‰¹å¾æå–é¡µé¢ - å®Œæ•´è¿˜åŸ5ç§æ–¹æ³• + åˆ†å­æŒ‡çº¹ (é€‚é…åŒç»„åˆ†)"""
    st.title("ğŸ§¬ åˆ†å­ç‰¹å¾æå–")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    # ä¼˜å…ˆä½¿ç”¨å¤„ç†åçš„æ•°æ®
    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data

    # æ£€æµ‹SMILESåˆ—
    text_cols = df.select_dtypes(include=['object']).columns.tolist()
    smiles_candidates = [col for col in text_cols if 'smiles' in col.lower() or 'smi' in col.lower()]

    if not text_cols:
        st.warning("âš ï¸ æ•°æ®ä¸­æœªæ£€æµ‹åˆ°æ–‡æœ¬åˆ—ï¼Œæ— æ³•æå–åˆ†å­ç‰¹å¾")
        return

    st.markdown("### ğŸ”¬ SMILESåˆ—é€‰æ‹©")

    # -----------------------------
    # å¤šç»„åˆ†/æ··åˆç‰© SMILES å¤„ç†
    # è¯´æ˜ï¼š
    # 1) å•åˆ—é‡Œå¯èƒ½ç”¨ ";" ç­‰åˆ†éš”ç¬¦è¡¨ç¤ºå¤šä¸ªç»„åˆ†ï¼ˆRDKit ä¸èƒ½ç›´æ¥è§£æ ";"ï¼Œä½†èƒ½è§£æ "."ï¼‰
    # 2) ä¹Ÿå¯èƒ½æ¯ä¸ªç»„åˆ†å•ç‹¬å ä¸€åˆ—ï¼ˆå¦‚ resin_smiles_1, resin_smiles_2 ...ï¼‰
    # è¿™é‡ŒæŠŠå¤šä¸ªç»„åˆ†ç»Ÿä¸€è½¬æ¢ä¸ºâ€œå¤šç‰‡æ®µ SMILESâ€ï¼ˆç”¨ "." è¿æ¥ï¼‰ï¼Œå†äº¤ç»™ RDKit/æŒ‡çº¹æå–å™¨ã€‚
    # -----------------------------
    import re

    def _split_smiles_cell(x):
        """æŠŠå•å…ƒæ ¼é‡Œçš„ SMILES æ‹†æˆç»„åˆ†åˆ—è¡¨ã€‚

        ä»…æŠŠå¸¸è§â€œåˆ—è¡¨åˆ†éš”ç¬¦â€å½“ä½œç»„åˆ†è¾¹ç•Œï¼š;ã€ï¼›ã€|ã€ä»¥åŠå¸¦ç©ºæ ¼çš„ +
        æ³¨æ„ï¼šä¸æŠŠâ€œ/â€å½“åˆ†éš”ç¬¦ï¼ˆå®ƒæ˜¯ SMILES ç«‹ä½“åŒ–å­¦çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚
        """
        if x is None or (isinstance(x, float) and np.isnan(x)):
            return []
        s = str(x).strip()
        if not s or s.lower() == 'nan':
            return []
        # ç»Ÿä¸€ä¸­æ–‡åˆ†å·
        s = s.replace('ï¼›', ';')

        # å…ˆæŒ‰ ; æˆ– | åˆ†å‰²
        parts = re.split(r"\s*[;|]\s*", s)

        # å†æŒ‰â€œå¸¦ç©ºæ ¼çš„ +â€åˆ†å‰²ï¼ˆé¿å…è¯¯ä¼¤ [N+] è¿™ç±»å¸¦ç”µè·å†™æ³•ï¼‰
        final = []
        for p in parts:
            final.extend(re.split(r"\s+\+\s+", p))

        # æ¸…ç†ç©ºä¸²
        final = [p.strip() for p in final if p and p.strip()]
        return final

    def _combine_components(df_in: pd.DataFrame, cols: list[str]):
        """æŠŠå¤šåˆ—/å•åˆ— SMILES åˆå¹¶æˆå¤šç‰‡æ®µ SMILESï¼Œå¹¶è¿”å›(åˆå¹¶åçš„Series, ç»„åˆ†æ•°é‡Series)"""
        if not cols:
            return pd.Series([np.nan] * len(df_in)), pd.Series([0] * len(df_in))

        combined = []
        counts = []
        for _, row in df_in[cols].iterrows():
            comps = []
            for c in cols:
                comps.extend(_split_smiles_cell(row[c]))
            counts.append(len(comps))
            combined.append('.'.join(comps) if comps else np.nan)
        return pd.Series(combined), pd.Series(counts)

    col1, col2 = st.columns(2)
    with col1:
        default_idx = 0
        if smiles_candidates:
            default_idx = text_cols.index(smiles_candidates[0])

        # è¿™é‡Œæ˜ç¡®è¿™æ˜¯ç¬¬ä¸€ç»„åˆ†ï¼ˆé€šå¸¸æ˜¯æ ‘è„‚ï¼‰
        smiles_col = st.selectbox(
            "é€‰æ‹©åŒ…å«SMILESçš„åˆ— (æ ‘è„‚/ä¸»ä½“)",
            text_cols,
            index=default_idx
        )

    with col2:
        st.markdown("**ç¤ºä¾‹SMILES:**")
        samples = df[smiles_col].dropna().head(3).tolist()
        for s in samples:
            st.code(s[:50] + "..." if len(str(s)) > 50 else s)

    # --- å¤šç»„åˆ†è®¾ç½®ï¼ˆæ ‘è„‚ä¾§ï¼‰ ---
    st.markdown("#### ğŸ§© å¤šç»„åˆ†/æ··åˆç‰©è®¾ç½® (å¯é€‰)")
    resin_mix_mode = st.checkbox(
        "æ ‘è„‚ä¸ºå¤šç»„åˆ†ï¼ˆæˆ–å•å…ƒæ ¼å†…åŒ…å«å¤šä¸ªSMILESï¼‰",
        value=False,
        help="å¦‚æœä½ çš„æ ‘è„‚åˆ—é‡Œå‡ºç° 'A;B' è¿™ç§å†™æ³•ï¼Œæˆ–æœ‰ resin_smiles_1/resin_smiles_2 è¿™ç§å¤šåˆ—ç»„åˆ†ï¼Œè¯·å¼€å¯ã€‚"
    )

    resin_component_cols = [smiles_col]
    resin_mix_layout = "å•åˆ—"  # ä»…ç”¨äº UI è®°å½•
    add_component_count_features = False

    if resin_mix_mode:
        resin_mix_layout = st.radio(
            "æ ‘è„‚ç»„åˆ†åœ¨è¡¨æ ¼ä¸­çš„ç»„ç»‡æ–¹å¼",
            ["å•åˆ—ï¼ˆåŒä¸€å•å…ƒæ ¼ç”¨åˆ†éš”ç¬¦è¡¨ç¤ºå¤šä¸ªç»„åˆ†ï¼Œå¦‚ A;Bï¼‰", "å¤šåˆ—ï¼ˆæ¯åˆ—ä¸€ä¸ªç»„åˆ†ï¼Œå¦‚ resin_smiles_1/resin_smiles_2â€¦ï¼‰"],
            index=0
        )

        if resin_mix_layout.startswith("å¤šåˆ—"):
            # è‡ªåŠ¨æ¨èï¼šä¸æ‰€é€‰åˆ—åŒå‰ç¼€ã€ä¸”ä»¥ _æ•°å­— ç»“å°¾çš„åˆ—
            pattern = re.compile(rf"^{re.escape(smiles_col)}_\d+$")
            auto_cols = [c for c in text_cols if pattern.match(c)]
            # æŒ‰æœ«å°¾æ•°å­—æ’åº
            def _tail_num(colname: str):
                try:
                    return int(colname.split('_')[-1])
                except:
                    return 0
            auto_cols = sorted(auto_cols, key=_tail_num)
            resin_component_cols = st.multiselect(
                "é€‰æ‹©æ ‘è„‚ç»„åˆ†åˆ—",
                options=text_cols,
                default=auto_cols if auto_cols else [smiles_col],
                help="ç³»ç»Ÿä¼šæŠŠè¿™äº›åˆ—çš„æ‰€æœ‰éç©ºç»„åˆ†åˆå¹¶ä¸ºä¸€ä¸ªå¤šç‰‡æ®µSMILESï¼ˆç”¨ '.' è¿æ¥ï¼‰"
            )
        else:
            st.caption("å°†è‡ªåŠ¨æŠŠ ';'ã€'ï¼›'ã€'|'ã€ä»¥åŠå¸¦ç©ºæ ¼çš„ ' + ' è½¬æ¢ä¸ºå¤šç»„åˆ†åˆ†éš”ï¼Œå¹¶ç”¨ '.' è¿æ¥ã€‚")

        add_component_count_features = st.checkbox(
            "é¢å¤–åŠ å…¥ç»„åˆ†æ•°é‡ç‰¹å¾ï¼ˆresin_n_components / hardener_n_componentsï¼‰",
            value=True,
            help="å¯¹å¾ˆå¤šæ··é…ä½“ç³»ï¼Œç»„åˆ†æ•°é‡æœ¬èº«ä¹Ÿä¼šå½±å“æ€§èƒ½ï¼›æ­¤é€‰é¡¹ä¼šæŠŠç»„åˆ†æ•°ä½œä¸ºé¢å¤–æ•°å€¼ç‰¹å¾å¹¶å…¥æ•°æ®é›†ã€‚"
        )

    st.markdown("---")

    # ğŸ”¥ æ ¸å¿ƒåŠŸèƒ½ï¼š5ç§æå–æ–¹æ³•é€‰æ‹©
    st.markdown("### ğŸ› ï¸ æå–æ–¹æ³•é€‰æ‹©")

    extraction_method = st.radio(
        "é€‰æ‹©åˆ†å­ç‰¹å¾æå–æ–¹æ³•",
        [
            "ğŸ‘† åˆ†å­æŒ‡çº¹ (MACCS/Morgan) [æ–°]",
            "ğŸ”¹ RDKit æ ‡å‡†ç‰ˆ (æ¨èæ–°æ‰‹)",
            "ğŸš€ RDKit å¹¶è¡Œç‰ˆ (å¤§æ•°æ®é›†)",
            "ğŸ’¾ RDKit å†…å­˜ä¼˜åŒ–ç‰ˆ (ä½å†…å­˜)",
            "ğŸ”¬ Mordred æè¿°ç¬¦ (1600+ç‰¹å¾)",
            "ğŸ•¸ï¸ å›¾ç¥ç»ç½‘ç»œç‰¹å¾ (æ‹“æ‰‘ç»“æ„)",
            "âš›ï¸ MLåŠ›åœºç‰¹å¾ (ANIèƒ½é‡/åŠ›)",
            "âš—ï¸ ç¯æ°§æ ‘è„‚ååº”ç‰¹å¾ (åŸºäºé¢†åŸŸçŸ¥è¯†)"
        ],
        help="ä¸åŒæ–¹æ³•é€‚ç”¨äºä¸åŒåœºæ™¯"
    )

    # UI å˜é‡åˆå§‹åŒ–
    fp_type = "MACCS"
    fp_bits = 2048
    fp_radius = 2
    hardener_col = None  # åˆå§‹åŒ–å›ºåŒ–å‰‚åˆ—å˜é‡
    phr_col = None

    # ============== [ä¿®æ”¹] æŒ‡çº¹å‚æ•°è®¾ç½® ==============
    if "åˆ†å­æŒ‡çº¹" in extraction_method:
        st.info("ğŸ’¡ æç¤ºï¼šå¯¹äºç¯æ°§æ ‘è„‚ä½“ç³»ï¼Œå»ºè®®åŒæ—¶é€‰æ‹©æ ‘è„‚å’Œå›ºåŒ–å‰‚åˆ—ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æ‹¼æ¥ä¸¤è€…çš„æŒ‡çº¹ä»¥æè¿°å®Œæ•´ç½‘ç»œç»“æ„ã€‚")

        col_fp1, col_fp2, col_fp3 = st.columns(3)
        with col_fp1:
            fp_type = st.selectbox("æŒ‡çº¹ç±»å‹", ["MACCS", "Morgan"])

        if fp_type == "Morgan":
            with col_fp2:
                fp_radius = st.selectbox("åŠå¾„ (Radius)", [2, 3, 4], index=0)
            with col_fp3:
                fp_bits = st.selectbox("ä½é•¿ (Bits)", [1024, 2048, 4096], index=1)

        # [æ–°å¢] åŒç»„åˆ†é€‰æ‹© UI
        st.markdown("#### åŒç»„åˆ†è®¾ç½® (æ¨è)")
        col_h1, col_h2 = st.columns(2)
        with col_h1:
            # æ’é™¤å·²é€‰çš„æ ‘è„‚åˆ—ï¼Œé¿å…é‡å¤é€‰æ‹©
            candidate_cols = ["æ—  (ä»…æå–å•åˆ—)"] + [c for c in text_cols if c != smiles_col]
            hardener_col_opt = st.selectbox("é€‰æ‹©ã€å›ºåŒ–å‰‚ã€‘SMILESåˆ—", candidate_cols)

            if hardener_col_opt != "æ—  (ä»…æå–å•åˆ—)":
                hardener_col = hardener_col_opt

    # ============== [UI] ç¯æ°§æ ‘è„‚ç‰¹å¾å‚æ•° ==============
    if "ç¯æ°§æ ‘è„‚ååº”ç‰¹å¾" in extraction_method:
        st.info("ğŸ’¡ è¯¥æ–¹æ³•éœ€è¦åŒæ—¶æä¾›ã€æ ‘è„‚ã€‘å’Œã€å›ºåŒ–å‰‚ã€‘çš„SMILESç»“æ„ã€‚")
        col_h, col_p = st.columns(2)
        with col_h:
            candidate_cols = [c for c in text_cols if c != smiles_col]
            hardener_col = st.selectbox("é€‰æ‹©ã€å›ºåŒ–å‰‚ã€‘SMILESåˆ—", candidate_cols)
        with col_p:
            num_cols = df.select_dtypes(include=np.number).columns.tolist()
            phr_col = st.selectbox("é€‰æ‹©ã€é…æ¯”/PHRã€‘åˆ— (å¯é€‰)", ["æ—  (å‡è®¾ç†æƒ³é…æ¯”)"] + num_cols)

    # å¹¶è¡Œç‰ˆå‚æ•°
    if "å¹¶è¡Œç‰ˆ" in extraction_method and OPTIMIZED_EXTRACTOR_AVAILABLE:
        col1, col2 = st.columns(2)
        with col1:
            n_jobs = st.slider("å¹¶è¡Œè¿›ç¨‹æ•°", 1, mp.cpu_count(), mp.cpu_count() // 2)
        with col2:
            batch_size = st.number_input("æ‰¹å¤„ç†å¤§å°", 100, 5000, 1000)

    st.markdown("---")

    # æ‰§è¡Œæå–
    if st.button("ğŸš€ å¼€å§‹æå–åˆ†å­ç‰¹å¾", type="primary"):
        # -----------------------------
        # 1) ç”Ÿæˆâ€œå¯è¢« RDKit è§£æâ€çš„ SMILES åˆ—è¡¨
        #    - å•åˆ—å¤šç»„åˆ†ï¼šå°† ';' ç­‰åˆ†éš”ç¬¦è½¬æ¢ä¸º '.'
        #    - å¤šåˆ—å¤šç»„åˆ†ï¼šåˆå¹¶å¤šåˆ—ä¸º '.' è¿æ¥çš„å¤šç‰‡æ®µ SMILES
        # -----------------------------
        resin_smiles_series, resin_ncomp = _combine_components(df, resin_component_cols)
        smiles_list = resin_smiles_series.tolist()

        # 2) å›ºåŒ–å‰‚ï¼ˆå¯é€‰ï¼‰â€”â€”åŒæ ·æ”¯æŒå¤šç»„åˆ†
        hardener_list = None
        hardener_ncomp = None
        if hardener_col:
            # å¦‚æœç”¨æˆ·é€‰æ‹©äº† curing_agent_smiles è¿™ç±»åˆ—ï¼ŒåŒæ—¶å­˜åœ¨ curing_agent_smiles_1/2/â€¦ï¼Œåˆ™ç»™å‡ºå¤šåˆ—æ¨¡å¼
            hardener_component_cols = [hardener_col]
            if resin_mix_mode:
                # ä»…åœ¨å¯ç”¨å¤šç»„åˆ†æ¨¡å¼æ—¶æ‰å±•ç¤º/ä½¿ç”¨å›ºåŒ–å‰‚å¤šåˆ—åˆå¹¶é€»è¾‘ï¼Œé¿å… UI è¿‡å¤æ‚
                st.caption("ï¼ˆæç¤ºï¼‰å›ºåŒ–å‰‚ä¹Ÿæ”¯æŒå¤šç»„åˆ†ï¼šå¦‚æœæœ‰ hardener_col_1/2/â€¦ å¯åœ¨ä¸‹æ–¹è‡ªåŠ¨åˆå¹¶ã€‚")

            # è‡ªåŠ¨åˆå¹¶ï¼šå¦‚æœå­˜åœ¨ hardener_col_\d åˆ—ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨å®ƒä»¬ï¼ˆç”¨æˆ·æœªæ˜¾å¼å¤šé€‰æ—¶ï¼‰
            pattern_h = re.compile(rf"^{re.escape(hardener_col)}_\d+$")
            auto_h_cols = [c for c in text_cols if pattern_h.match(c)]
            if auto_h_cols:
                def _tail_num_h(colname: str):
                    try:
                        return int(colname.split('_')[-1])
                    except:
                        return 0
                auto_h_cols = sorted(auto_h_cols, key=_tail_num_h)
                hardener_component_cols = auto_h_cols

            hardener_smiles_series, hardener_ncomp = _combine_components(df, hardener_component_cols)
            hardener_list = hardener_smiles_series.tolist()

        try:
            progress_bar = st.progress(0)
            status_text = st.empty()

            # --- [é€»è¾‘ä¿®æ”¹] åˆ†å‘æå–ä»»åŠ¡ ---
            if "åˆ†å­æŒ‡çº¹" in extraction_method:
                from core.molecular_features import FingerprintExtractor

                # æç¤ºç”¨æˆ·å½“å‰æ¨¡å¼
                mode_str = "åŒç»„åˆ†æ‹¼æ¥" if hardener_list else "å•ç»„åˆ†"
                status_text.text(f"æ­£åœ¨æå– {fp_type} æŒ‡çº¹ ({mode_str}æ¨¡å¼)...")

                extractor = FingerprintExtractor()
                # ä¼ å…¥ smiles_list_2 (å›ºåŒ–å‰‚)
                features_df, valid_indices = extractor.smiles_to_fingerprints(
                    smiles_list,
                    smiles_list_2=hardener_list,
                    fp_type=fp_type, n_bits=fp_bits, radius=fp_radius
                )

            elif "æ ‡å‡†ç‰ˆ" in extraction_method:
                status_text.text("æ­£åœ¨ä½¿ç”¨RDKitæ ‡å‡†ç‰ˆæå–...")
                extractor = AdvancedMolecularFeatureExtractor()
                features_df, valid_indices = extractor.smiles_to_rdkit_features(smiles_list)

            elif "å¹¶è¡Œç‰ˆ" in extraction_method:
                if OPTIMIZED_EXTRACTOR_AVAILABLE:
                    status_text.text(f"æ­£åœ¨ä½¿ç”¨RDKitå¹¶è¡Œç‰ˆæå– ({n_jobs}è¿›ç¨‹)...")
                    extractor = OptimizedRDKitFeatureExtractor(n_jobs=n_jobs, batch_size=batch_size)
                    features_df, valid_indices = extractor.smiles_to_rdkit_features(smiles_list)
                else:
                    st.warning("å¹¶è¡Œç‰ˆä¸å¯ç”¨ï¼Œå›é€€åˆ°æ ‡å‡†ç‰ˆ")
                    extractor = AdvancedMolecularFeatureExtractor()
                    features_df, valid_indices = extractor.smiles_to_rdkit_features(smiles_list)

            elif "å†…å­˜ä¼˜åŒ–ç‰ˆ" in extraction_method:
                status_text.text("æ­£åœ¨ä½¿ç”¨RDKitå†…å­˜ä¼˜åŒ–ç‰ˆ...")
                extractor = MemoryEfficientRDKitExtractor()
                features_df, valid_indices = extractor.smiles_to_rdkit_features(smiles_list)

            elif "Mordred" in extraction_method:
                status_text.text("æ­£åœ¨ä½¿ç”¨Mordredæå–...")
                extractor = AdvancedMolecularFeatureExtractor()
                features_df, valid_indices = extractor.smiles_to_mordred(smiles_list)

            elif "å›¾ç¥ç»ç½‘ç»œ" in extraction_method:
                status_text.text("æ­£åœ¨æå–å›¾ç»“æ„ç‰¹å¾...")
                extractor = AdvancedMolecularFeatureExtractor()
                features_df, valid_indices = extractor.smiles_to_graph_features(smiles_list)

            elif "MLåŠ›åœº" in extraction_method:
                from core.molecular_features import MLForceFieldExtractor
                status_text.text("æ­£åœ¨è®¡ç®—ANIåŠ›åœºç‰¹å¾...")
                extractor = MLForceFieldExtractor()
                if not extractor.AVAILABLE:
                    st.error("TorchANI æœªå®‰è£…")
                    return
                features_df, valid_indices = extractor.smiles_to_ani_features(smiles_list)

            elif "ç¯æ°§æ ‘è„‚" in extraction_method:
                from core.molecular_features import EpoxyDomainFeatureExtractor
                status_text.text("æ­£åœ¨è®¡ç®—ç¯æ°§æ ‘è„‚é¢†åŸŸç‰¹å¾...")
                if hardener_col is None:
                    st.error("è¯·é€‰æ‹©å›ºåŒ–å‰‚åˆ—ï¼")
                    return

                phr_list = None
                if phr_col and phr_col != "æ—  (å‡è®¾ç†æƒ³é…æ¯”)":
                    phr_list = df[phr_col].tolist()

                extractor = EpoxyDomainFeatureExtractor()
                features_df, valid_indices = extractor.extract_features(smiles_list, hardener_list, phr_list)

            progress_bar.progress(100)

            # --- åˆå¹¶ç»“æœé€»è¾‘ ---
            if len(features_df) > 0:
                st.session_state.molecular_features = features_df
                prefix = f"{smiles_col}_"
                features_df = features_df.add_prefix(prefix)

                df_valid = df.iloc[valid_indices].reset_index(drop=True)
                features_df = features_df.reset_index(drop=True)

                # é˜²æ­¢åˆ—åå†²çªï¼šå¦‚æœæ–°ç‰¹å¾åå·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤æ—§çš„
                cols_to_drop = [col for col in features_df.columns if col in df_valid.columns]
                if cols_to_drop:
                    df_valid = df_valid.drop(columns=cols_to_drop)

                merged_df = pd.concat([df_valid, features_df], axis=1)

                # å¯é€‰ï¼šè¿½åŠ ç»„åˆ†æ•°é‡ç‰¹å¾
                if resin_mix_mode and add_component_count_features:
                    merged_df[f"{smiles_col}_resin_n_components"] = resin_ncomp.iloc[valid_indices].reset_index(drop=True)
                    if hardener_ncomp is not None:
                        merged_df[f"{smiles_col}_hardener_n_components"] = hardener_ncomp.iloc[valid_indices].reset_index(drop=True)

                merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]
                st.session_state.processed_data = merged_df

                st.success(f"âœ… æˆåŠŸæå– {len(features_df)} ä¸ªæ ·æœ¬çš„ {features_df.shape[1]} ä¸ªåˆ†å­ç‰¹å¾")

                # ç»“æœç»Ÿè®¡
                col1, col2, col3 = st.columns(3)
                col1.metric("æœ‰æ•ˆæ ·æœ¬", len(valid_indices))
                col2.metric("ç‰¹å¾æ•°é‡", features_df.shape[1])
                col3.metric("åŒç»„åˆ†æ¨¡å¼", "æ˜¯" if hardener_list else "å¦")

                st.markdown("### ğŸ“‹ ç‰¹å¾é¢„è§ˆ")
                st.dataframe(features_df.head(), use_container_width=True)
            else:
                st.error("âŒ æœªèƒ½æå–ä»»ä½•ç‰¹å¾ï¼Œè¯·æ£€æŸ¥SMILESæ ¼å¼")

        except Exception as e:
            st.error(f"âŒ æå–å¤±è´¥: {str(e)}")
            st.code(traceback.format_exc())


# ============================================================
# é¡µé¢ï¼šç‰¹å¾é€‰æ‹©ï¼ˆå®Œæ•´ç‰ˆï¼‰
# ============================================================
def page_feature_selection():
    """ç‰¹å¾é€‰æ‹©é¡µé¢ - è°ƒç”¨å®Œæ•´çš„show_robust_feature_selection"""
    st.title("ğŸ¯ ç‰¹å¾é€‰æ‹©")

    # è°ƒç”¨å®Œæ•´çš„ç‰¹å¾é€‰æ‹©UI
    show_robust_feature_selection()



# ============================================================
# é¡µé¢ï¼šæ¨¡å‹è®­ç»ƒï¼ˆæ›´æ–°ç‰ˆï¼šå«è¡¨æ ¼ã€ä¸€é”®è¾“å‡ºã€å›¾ç‰‡é˜²æŠ–ï¼‰
# ============================================================
def page_model_training():
    """æ¨¡å‹è®­ç»ƒé¡µé¢"""
    st.title("ğŸ¤– æ¨¡å‹è®­ç»ƒ")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    if not st.session_state.feature_cols:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ç‰¹å¾é€‰æ‹©é¡µé¢é€‰æ‹©ç‰¹å¾")
        return

    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    X = df[st.session_state.feature_cols]
    y = df[st.session_state.target_col]
    trainer = EnhancedModelTrainer()

    col1, col2 = st.columns([1, 2])
    with col1:
        st.markdown("### ğŸ“¦ æ¨¡å‹é€‰æ‹©")
        model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", trainer.get_available_models())
        st.markdown("### âš™ï¸ è®­ç»ƒè®¾ç½®")
        test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.4, 0.2)
        random_state = st.number_input("éšæœºç§å­", 0, 1000, 42)

    with col2:
        st.markdown("### ğŸ›ï¸ æ‰‹åŠ¨è°ƒå‚")
        # åº”ç”¨ä¼˜åŒ–å‚æ•°
        if st.session_state.best_params and st.session_state.get('optimized_model_name') == model_name:
            st.info(f"ğŸ’¡ æ£€æµ‹åˆ°ä¼˜åŒ–å‚æ•° (RÂ²: {st.session_state.get('best_score', 0):.4f})")
            if st.button("ğŸ”„ åº”ç”¨æœ€ä½³å‚æ•°"):
                for k, v in st.session_state.best_params.items():
                    st.session_state[f"param_{model_name}_{k}"] = v
                st.rerun()

        # ç”Ÿæˆå‚æ•°æ§ä»¶ (ä¿®å¤çŠ¶æ€å†²çª)
        manual_params = {}
        if model_name in MANUAL_TUNING_PARAMS:
            configs = MANUAL_TUNING_PARAMS[model_name]
            p_cols = st.columns(2)
            for i, config in enumerate(configs):
                with p_cols[i % 2]:
                    key = f"param_{model_name}_{config['name']}"
                    # ä¼˜å…ˆåˆå§‹åŒ–
                    if key not in st.session_state: st.session_state[key] = config['default']

                    if config['widget'] == 'slider':
                        manual_params[config['name']] = st.slider(config['label'], key=key, **config.get('args', {}))
                    elif config['widget'] == 'number_input':
                        manual_params[config['name']] = st.number_input(config['label'], key=key,
                                                                        **config.get('args', {}))
                    elif config['widget'] == 'selectbox':
                        manual_params[config['name']] = st.selectbox(config['label'], options=config['args']['options'],
                                                                     key=key)
                    elif config['widget'] == 'text_input':
                        manual_params[config['name']] = st.text_input(config['label'], key=key)

    st.markdown("---")

    # æŒ‰é’®åŒº
    c_btn1, c_btn2 = st.columns(2)
    with c_btn1:
        if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", type="primary"):
            with st.spinner("è®­ç»ƒä¸­..."):
                # å‡†å¤‡å‚æ•°
                params = manual_params.copy()
                if 'random_state' in params: params.pop('random_state')

                # è®­ç»ƒ
                res = trainer.train_model(X, y, model_name, test_size, random_state, **params)

                # ä¿å­˜ç»“æœ
                st.session_state.model = res['model']
                st.session_state.train_result = res
                st.session_state.scaler = res['scaler']
                st.session_state.X_train = res['X_train'];
                st.session_state.X_test = res['X_test']
                st.session_state.y_train = res['y_train'];
                st.session_state.y_test = res['y_test']
                st.session_state.model_name = model_name
                st.session_state.manual_params = params  # ç”¨äºè„šæœ¬å¯¼å‡º

                st.success("âœ… è®­ç»ƒå®Œæˆ")

                # æŒ‡æ ‡
                m1, m2, m3 = st.columns(3)
                m1.metric("RÂ²", f"{res['r2']:.4f}")
                m2.metric("RMSE", f"{res['rmse']:.4f}")
                m3.metric("MAE", f"{res['mae']:.4f}")

                # --- æ–°å¢ï¼šç»“æœè¡¨æ ¼ä¸å¯¼å‡º ---
                st.markdown("### ğŸ“ˆ é¢„æµ‹ç»“æœè¯¦æƒ…")
                res_df = pd.DataFrame({"çœŸå®å€¼": res['y_test'], "é¢„æµ‹å€¼": res['y_pred']})
                res_df['æ®‹å·®'] = res_df['çœŸå®å€¼'] - res_df['é¢„æµ‹å€¼']

                t1, t2 = st.columns([3, 1])
                with t1:
                    st.dataframe(res_df, use_container_width=True, height=200)
                with t2:
                    csv = res_df.to_csv(index=False).encode('utf-8')
                    st.download_button("ğŸ“¥ å¯¼å‡ºç»“æœ CSV", csv, "predictions.csv", "text/csv")

                # --- ä¼˜åŒ–ï¼šå›¾ç‰‡å±…ä¸­ ---
                visualizer = Visualizer()
                col_img1, col_img2, col_img3 = st.columns([1, 2, 1])
                with col_img2:
                    if 'y_pred_train' in res:
                        fig, _ = visualizer.plot_parity_train_test(
                            res['y_train'], res['y_pred_train'],
                            res['y_test'], res['y_pred_test'],
                            target_name=st.session_state.target_col
                        )
                    else:
                        fig, _ = visualizer.plot_predictions_vs_true(res['y_test'], res['y_pred'], model_name)
                    st.pyplot(fig, use_container_width=True)

    with c_btn2:
        # è„šæœ¬å¯¼å‡ºæŒ‰é’®
        if st.session_state.model and st.session_state.model_name == model_name:
            # æ£€æŸ¥æ˜¯å¦æœ‰è„šæœ¬ç”Ÿæˆå‡½æ•°
            if 'generate_training_script_code' in globals():
                script = generate_training_script_code(model_name, manual_params, st.session_state.feature_cols,
                                                       st.session_state.target_col)
                st.download_button("ğŸ’¾ å¯¼å‡º Python è®­ç»ƒè„šæœ¬", script, "train_script.py")


def page_model_interpretation():
    """æ¨¡å‹è§£é‡Šé¡µé¢"""
    st.title("ğŸ“Š æ¨¡å‹è§£é‡Š")

    if st.session_state.model is None:
        st.warning("âš ï¸ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return

    model = st.session_state.model
    model_name = st.session_state.model_name
    X_train = st.session_state.X_train
    y_train = st.session_state.y_train
    X_test = st.session_state.X_test
    y_test = st.session_state.y_test
    feature_names = st.session_state.feature_cols

    tab1, tab2, tab3 = st.tabs(["ğŸ” SHAPåˆ†æ", "ğŸ“ˆ é¢„æµ‹æ€§èƒ½", "ğŸ¯ ç‰¹å¾é‡è¦æ€§"])

    # --- 1. SHAP åˆ†æ (æ¢å¤é€‰é¡¹) ---
    with tab1:
        st.markdown("### SHAPç‰¹å¾é‡è¦æ€§")

        # æ¢å¤è¿™ä¸¤ä¸ªé€‰é¡¹æ§ä»¶
        c_opt1, c_opt2 = st.columns(2)
        with c_opt1:
            plot_type = st.selectbox("å›¾è¡¨ç±»å‹", ["bar", "beeswarm"], index=0)
        with c_opt2:
            max_display = st.slider("æ˜¾ç¤ºç‰¹å¾æ•°é‡", 5, 50, 20)

        if st.button("ğŸ” è®¡ç®—SHAPå€¼"):
            with st.spinner("æ­£åœ¨è®¡ç®— SHAP å€¼ (å¯èƒ½è¾ƒæ…¢)..."):
                try:
                    interp = EnhancedModelInterpreter(
                        model, X_train, y_train, X_test, y_test,
                        model_name, feature_names=feature_names
                    )
                    # è°ƒç”¨ä¿®æ”¹åçš„ plot_summaryï¼Œè·å–å›¾å’Œæ•°æ®
                    fig, df_shap = interp.plot_summary(plot_type=plot_type, max_display=max_display)

                    if fig:
                        # é™åˆ¶å›¾ç‰‡å®½åº¦
                        c1, c2, c3 = st.columns([1, 6, 1])
                        with c2:
                            st.pyplot(fig, use_container_width=True)

                            # SHAP æ•°æ®å¯¼å‡º
                            if df_shap is not None:
                                csv = df_shap.to_csv(index=False).encode('utf-8')
                                st.download_button("ğŸ“¥ å¯¼å‡º SHAP æ•°æ® (CSV)", csv, "shap_values.csv", "text/csv")
                    else:
                        st.error("æ— æ³•ç”Ÿæˆ SHAP å›¾ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒã€‚")
                except Exception as e:
                    st.error(f"è®¡ç®—å‡ºé”™: {str(e)}")

    # --- 2. é¢„æµ‹æ€§èƒ½ ---
    with tab2:
        st.markdown("### é¢„æµ‹æ€§èƒ½")
        visualizer = Visualizer()
        c1, c2, c3 = st.columns([1, 2, 1])
        with c2:
            fig, df_res = visualizer.plot_residuals(y_test, st.session_state.train_result['y_pred'], model_name)
            st.pyplot(fig, use_container_width=True)
            csv = df_res.to_csv(index=False).encode('utf-8')
            st.download_button("ğŸ“¥ å¯¼å‡ºæ®‹å·®æ•°æ®", csv, "residuals.csv")

    # --- 3. ç‰¹å¾é‡è¦æ€§ (å« MACCS è§£é‡Š) ---
    with tab3:
        st.markdown("### ç‰¹å¾é‡è¦æ€§")
        if hasattr(model, 'feature_importances_'):
            visualizer = Visualizer()
            c1, c2, c3 = st.columns([1, 2, 1])
            with c2:
                fig, df_imp = visualizer.plot_feature_importance(model.feature_importances_, feature_names, model_name)
                st.pyplot(fig, use_container_width=True)
                csv = df_imp.to_csv(index=False).encode('utf-8')
                st.download_button("ğŸ“¥ å¯¼å‡ºé‡è¦æ€§æ•°æ®", csv, "importance.csv")

            # MACCS è§£é‡Šè¡¨
            st.markdown("#### ğŸ§¬ ç‰¹å¾å«ä¹‰è§£æ")
            exps = []
            for f in df_imp.head(15)['Feature']:
                desc = "æ•°å€¼ç‰¹å¾"
                if "MACCS" in f:
                    try:
                        # åŠ¨æ€å¯¼å…¥é˜²æ­¢æŠ¥é”™
                        from core.molecular_features import get_maccs_description
                        idx = int(f.split('_')[-1])
                        desc = get_maccs_description(idx)
                    except:
                        desc = "MACCS æŒ‡çº¹ç‰‡æ®µ"
                exps.append({"ç‰¹å¾å": f, "å«ä¹‰": desc})
            st.table(pd.DataFrame(exps))
        else:
            st.info("è¯¥æ¨¡å‹ä¸æ”¯æŒåŸç”Ÿç‰¹å¾é‡è¦æ€§ï¼Œè¯·ä½¿ç”¨ SHAP åˆ†æã€‚")

def page_prediction():
    """é¢„æµ‹åº”ç”¨é¡µé¢"""
    st.title("ğŸ”® é¢„æµ‹åº”ç”¨")

    if st.session_state.model is None:
        st.warning("âš ï¸ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return

    model = st.session_state.model
    model_name = st.session_state.model_name
    feature_cols = st.session_state.feature_cols
    scaler = st.session_state.scaler

    tab1, tab2, tab3 = st.tabs(["ğŸ“ æ‰‹åŠ¨è¾“å…¥", "ğŸ“ æ‰¹é‡é¢„æµ‹", "ğŸ¯ é€‚ç”¨åŸŸåˆ†æ"])

    with tab1:
        st.markdown("### æ‰‹åŠ¨è¾“å…¥ç‰¹å¾å€¼")

        input_values = {}
        cols = st.columns(3)

        for i, feature in enumerate(feature_cols):
            with cols[i % 3]:
                input_values[feature] = st.number_input(
                    feature,
                    value=0.0,
                    format="%.4f",
                    key=f"input_{feature}"
                )

        if st.button("ğŸ”® é¢„æµ‹", type="primary"):
            try:
                input_df = pd.DataFrame([input_values])

                # ä½¿ç”¨pipelineæˆ–ç›´æ¥é¢„æµ‹
                if st.session_state.pipeline is not None:
                    prediction = st.session_state.pipeline.predict(input_df)
                else:
                    prediction = model.predict(input_df)

                st.success(f"### é¢„æµ‹ç»“æœ: **{prediction[0]:.4f}**")

            except Exception as e:
                st.error(f"é¢„æµ‹å¤±è´¥: {str(e)}")

    with tab2:
        st.markdown("### æ‰¹é‡é¢„æµ‹")

        uploaded_file = st.file_uploader("ä¸Šä¼ å¾…é¢„æµ‹æ•°æ®", type=['csv', 'xlsx'])

        if uploaded_file is not None:
            try:
                if uploaded_file.name.endswith('.csv'):
                    new_df = pd.read_csv(uploaded_file)
                else:
                    new_df = pd.read_excel(uploaded_file)

                st.dataframe(new_df.head(), use_container_width=True)

                # æ£€æŸ¥ç‰¹å¾åˆ—
                missing_cols = set(feature_cols) - set(new_df.columns)
                if missing_cols:
                    st.error(f"ç¼ºå°‘ç‰¹å¾åˆ—: {missing_cols}")
                else:
                    if st.button("ğŸš€ æ‰§è¡Œæ‰¹é‡é¢„æµ‹"):
                        X_new = new_df[feature_cols]

                        if st.session_state.pipeline is not None:
                            predictions = st.session_state.pipeline.predict(X_new)
                        else:
                            predictions = model.predict(X_new)

                        new_df['é¢„æµ‹å€¼'] = predictions
                        st.dataframe(new_df, use_container_width=True)

                        # ä¸‹è½½
                        csv = new_df.to_csv(index=False).encode('utf-8')
                        st.download_button(
                            "ğŸ“¥ ä¸‹è½½é¢„æµ‹ç»“æœ",
                            csv,
                            "batch_predictions.csv",
                            "text/csv"
                        )
            except Exception as e:
                st.error(f"åŠ è½½å¤±è´¥: {str(e)}")

    with tab3:
        st.markdown("### é€‚ç”¨åŸŸåˆ†æ")
        st.info("åˆ†ææ–°æ ·æœ¬æ˜¯å¦åœ¨æ¨¡å‹è®­ç»ƒæ•°æ®çš„é€‚ç”¨èŒƒå›´å†…")

        if st.session_state.X_train is not None and scaler is not None:
            try:
                X_train = st.session_state.X_train

                # åˆ›å»ºåˆ†æå™¨
                analyzer = ApplicabilityDomainAnalyzer(X_train)

                st.markdown("#### è¾“å…¥å¾…åˆ†ææ ·æœ¬")

                input_values = {}
                cols = st.columns(3)
                for i, feature in enumerate(feature_cols):
                    with cols[i % 3]:
                        input_values[feature] = st.number_input(
                            feature,
                            value=0.0,
                            format="%.4f",
                            key=f"ad_input_{feature}"
                        )

                if st.button("ğŸ¯ åˆ†æé€‚ç”¨åŸŸ"):
                    input_df = pd.DataFrame([input_values])
                    is_in_domain, fig = analyzer.analyze(input_df, scaler)

                    if is_in_domain:
                        st.success("âœ… æ ·æœ¬åœ¨æ¨¡å‹é€‚ç”¨åŸŸå†…ï¼Œé¢„æµ‹ç»“æœå¯é ")
                    else:
                        st.warning("âš ï¸ æ ·æœ¬è¶…å‡ºæ¨¡å‹é€‚ç”¨åŸŸï¼Œé¢„æµ‹ç»“æœå¯èƒ½ä¸å¯é ")

                    st.pyplot(fig)
                    plt.close()

            except Exception as e:
                st.error(f"é€‚ç”¨åŸŸåˆ†æå¤±è´¥: {str(e)}")
        else:
            st.warning("éœ€è¦è®­ç»ƒæ•°æ®å’Œscaleræ‰èƒ½è¿›è¡Œé€‚ç”¨åŸŸåˆ†æ")


# ============================================================
# é¡µé¢ï¼šè¶…å‚ä¼˜åŒ–
# ============================================================
def page_hyperparameter_optimization():
    """è¶…å‚æ•°ä¼˜åŒ–é¡µé¢"""
    st.title("âš™ï¸ è¶…å‚æ•°ä¼˜åŒ–")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    if not st.session_state.feature_cols or not st.session_state.target_col:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ç‰¹å¾é€‰æ‹©é¡µé¢é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡å˜é‡")
        return

    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    feature_cols = st.session_state.feature_cols
    target_col = st.session_state.target_col

    X = df[feature_cols]
    y = df[target_col]

    st.markdown("### Optunaæ™ºèƒ½è¶…å‚æ•°ä¼˜åŒ–")

    col1, col2 = st.columns(2)

    with col1:
        trainer = EnhancedModelTrainer()
        available_models = trainer.get_available_models()

        # æ”¯æŒä¼˜åŒ–çš„æ¨¡å‹
        optimizable_models = [
            "éšæœºæ£®æ—", "XGBoost", "LightGBM", "CatBoost",
            "SVR", "Ridgeå›å½’", "Lassoå›å½’", "ElasticNet",
            "AdaBoost", "æ¢¯åº¦æå‡æ ‘"
        ]
        optimizable_models = [m for m in optimizable_models if m in available_models]

        model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", optimizable_models)

    with col2:
        n_trials = st.slider("ä¼˜åŒ–è½®æ•°", 10, 200, DEFAULT_OPTUNA_TRIALS)
        cv_folds = st.slider("äº¤å‰éªŒè¯æŠ˜æ•°", 3, 10, 5)

    # --- [æ–°å¢] è¿›åº¦æ¡ç»„ä»¶ ---
    progress_bar = st.progress(0)
    status_text = st.empty()

    if st.button("ğŸš€ å¼€å§‹ä¼˜åŒ–", type="primary"):
        try:
            optimizer = HyperparameterOptimizer()

            # å®šä¹‰è¿›åº¦æ›´æ–°å›è°ƒ
            def update_progress(p):
                progress_bar.progress(min(p, 1.0))
                status_text.text(f"æ­£åœ¨è¿›è¡Œä¼˜åŒ–... è¿›åº¦: {int(p * 100)}%")

            with st.spinner(f"æ­£åœ¨ä¼˜åŒ– {model_name}..."):
                # ä¼ é€’ progress_callback
                best_params, best_score, study = optimizer.optimize(
                    model_name, X, y,
                    n_trials=n_trials,
                    cv=cv_folds,
                    progress_callback=update_progress
                )

            # ä¼˜åŒ–å®Œæˆï¼Œè¿›åº¦æ¡æ»¡
            progress_bar.progress(100)
            status_text.text("ä¼˜åŒ–å®Œæˆï¼")

            st.success(f"âœ… ä¼˜åŒ–å®Œæˆï¼æœ€ä½³RÂ²åˆ†æ•°: {best_score:.4f}")

            # ä¿å­˜åˆ° session_state
            st.session_state.best_params = best_params
            st.session_state.best_score = best_score
            st.session_state.optimized_model_name = model_name  # è®°å½•ä¼˜åŒ–çš„æ˜¯å“ªä¸ªæ¨¡å‹

            st.markdown("### æœ€ä½³å‚æ•°")
            st.json(best_params)

            # ä¼˜åŒ–å†å²å¯è§†åŒ–
            if study is not None:
                st.markdown("### ä¼˜åŒ–å†å²")

                try:
                    import plotly.graph_objects as go

                    trials = study.trials
                    values = [t.value for t in trials if t.value is not None]

                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        y=values,
                        mode='lines+markers',
                        name='RÂ² Score'
                    ))
                    fig.update_layout(
                        title='ä¼˜åŒ–è¿‡ç¨‹',
                        xaxis_title='Trial',
                        yaxis_title='RÂ² Score'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                except:
                    pass

            # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒ
            if st.button("ğŸ¯ ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹"):
                result = trainer.train_model(
                    X, y,
                    model_name=model_name,
                    **best_params
                )

                st.session_state.model = result['model']
                st.session_state.model_name = model_name
                st.session_state.train_result = result
                st.session_state.best_params = best_params

                st.success(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼RÂ²: {result['r2']:.4f}")

        except Exception as e:
            st.error(f"âŒ ä¼˜åŒ–å¤±è´¥: {str(e)}")
            st.code(traceback.format_exc())


# ============================================================
# ä¸»å‡½æ•°
# ============================================================
def main():
    """ä¸»å‡½æ•°"""
    page = render_sidebar()

    if page == "ğŸ  é¦–é¡µ":
        page_home()
    elif page == "ğŸ“¤ æ•°æ®ä¸Šä¼ ":
        page_data_upload()
    elif page == "ğŸ” æ•°æ®æ¢ç´¢":
        page_data_explore()
    elif page == "ğŸ§¹ æ•°æ®æ¸…æ´—":
        page_data_cleaning()
    elif page == "âœ¨ æ•°æ®å¢å¼º":
        page_data_enhancement()
    elif page == "ğŸ§¬ åˆ†å­ç‰¹å¾":
        page_molecular_features()
    elif page == "ğŸ¯ ç‰¹å¾é€‰æ‹©":
        page_feature_selection()
    elif page == "ğŸ¤– æ¨¡å‹è®­ç»ƒ":
        page_model_training()
    elif page == "ğŸ“Š æ¨¡å‹è§£é‡Š":
        page_model_interpretation()
    elif page == "ğŸ”® é¢„æµ‹åº”ç”¨":
        page_prediction()
    elif page == "âš™ï¸ è¶…å‚ä¼˜åŒ–":
        page_hyperparameter_optimization()


if __name__ == "__main__":
    main()