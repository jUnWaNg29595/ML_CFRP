# -*- coding: utf-8 -*-
"""
ç¢³çº¤ç»´å¤åˆææ–™æ™ºèƒ½é¢„æµ‹å¹³å° v1.3.0
æ›´æ–°å†…å®¹ï¼š
1. ä¿®å¤SHAPå›¾è¡¨æ˜¾ç¤ºå’Œç‰¹å¾åç¼ºå¤±é—®é¢˜
2. ä¼˜åŒ–æ‰€æœ‰å›¾è¡¨å¸ƒå±€ï¼Œé˜²æ­¢ç¼©æ”¾å˜å½¢
3. ä¸ºæ‰€æœ‰å›¾è¡¨å¢åŠ æ•°æ®å¯¼å‡º(CSV)åŠŸèƒ½
4. å¢åŠ åŒç»„åˆ†åˆ†å­æŒ‡çº¹æ‹¼æ¥åŠŸèƒ½
5. å¢åŠ è®­ç»ƒè„šæœ¬ä¸€é”®å¯¼å‡ºåŠŸèƒ½
"""
try:
    import torchani

    TORCHANI_AVAILABLE = True
except ImportError:
    TORCHANI_AVAILABLE = False
import streamlit as st
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import torch
import traceback
import json
import io
from datetime import datetime
import multiprocessing as mp
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="ç¢³çº¤ç»´å¤åˆææ–™æ™ºèƒ½é¢„æµ‹å¹³å°",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from core.data_processor import AdvancedDataCleaner, SparseDataHandler, DataEnhancer
from core.data_explorer import EnhancedDataExplorer
from core.model_trainer import EnhancedModelTrainer, AutoGluonWrapper  # ç¡®ä¿å¼•å…¥ Wrapper
from core.model_interpreter import ModelInterpreter, EnhancedModelInterpreter
from core.molecular_features import AdvancedMolecularFeatureExtractor, RDKitFeatureExtractor
from core.feature_selector import SmartFeatureSelector, SmartSparseDataSelector, show_robust_feature_selection
from core.optimizer import HyperparameterOptimizer, InverseDesigner, generate_tuning_suggestions
from core.visualizer import Visualizer
from core.applicability_domain import ApplicabilityDomainAnalyzer, TanimotoADAnalyzer
from core.ui_config import (
    MANUAL_TUNING_PARAMS,
    MODEL_PARAMETERS,
    DEFAULT_OPTUNA_TRIALS,
    DEFAULT_TEST_SIZE,
    DEFAULT_RANDOM_STATE
)

from config import APP_NAME, VERSION, DATA_DIR
from generate_sample_data import generate_hybrid_dataset, generate_pure_numeric_dataset

# å¯é€‰æ¨¡å—å¯¼å…¥
try:
    from core.molecular_features import OptimizedRDKitFeatureExtractor, MemoryEfficientRDKitExtractor, \
        FingerprintExtractor

    OPTIMIZED_EXTRACTOR_AVAILABLE = True
except ImportError:
    OPTIMIZED_EXTRACTOR_AVAILABLE = False

try:
    from core.graph_utils import GNNFeaturizer, smiles_to_pyg_graph

    GNN_AVAILABLE = True
except ImportError:
    GNN_AVAILABLE = False

try:
    from core.ann_model import ANNRegressor

    ANN_AVAILABLE = True
except ImportError:
    ANN_AVAILABLE = False

try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False


@st.cache_data(ttl=3600, show_spinner="æ­£åœ¨è¯»å–æ•°æ®æ–‡ä»¶...")
def load_data_file(uploaded_file):
    """å¸¦ç¼“å­˜çš„æ•°æ®åŠ è½½å‡½æ•°ï¼Œé¿å…æ¯æ¬¡äº¤äº’éƒ½é‡æ–°è¯»å–æ–‡ä»¶"""
    # å¿…é¡»é‡ç½®æ–‡ä»¶æŒ‡é’ˆåˆ°å¼€å¤´ï¼Œå› ä¸ºStreamlitå¯èƒ½ä¼šå¤šæ¬¡è¯»å–åŒä¸€ä¸ªæ–‡ä»¶å¯¹è±¡
    uploaded_file.seek(0)

    if uploaded_file.name.endswith('.csv'):
        return pd.read_csv(uploaded_file)
    else:
        return pd.read_excel(uploaded_file)


# --- [æ–°å¢] ç”Ÿæˆç‹¬ç«‹è®­ç»ƒè„šæœ¬çš„å‡½æ•° ---
def generate_training_script_code(model_name, params, feature_cols, target_col):
    """ç”Ÿæˆç‹¬ç«‹çš„ Python è®­ç»ƒè„šæœ¬"""
    script_template = f'''# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨ç”Ÿæˆçš„æœºå™¨å­¦ä¹ è®­ç»ƒè„šæœ¬
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
æ¨¡å‹ç±»å‹: {model_name}
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, ExtraTreesRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
try: from xgboost import XGBRegressor
except ImportError: pass
try: from lightgbm import LGBMRegressor
except ImportError: pass
try: from catboost import CatBoostRegressor
except ImportError: pass

MODEL_NAME = "{model_name}"
FEATURE_COLS = {json.dumps(feature_cols, ensure_ascii=False)}
TARGET_COL = "{target_col}"
HYPERPARAMETERS = {json.dumps(params, indent=4, ensure_ascii=False)}
DATA_PATH = "data.csv" 

def load_and_train():
    print(f"æ­£åœ¨åŠ è½½æ•°æ®: {{DATA_PATH}}...")
    try:
        if DATA_PATH.endswith('.csv'): df = pd.read_csv(DATA_PATH)
        else: df = pd.read_excel(DATA_PATH)
    except FileNotFoundError:
        print("âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶")
        return

    X = df[FEATURE_COLS].values
    y = df[TARGET_COL].values
    y = pd.to_numeric(y, errors='coerce')
    mask = ~np.isnan(y)
    X, y = X[mask], y[mask]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    imputer = SimpleImputer(strategy='median')
    X_train = imputer.fit_transform(X_train)
    X_test = imputer.transform(X_test)
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    print(f"æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹: {{MODEL_NAME}}...")
    model = None
    if MODEL_NAME == "éšæœºæ£®æ—": model = RandomForestRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "XGBoost": model = XGBRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "LightGBM": model = LGBMRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "CatBoost": model = CatBoostRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "SVR": model = SVR(**HYPERPARAMETERS)
    elif MODEL_NAME == "å†³ç­–æ ‘": model = DecisionTreeRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "æ¢¯åº¦æå‡æ ‘": model = GradientBoostingRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "AdaBoost": model = AdaBoostRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "å¤šå±‚æ„ŸçŸ¥å™¨": model = MLPRegressor(**HYPERPARAMETERS)
    elif MODEL_NAME == "çº¿æ€§å›å½’": model = LinearRegression(**HYPERPARAMETERS)
    elif MODEL_NAME == "Ridgeå›å½’": model = Ridge(**HYPERPARAMETERS)
    elif MODEL_NAME == "Lassoå›å½’": model = Lasso(**HYPERPARAMETERS)
    elif MODEL_NAME == "ElasticNet": model = ElasticNet(**HYPERPARAMETERS)

    if model:
        print("å¼€å§‹è®­ç»ƒ...")
        model.fit(X_train, y_train)
        print("æ­£åœ¨è¯„ä¼°...")
        y_pred = model.predict(X_test)
        r2 = r2_score(y_test, y_pred)
        print(f"è®­ç»ƒå®Œæˆï¼RÂ² Score: {{r2:.4f}}")

if __name__ == "__main__":
    load_and_train()
'''
    return script_template


# --- å…¨å±€å¸¸é‡ ---
USER_DATA_DB = "datasets/user_data.csv"

# --- è‡ªå®šä¹‰ CSS æ ·å¼ ---
# --- è‡ªå®šä¹‰ CSS æ ·å¼ (å«å›¾ç‰‡é˜²æŠ–) ---
CUSTOM_CSS = """
<style>
    :root { --primary-color: #4F46E5; }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        border-radius: 12px; padding: 20px; color: white; text-align: center;
        margin: 8px 0;
    }
    /* å›¾ç‰‡å®¹å™¨é«˜åº¦å›ºå®šï¼Œé˜²æ­¢é¡µé¢æŠ–åŠ¨ */
    div[data-testid="stImage"] { min-height: 400px; display: flex; align-items: center; justify-content: center; }
    .stPlotlyChart { min-height: 400px; }
</style>
"""


st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


# ============================================================
# Session State åˆå§‹åŒ–
# ============================================================
def init_session_state():
    """åˆå§‹åŒ–æ‰€æœ‰session stateå˜é‡"""
    defaults = {
        'data': None,
        'processed_data': None,
        'molecular_features': None,
        'target_col': None,
        'feature_cols': [],
        'model': None,
        'model_name': None,
        'train_result': None,
        'cv_result': None,
        'scaler': None,
        'imputer': None,
        'pipeline': None,
        'X_train': None,
        'X_test': None,
        'y_train': None,
        'y_test': None,
        'optimization_history': [],
        'best_params': None,
        'molecular_feature_names': [],
        'optimized_model_name': None  # æ–°å¢ï¼šè®°å½•ä¼˜åŒ–çš„æ¨¡å‹å
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


init_session_state()


# ============================================================
# ä¾§è¾¹æ æ¸²æŸ“
# ============================================================
def render_sidebar():
    """æ¸²æŸ“ä¾§è¾¹æ å¯¼èˆª"""
    with st.sidebar:
        st.title(f"ğŸ”¬ {APP_NAME}")
        st.caption(f"ç‰ˆæœ¬ {VERSION}")
        st.markdown("---")

        page = st.radio(
            "ğŸ“Œ åŠŸèƒ½å¯¼èˆª",
            [
                "ğŸ  é¦–é¡µ",
                "ğŸ“¤ æ•°æ®ä¸Šä¼ ",
                "ğŸ” æ•°æ®æ¢ç´¢",
                "ğŸ§¹ æ•°æ®æ¸…æ´—",
                "âœ¨ æ•°æ®å¢å¼º",
                "ğŸ§¬ åˆ†å­ç‰¹å¾",
                "ğŸ¯ ç‰¹å¾é€‰æ‹©",
                "ğŸ¤– æ¨¡å‹è®­ç»ƒ",
                "ğŸ“Š æ¨¡å‹è§£é‡Š",
                "ğŸ”® é¢„æµ‹åº”ç”¨",
                "âš™ï¸ è¶…å‚ä¼˜åŒ–",
            ],
            label_visibility="collapsed"
        )

        st.markdown("---")
        st.markdown("### ğŸ“Š æ•°æ®çŠ¶æ€")

        # ä¼˜å…ˆè·å– processed_data (æ¸…æ´—/å¤„ç†åçš„æ•°æ®)
        current_df = st.session_state.get('processed_data')
        original_df = st.session_state.get('data')

        # ç¡®å®šè¦æ˜¾ç¤ºå“ªä¸ªæ•°æ®çš„ä¿¡æ¯
        display_df = current_df if current_df is not None else original_df

        if display_df is not None:
            # 1. æ˜¾ç¤ºè¡Œ/åˆ—æ•°
            status_label = "âœ… å½“å‰æ•°æ® (å·²æ¸…æ´—)" if current_df is not None else "âœ… åŸå§‹æ•°æ®"
            st.success(f"{status_label}\n\n**{display_df.shape[0]} è¡Œ Ã— {display_df.shape[1]} åˆ—**")

            # 2. æ˜¾ç¤ºåˆ†å­ç‰¹å¾çŠ¶æ€
            if st.session_state.get('molecular_features') is not None:
                mf = st.session_state.molecular_features
                st.info(f"ğŸ§¬ åˆ†å­ç‰¹å¾: {mf.shape[1]} ä¸ª")

            # 3. æ˜¾ç¤ºç‰¹å¾é€‰æ‹©çŠ¶æ€
            feature_cols = st.session_state.get('feature_cols')
            target_col = st.session_state.get('target_col')

            if feature_cols:
                st.info(f"ğŸ¯ å·²é€‰ç‰¹å¾ (X): {len(feature_cols)} ä¸ª")

            if target_col:
                st.caption(f"ğŸ¯ ç›®æ ‡å˜é‡ (Y): {target_col}")

        else:
            st.warning("âš ï¸ æœªåŠ è½½æ•°æ®")

        if st.session_state.model is not None:
            st.success(f"ğŸ¤– å·²è®­ç»ƒ: {st.session_state.model_name}")
            # å¦‚æœæœ‰è®­ç»ƒç»“æœï¼Œä¹Ÿå¯ä»¥æ˜¾ç¤ºR2
            if st.session_state.get('train_result'):
                r2 = st.session_state.train_result.get('r2', 0)
                st.caption(f"å½“å‰ RÂ²: {r2:.4f}")

        st.markdown("---")
        st.markdown("### ğŸ”§ ç³»ç»Ÿä¿¡æ¯")
        st.caption(f"CPUæ ¸å¿ƒ: {mp.cpu_count()}")
        if PSUTIL_AVAILABLE:
            mem = psutil.virtual_memory()
            st.caption(f"å†…å­˜ä½¿ç”¨: {mem.percent}%")

        return page


# ============================================================
# é¡µé¢ï¼šé¦–é¡µ
# ============================================================
def page_home():
    """é¦–é¡µ"""
    st.title("ğŸ”¬ ç¢³çº¤ç»´å¤åˆææ–™æ™ºèƒ½é¢„æµ‹å¹³å°")
    st.markdown(f"**ç‰ˆæœ¬ {VERSION}** ")

    st.markdown("---")

    # åŠŸèƒ½å¡ç‰‡
    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("""
        <div class="metric-card">
            <div class="metric-label">æ•°æ®å¤„ç†</div>
            <div class="metric-value">ğŸ“Š</div>
            <p>æ™ºèƒ½æ¸…æ´— Â· VAEå¢å¼º Â· ç±»åˆ«å¹³è¡¡</p>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.markdown("""
        <div class="metric-card metric-card-success">
            <div class="metric-label">åˆ†å­ç‰¹å¾</div>
            <div class="metric-value">ğŸ§¬</div>
            <p>RDKit Â· æŒ‡çº¹(MACCS) Â· å›¾ç‰¹å¾</p>
        </div>
        """, unsafe_allow_html=True)

    with col3:
        st.markdown("""
        <div class="metric-card metric-card-warning">
            <div class="metric-label">æ¨¡å‹è®­ç»ƒ</div>
            <div class="metric-value">ğŸ¤–</div>
            <p>15+æ¨¡å‹ Â· æ‰‹åŠ¨è°ƒå‚ Â· Optunaä¼˜åŒ–</p>
        </div>
        """, unsafe_allow_html=True)

    st.markdown("---")

    # æ ¸å¿ƒåŠŸèƒ½ä»‹ç»
    st.markdown("## ğŸš€ æ ¸å¿ƒåŠŸèƒ½")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        ### ğŸ“Š æ•°æ®å¤„ç†
        - **æ™ºèƒ½æ•°æ®æ¸…æ´—**: ç¼ºå¤±å€¼å¤„ç†ã€å¼‚å¸¸å€¼æ£€æµ‹ã€æ•°æ®ç±»å‹ä¿®å¤
        - **VAEæ•°æ®å¢å¼º**: åŸºäºå˜åˆ†è‡ªç¼–ç å™¨çš„è¡¨æ ¼æ•°æ®ç”Ÿæˆ
        - **ç±»åˆ«å¹³è¡¡**: è§£å†³åŒ–å­¦å•ä½“æ ·æœ¬ä¸å¹³è¡¡é—®é¢˜

        ### ğŸ§¬ åˆ†å­ç‰¹å¾æå–
        - **åˆ†å­æŒ‡çº¹**: MACCS Keys, Morgan (ECFP) æŒ‡çº¹
        - **RDKitæ ‡å‡†ç‰ˆ**: 200+åˆ†å­æè¿°ç¬¦
        - **å›¾ç¥ç»ç½‘ç»œç‰¹å¾**: åˆ†å­æ‹“æ‰‘ç»“æ„ç‰¹å¾
        - **MLåŠ›åœºç‰¹å¾**: ANI-2x é«˜ç²¾åº¦èƒ½é‡/åŠ›
        """)

    with col2:
        st.markdown("""
        ### ğŸ¤– æ¨¡å‹è®­ç»ƒ
        - **é›†æˆæ¨¡å‹**: éšæœºæ£®æ—ã€XGBoostã€LightGBMã€CatBoost
        - **AutoML**: AutoGluon è‡ªåŠ¨å»ºæ¨¡
        - **æ‰‹åŠ¨è°ƒå‚**: å¯è§†åŒ–å‚æ•°é…ç½®ç•Œé¢

        ### ğŸ“Š æ¨¡å‹è§£é‡Š
        - **SHAPåˆ†æ**: ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
        - **å­¦ä¹ æ›²çº¿**: æ¨¡å‹æ”¶æ•›åˆ†æ
        - **é€‚ç”¨åŸŸåˆ†æ**: PCAå‡¸åŒ…è¾¹ç•Œæ£€æµ‹
        """)

    st.markdown("---")

    # å¿«é€Ÿå¼€å§‹
    st.markdown("## âš¡ å¿«é€Ÿå¼€å§‹")
    st.info("""
    1. **ä¸Šä¼ æ•°æ®** â†’ æ”¯æŒCSVã€Excelæ ¼å¼
    2. **æ•°æ®æ¸…æ´—** â†’ ä½¿ç”¨â€œç±»åˆ«å¹³è¡¡â€å¤„ç†é«˜é¢‘å•ä½“
    3. **åˆ†å­ç‰¹å¾** â†’ æå–SMILESæŒ‡çº¹æˆ–æè¿°ç¬¦
    4. **ç‰¹å¾é€‰æ‹©** â†’ é€‰æ‹©ç›®æ ‡å˜é‡å’Œè¾“å…¥ç‰¹å¾
    5. **æ¨¡å‹è®­ç»ƒ** â†’ é€‰æ‹©æ¨¡å‹å¹¶è°ƒæ•´å‚æ•°
    6. **æ¨¡å‹è§£é‡Š** â†’ SHAPåˆ†æå’Œæ€§èƒ½è¯„ä¼°
    """)


# ============================================================
# é¡µé¢ï¼šæ•°æ®ä¸Šä¼ 
# ============================================================
def page_data_upload():
    """æ•°æ®ä¸Šä¼ é¡µé¢"""
    st.title("ğŸ“¤ æ•°æ®ä¸Šä¼ ")

    tab1, tab2 = st.tabs(["ğŸ“ ä¸Šä¼ æ–‡ä»¶", "ğŸ“ ç”Ÿæˆç¤ºä¾‹æ•°æ®"])

    with tab1:
        uploaded_file = st.file_uploader(
            "é€‰æ‹©æ•°æ®æ–‡ä»¶",
            type=['csv', 'xlsx', 'xls'],
            help="æ”¯æŒCSVå’ŒExcelæ ¼å¼"
        )

        if uploaded_file is not None:
            try:
                # ä½¿ç”¨ç¼“å­˜å‡½æ•°åŠ è½½æ•°æ®
                df = load_data_file(uploaded_file)

                # å»é‡ååˆ—
                if df.columns.duplicated().any():
                    st.warning("âš ï¸ æ£€æµ‹åˆ°é‡ååˆ—ï¼Œç³»ç»Ÿå·²è‡ªåŠ¨é‡å‘½åå¤„ç†")
                    df = df.loc[:, ~df.columns.duplicated()]

                st.session_state.data = df
                st.session_state.processed_data = df.copy()

                st.success(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {df.shape[0]}è¡Œ Ã— {df.shape[1]}åˆ—")

                # æ•°æ®é¢„è§ˆ
                st.markdown("### ğŸ“‹ æ•°æ®é¢„è§ˆ")
                st.dataframe(df.head(10), use_container_width=True)

                # åˆ—ä¿¡æ¯
                col1, col2 = st.columns(2)
                with col1:
                    st.markdown("#### æ•°å€¼åˆ—")
                    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
                    for col in numeric_cols[:10]:
                        st.markdown(f"<span class='feature-badge'>{col}</span>", unsafe_allow_html=True)
                    if len(numeric_cols) > 10:
                        st.caption(f"... ç­‰å…± {len(numeric_cols)} ä¸ªæ•°å€¼åˆ—")

                with col2:
                    st.markdown("#### æ–‡æœ¬åˆ—")
                    text_cols = df.select_dtypes(include=['object']).columns.tolist()
                    for col in text_cols[:10]:
                        st.markdown(f"<span class='feature-badge'>{col}</span>", unsafe_allow_html=True)
                    if len(text_cols) > 10:
                        st.caption(f"... ç­‰å…± {len(text_cols)} ä¸ªæ–‡æœ¬åˆ—")

            except Exception as e:
                st.error(f"âŒ åŠ è½½å¤±è´¥: {str(e)}")

    with tab2:
        st.markdown("### ç”Ÿæˆç¤ºä¾‹æ•°æ®é›†")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("#### ğŸ§ª æ··åˆæ•°æ®é›†")
            st.caption("åŒ…å«å·¥è‰ºå‚æ•°å’ŒSMILESåˆ†å­ç»“æ„")
            n_samples_hybrid = st.number_input("æ ·æœ¬æ•°é‡", 100, 2000, 500, key="n_hybrid")
            if st.button("ç”Ÿæˆæ··åˆæ•°æ®é›†", type="primary"):
                df = generate_hybrid_dataset(n_samples=n_samples_hybrid)
                st.session_state.data = df
                st.session_state.processed_data = df.copy()
                st.success(f"âœ… å·²ç”Ÿæˆæ··åˆæ•°æ®é›†: {df.shape}")
                st.dataframe(df.head(), use_container_width=True)

        with col2:
            st.markdown("#### ğŸ“Š çº¯æ•°å€¼æ•°æ®é›†")
            st.caption("ä»…åŒ…å«æ•°å€¼å‹å·¥è‰ºå‚æ•°")
            n_samples_numeric = st.number_input("æ ·æœ¬æ•°é‡", 100, 2000, 500, key="n_numeric")
            if st.button("ç”Ÿæˆçº¯æ•°å€¼æ•°æ®é›†", type="primary"):
                df = generate_pure_numeric_dataset(n_samples=n_samples_numeric)
                st.session_state.data = df
                st.session_state.processed_data = df.copy()
                st.success(f"âœ… å·²ç”Ÿæˆçº¯æ•°å€¼æ•°æ®é›†: {df.shape}")
                st.dataframe(df.head(), use_container_width=True)


# ============================================================
# é¡µé¢ï¼šæ•°æ®æ¢ç´¢
# ============================================================
def page_data_explore():
    """æ•°æ®æ¢ç´¢é¡µé¢"""
    st.title("ğŸ” æ•°æ®æ¢ç´¢")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    explorer = EnhancedDataExplorer(df)

    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“Š æè¿°ç»Ÿè®¡", "ğŸ”— ç›¸å…³æ€§åˆ†æ", "ğŸ“ˆ åˆ†å¸ƒå›¾", "â“ ç¼ºå¤±å€¼", "ğŸ’¾ å¯¼å‡º"
    ])

    with tab1:
        stats = explorer.generate_summary_stats()

        col1, col2, col3, col4 = st.columns(4)
        col1.metric("æ€»è¡Œæ•°", stats['basic_info']['total_rows'])
        col2.metric("æ€»åˆ—æ•°", stats['basic_info']['total_columns'])
        col3.metric("æ•°å€¼åˆ—", stats['basic_info']['numeric_columns'])
        col4.metric("ç¼ºå¤±å€¼", stats['basic_info']['missing_values'])

        st.markdown("### æ•°å€¼ç‰¹å¾ç»Ÿè®¡")
        if explorer.numeric_cols:
            st.dataframe(df[explorer.numeric_cols].describe(), use_container_width=True)

    with tab2:
        st.markdown("### ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ")
        fig = explorer.plot_correlation_matrix()
        if fig:
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("éœ€è¦è‡³å°‘2ä¸ªæ•°å€¼åˆ—")

        # é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹
        pairs = explorer.get_high_correlation_pairs(threshold=0.8)
        if pairs:
            st.markdown("### âš ï¸ é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹ (|r| > 0.8)")
            for p in pairs[:10]:
                st.write(f"- **{p['feature1']}** â†” **{p['feature2']}**: {p['correlation']:.3f}")

    with tab3:
        st.markdown("### æ•°å€¼ç‰¹å¾åˆ†å¸ƒ")
        fig = explorer.plot_distributions()
        if fig:
            st.plotly_chart(fig, use_container_width=True)

        st.markdown("### ç®±çº¿å›¾")
        fig_box = explorer.plot_boxplots()
        if fig_box:
            st.plotly_chart(fig_box, use_container_width=True)

    with tab4:
        st.markdown("### ç¼ºå¤±å€¼åˆ†æ")
        fig_missing = explorer.plot_missing_values()
        if fig_missing:
            st.plotly_chart(fig_missing, use_container_width=True)
        else:
            st.success("âœ… æ•°æ®æ— ç¼ºå¤±å€¼")

    with tab5:
        st.markdown("### å¯¼å‡ºæ•°æ®")
        col1, col2 = st.columns(2)

        with col1:
            csv = df.to_csv(index=False).encode('utf-8')
            st.download_button(
                "ğŸ“¥ ä¸‹è½½CSV",
                csv,
                "data_export.csv",
                "text/csv"
            )

        with col2:
            buffer = io.BytesIO()
            df.to_excel(buffer, index=False)
            st.download_button(
                "ğŸ“¥ ä¸‹è½½Excel",
                buffer.getvalue(),
                "data_export.xlsx",
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )


# ============================================================
# é¡µé¢ï¼šæ•°æ®æ¸…æ´—
# ============================================================
def page_data_cleaning():
    """æ•°æ®æ¸…æ´—é¡µé¢"""
    st.title("ğŸ§¹ æ•°æ®æ¸…æ´—")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    cleaner = AdvancedDataCleaner(df)

    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "â“ ç¼ºå¤±å€¼å¤„ç†", "ğŸ“Š å¼‚å¸¸å€¼æ£€æµ‹", "ğŸ”„ é‡å¤æ•°æ®", "ğŸ”§ æ•°æ®ç±»å‹", "ğŸ§© SMILESç»„åˆ†åˆ†åˆ—", "âš–ï¸ ç±»åˆ«å¹³è¡¡"
    ])

    with tab1:
        st.markdown("### ç¼ºå¤±å€¼å¤„ç†")

        # ç¼ºå¤±å€¼ç»Ÿè®¡
        missing = df.isnull().sum()
        missing = missing[missing > 0]

        if len(missing) > 0:
            st.warning(f"æ£€æµ‹åˆ° {len(missing)} åˆ—å­˜åœ¨ç¼ºå¤±å€¼")

            col1, col2 = st.columns(2)
            with col1:
                st.dataframe(pd.DataFrame({
                    'åˆ—å': missing.index,
                    'ç¼ºå¤±æ•°é‡': missing.values,
                    'ç¼ºå¤±æ¯”ä¾‹': (missing.values / len(df) * 100).round(2)
                }), use_container_width=True)

            with col2:
                strategy = st.selectbox(
                    "é€‰æ‹©å¡«å……ç­–ç•¥",
                    ["median", "mean", "mode", "knn", "drop_rows", "constant"]
                )

                fill_value = None
                if strategy == "constant":
                    fill_value = st.number_input("å¡«å……å¸¸æ•°å€¼", value=0.0)

                if st.button("ğŸ”§ æ‰§è¡Œç¼ºå¤±å€¼å¤„ç†", type="primary"):
                    cleaned_df = cleaner.handle_missing_values(strategy=strategy, fill_value=fill_value)
                    st.session_state.processed_data = cleaned_df
                    st.success("âœ… ç¼ºå¤±å€¼å¤„ç†å®Œæˆ")
                    st.rerun()
        else:
            st.success("âœ… æ•°æ®æ— ç¼ºå¤±å€¼")

    with tab2:
        st.markdown("### å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†")

        col1, col2 = st.columns(2)
        with col1:
            method = st.selectbox("æ£€æµ‹æ–¹æ³•", ["iqr", "zscore"])
            threshold = st.slider("é˜ˆå€¼", 1.0, 5.0, 1.5 if method == "iqr" else 3.0)

        with col2:
            handle_method = st.selectbox("å¤„ç†æ–¹æ³•", ["clip", "replace_median", "remove"])

        if st.button("ğŸ” æ£€æµ‹å¼‚å¸¸å€¼"):
            outliers = cleaner.detect_outliers(method=method, threshold=threshold)
            if outliers:
                st.warning(f"æ£€æµ‹åˆ° {len(outliers)} åˆ—å­˜åœ¨å¼‚å¸¸å€¼")
                st.json(outliers)
            else:
                st.success("âœ… æœªæ£€æµ‹åˆ°æ˜¾è‘—å¼‚å¸¸å€¼")

        if st.button("ğŸ”§ å¤„ç†å¼‚å¸¸å€¼", type="primary"):
            cleaned_df = cleaner.handle_outliers(method=handle_method, threshold=threshold)
            st.session_state.processed_data = cleaned_df
            st.success("âœ… å¼‚å¸¸å€¼å¤„ç†å®Œæˆ")

    with tab3:
        st.markdown("### ğŸ”„ æ•°æ®å»é‡ä¸åˆ†å¸ƒä¼˜åŒ–")

        col_clean_1, col_clean_2 = st.columns(2)

        with col_clean_1:
            st.markdown("#### 1. è¡Œå»é‡")
            st.caption("åˆ é™¤å®Œå…¨é‡å¤çš„æ ·æœ¬è¡Œ")
            dup_count = df.duplicated().sum()
            st.metric("å®Œå…¨é‡å¤è¡Œæ•°", dup_count)

            if dup_count > 0:
                if st.button("ğŸ—‘ï¸ åˆ é™¤é‡å¤è¡Œ", type="primary"):
                    cleaned_df = cleaner.remove_duplicates()
                    st.session_state.processed_data = cleaned_df
                    st.success(f"âœ… å·²åˆ é™¤ {dup_count} è¡Œé‡å¤æ•°æ®")
                    st.rerun()
            else:
                st.info("âœ… æ— é‡å¤è¡Œ")

        st.markdown("---")

        with col_clean_2:
            st.markdown("#### 2. ç‰¹å¾åˆ†å¸ƒä¼˜åŒ– (é’ˆå¯¹æ•°å€¼)")
            st.caption("é™ä½æŸä¸€ç‰¹å¾ä¸­ä¼—æ•°ï¼ˆå‡ºç°æœ€å¤šçš„å€¼ï¼‰çš„æ¯”ä¾‹ï¼Œå¹³è¡¡æ•°æ®åˆ†å¸ƒ")

            # æ£€æµ‹é˜ˆå€¼è®¾ç½®
            rep_threshold = st.slider("é«˜é‡å¤ç‡æ£€æµ‹é˜ˆå€¼", 0.5, 0.99, 0.8, 0.05,
                                      help="æ£€æµ‹ä¼—æ•°å æ¯”è¶…è¿‡æ­¤æ¯”ä¾‹çš„ç‰¹å¾")

            high_rep_cols = cleaner.detect_high_repetition_columns(rep_threshold)

            if high_rep_cols:
                st.warning(f"âš ï¸ æ£€æµ‹åˆ° {len(high_rep_cols)} ä¸ªç‰¹å¾å­˜åœ¨é«˜é‡å¤å€¼")

                # æ˜¾ç¤ºè¯¦æƒ…
                rep_data = []
                for col, info in high_rep_cols.items():
                    rep_data.append({
                        "ç‰¹å¾": col,
                        "ä¼—æ•°": str(info['most_frequent_value']),
                        "å½“å‰å æ¯”": f"{info['frequency'] * 100:.1f}%"
                    })
                st.dataframe(pd.DataFrame(rep_data), use_container_width=True, hide_index=True)

                # æ“ä½œåŒº
                st.markdown("##### ğŸ”§ æ‰§è¡Œä¼˜åŒ–")
                target_col = st.selectbox("é€‰æ‹©è¦ä¼˜åŒ–çš„ç‰¹å¾", list(high_rep_cols.keys()))

                # æ™ºèƒ½è®¡ç®—æ»‘å—èŒƒå›´ï¼šä¸èƒ½æ¯”å½“å‰å æ¯”è¿˜é«˜ï¼Œä¹Ÿä¸èƒ½å¤ªä½ï¼ˆå¦‚0%ï¼‰
                current_freq = high_rep_cols[target_col]['frequency']
                target_rate = st.slider(
                    f"ç›®æ ‡å æ¯” (é’ˆå¯¹ {target_col})",
                    0.1, float(current_freq), 0.5, 0.05,
                    help="é€šè¿‡éšæœºåˆ é™¤åŒ…å«ä¼—æ•°çš„æ ·æœ¬ï¼Œä½¿å…¶å æ¯”é™ä½åˆ°æ­¤å€¼"
                )

                if st.button(f"ğŸ“‰ é™ä½ '{target_col}' çš„é‡å¤ç‡", type="primary"):
                    original_len = len(df)
                    cleaned_df = cleaner.reduce_feature_repetition(target_col, target_rate)
                    new_len = len(cleaned_df)
                    st.session_state.processed_data = cleaned_df

                    st.success(f"âœ… ä¼˜åŒ–å®Œæˆï¼åˆ é™¤äº† {original_len - new_len} ä¸ªæ ·æœ¬")
                    st.info(f"ğŸ“Š å½“å‰è¡Œæ•°: {new_len}ï¼Œ'{target_col}' çš„ä¼—æ•°å æ¯”å·²è°ƒæ•´è‡³ {target_rate * 100:.1f}%")
                    st.rerun()
            else:
                st.success("âœ… æœªæ£€æµ‹åˆ°é«˜é‡å¤ç‡ç‰¹å¾")

        st.markdown("---")
        st.markdown("#### 3. ğŸ§ª æŒ‰é…æ–¹/é”®èšåˆé‡å¤è®°å½•ï¼ˆæ¨èï¼šTg / åŠ›å­¦æ€§è´¨ï¼‰")
        st.caption("åŒä¸€é…æ–¹(æˆ–åŒä¸€æµ‹è¯•å£å¾„)çš„é‡å¤æµ‹é‡å¾€å¾€ä¼šå¼•å…¥æ ‡ç­¾å™ªå£°ã€‚å¯¹ target åšç¨³å¥èšåˆï¼ˆå¦‚ medianï¼‰å¯æ˜¾è‘—æå‡æ³›åŒ–ç¨³å®šæ€§ã€‚")

        all_cols = df.columns.tolist()

        # é»˜è®¤èšåˆé”®ï¼šresin_smiles + curing_agent_smiles (+ tg_method)
        default_keys = []
        for k in ["resin_smiles", "curing_agent_smiles", "tg_method"]:
            if k in all_cols:
                default_keys.append(k)

        keys = st.multiselect(
            "é€‰æ‹©èšåˆé”®ï¼ˆGroup Byï¼‰",
            options=all_cols,
            default=default_keys,
            help="å»ºè®®ï¼šresin_smiles + curing_agent_smilesï¼›å¦‚æœå­˜åœ¨ tg_method ä¸”ç›®æ ‡ä¸º Tgï¼Œå»ºè®®ä¹ŸåŠ å…¥ tg_method ä»¥ç»Ÿä¸€å£å¾„ã€‚"
        )

        # é»˜è®¤ç›®æ ‡ï¼šä¼˜å…ˆç”¨å·²é€‰ targetï¼Œå…¶æ¬¡ tg_c
        default_target = st.session_state.get("target_col") if st.session_state.get("target_col") in all_cols else ("tg_c" if "tg_c" in all_cols else all_cols[0])
        target_col_for_agg = st.selectbox("é€‰æ‹©éœ€è¦èšåˆçš„ç›®æ ‡åˆ—", options=all_cols, index=all_cols.index(default_target))

        agg_method = st.selectbox("èšåˆæ–¹å¼", options=["median", "mean", "min", "max"], index=0)
        dropna_target = st.checkbox("åˆ é™¤èšåˆåç›®æ ‡ä»ä¸ºç©º(NaN)çš„ç»„", value=True)

        if keys:
            try:
                n_unique = df[keys].drop_duplicates().shape[0]
                dup_like = len(df) - n_unique
                c1, c2, c3 = st.columns(3)
                c1.metric("å½“å‰æ ·æœ¬æ•°", len(df))
                c2.metric("æŒ‰é”®å”¯ä¸€ç»„æ•°", n_unique)
                c3.metric("å¯åˆå¹¶çš„é‡å¤è®°å½•", dup_like)
            except Exception:
                pass

        if st.button("ğŸ”„ æ‰§è¡Œèšåˆï¼ˆç”Ÿæˆ *_rep_n / *_rep_stdï¼‰", type="primary"):
            if not keys:
                st.error("è¯·è‡³å°‘é€‰æ‹© 1 ä¸ªèšåˆé”®")
            else:
                try:
                    new_df = cleaner.aggregate_by_keys(keys=keys, target_col=target_col_for_agg, agg=agg_method, dropna_target=dropna_target)
                    st.session_state.processed_data = new_df
                    st.success(f"âœ… èšåˆå®Œæˆï¼š{len(df)} è¡Œ â†’ {len(new_df)} è¡Œ")
                    st.info("å·²ç”Ÿæˆé‡å¤ç»Ÿè®¡åˆ—ï¼štg_rep_n / tg_rep_stdï¼ˆæˆ– <target>_rep_n / <target>_rep_stdï¼‰")
                    st.rerun()
                except Exception as e:
                    st.error(f"âŒ èšåˆå¤±è´¥: {e}")

    with tab4:
        st.markdown("### æ•°æ®ç±»å‹è¯Šæ–­")

        pseudo_numeric = cleaner.detect_pseudo_numeric_columns()

        if pseudo_numeric:
            st.warning(f"æ£€æµ‹åˆ° {len(pseudo_numeric)} ä¸ªä¼ªæ•°å€¼åˆ—")
            st.json(pseudo_numeric)

            if st.button("ğŸ”§ ä¿®å¤ä¼ªæ•°å€¼åˆ—", type="primary"):
                cleaned_df = cleaner.fix_pseudo_numeric_columns()
                st.session_state.processed_data = cleaned_df
                st.success("âœ… æ•°æ®ç±»å‹ä¿®å¤å®Œæˆ")
        else:
            st.success("âœ… æ•°æ®ç±»å‹æ­£å¸¸")

        st.markdown("---")
        st.markdown("### ğŸ”¤ One-Hot ç¼–ç ï¼ˆæŠŠç±»åˆ«åˆ—è½¬æˆæ•°å€¼ç‰¹å¾ï¼‰")
        st.caption("é€‚åˆï¼štg_methodã€æ ‘è„‚ä½“ç³»ç±»å‹ç­‰ç±»åˆ«ä¿¡æ¯ã€‚å¦‚æœä½ å¸Œæœ›ä¸€ä¸ªæ¨¡å‹è¦†ç›–å¤šå£å¾„ï¼Œå¯å°† tg_method one-hot ååŠ å…¥ç‰¹å¾ã€‚")

        cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        if cat_cols:
            default_encode = ["tg_method"] if "tg_method" in cat_cols else []
            encode_cols = st.multiselect("é€‰æ‹©è¦ç¼–ç çš„åˆ—", options=cat_cols, default=default_encode)
            drop_first = st.checkbox("drop_firstï¼ˆå¯é€‰ï¼šé¿å…å®Œå…¨å…±çº¿ï¼‰", value=False)

            if st.button("ğŸ”¤ æ‰§è¡Œ One-Hot ç¼–ç ", type="primary"):
                if not encode_cols:
                    st.error("è¯·è‡³å°‘é€‰æ‹© 1 ä¸ªè¦ç¼–ç çš„åˆ—")
                else:
                    try:
                        new_df = cleaner.one_hot_encode(encode_cols, drop_first=drop_first, dummy_na=False)
                        st.session_state.processed_data = new_df
                        st.success(f"âœ… One-Hot ç¼–ç å®Œæˆï¼šåˆ—æ•° {df.shape[1]} â†’ {new_df.shape[1]}")
                        st.rerun()
                    except Exception as e:
                        st.error(f"âŒ One-Hot ç¼–ç å¤±è´¥: {e}")
        else:
            st.info("æœªæ£€æµ‹åˆ°å¯ç¼–ç çš„ç±»åˆ«åˆ—")

    with tab5:
        st.markdown("### ğŸ§© SMILESç»„åˆ†è‡ªåŠ¨åˆ†åˆ—ï¼ˆæ ‘è„‚/å›ºåŒ–å‰‚/æ”¹æ€§å‰‚ï¼‰")
        st.info(
            "ğŸ’¡ å°†å•å…ƒæ ¼å†…çš„å¤šç»„åˆ† SMILESï¼ˆå¦‚ 'A;B' æˆ– 'A + B' æˆ– 'A.B'ï¼‰è‡ªåŠ¨æ‹†åˆ†åˆ°å¤šåˆ—ï¼š"
            "ä¾‹å¦‚ curing_agent_smiles_1 / curing_agent_smiles_2 â€¦ã€‚"
            "åŒæ—¶å¯é€‰åš RDKit canonical åŒ–ï¼Œç”Ÿæˆ *_keyï¼ˆé…æ–¹é”®ï¼‰ï¼Œæ–¹ä¾¿åç»­ç±»åˆ«å¹³è¡¡ä¸åˆ†ç»„åˆ’åˆ†ã€‚"
        )

        from core.smiles_utils import split_smiles_column, build_formulation_key
        import re

        text_cols_local = df.select_dtypes(include=['object', 'category']).columns.tolist()
        smiles_cols = [c for c in text_cols_local if 'smiles' in c.lower()]
        candidate_cols = smiles_cols if smiles_cols else text_cols_local

        if not candidate_cols:
            st.warning("âš ï¸ æœªæ£€æµ‹åˆ°å¯åˆ†åˆ—çš„æ–‡æœ¬åˆ—ï¼ˆobject/categoryï¼‰ã€‚")
        else:
            # é»˜è®¤ä¼˜å…ˆï¼šresin_smiles / curing_agent_smiles
            default_cols = []
            for cand in ["resin_smiles", "curing_agent_smiles", "hardener_smiles", "curing_agent", "curing_agent_smiles"]:
                if cand in candidate_cols:
                    default_cols.append(cand)
            if not default_cols:
                default_cols = [candidate_cols[0]]

            cols_to_split = st.multiselect(
                "é€‰æ‹©è¦åˆ†åˆ—çš„åˆ—",
                options=candidate_cols,
                default=default_cols,
                help="å»ºè®®è‡³å°‘é€‰æ‹© resin_smiles ä¸ curing_agent_smiles ä¸¤åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚"
            )

            col_s1, col_s2, col_s3 = st.columns(3)
            with col_s1:
                max_components = st.slider("æœ€å¤§åˆ†åˆ—ç»„åˆ†æ•°", 1, 12, 6, help="æ¯åˆ—æœ€å¤šæ‹†æˆå¤šå°‘ä¸ªç»„åˆ†ï¼ˆ*_1~*_kï¼‰")
            with col_s2:
                canonicalize = st.checkbox("RDKit canonical åŒ–ç»„åˆ†ï¼ˆæ¨èï¼‰", value=True)
            with col_s3:
                keep_original = st.checkbox("ä¿ç•™åŸå§‹åˆ—", value=True)

            add_key = st.checkbox("ç”Ÿæˆ *_key é…æ–¹é”®ï¼ˆæ’åºå»é‡å '.' æ‹¼æ¥ï¼‰", value=True)
            add_n = st.checkbox("ç”Ÿæˆ *_n_components ç»„åˆ†æ•°åˆ—", value=True)

            if st.button("ğŸ§© æ‰§è¡Œåˆ†åˆ—", type="primary"):
                new_df = df.copy()
                created_cols = []

                for c in cols_to_split:
                    new_df, new_cols = split_smiles_column(
                        new_df,
                        column=c,
                        max_components=max_components,
                        canonicalize=canonicalize,
                        add_key=add_key,
                        add_n_components=add_n,
                        keep_original=keep_original,
                        prefix=None
                    )
                    created_cols.extend(new_cols)

                # å¦‚æœåŒæ—¶åˆ†åˆ—äº†æ ‘è„‚ä¸å›ºåŒ–å‰‚ï¼Œè‡ªåŠ¨ç”Ÿæˆä½“ç³»é…æ–¹é”® formulation_key
                if add_key:
                    resin_key = None
                    hard_key = None
                    for c in cols_to_split:
                        if resin_key is None and "resin" in c.lower():
                            if f"{c}_key" in new_df.columns:
                                resin_key = f"{c}_key"
                        if hard_key is None and ("curing" in c.lower() or "hardener" in c.lower()):
                            if f"{c}_key" in new_df.columns:
                                hard_key = f"{c}_key"
                    if resin_key and hard_key:
                        new_df = build_formulation_key(
                            new_df,
                            resin_key_col=resin_key,
                            hardener_key_col=hard_key,
                            new_col="formulation_key"
                        )
                        created_cols.append("formulation_key")

                st.session_state.processed_data = new_df
                st.success(f"âœ… åˆ†åˆ—å®Œæˆï¼šæ–°å¢ {len(created_cols)} åˆ—")
                if created_cols:
                    st.caption("æ–°å¢åˆ—ç¤ºä¾‹ï¼ˆå‰ 20 ä¸ªï¼‰ï¼š " + ", ".join(created_cols[:20]) + (" ..." if len(created_cols) > 20 else ""))
                st.rerun()

            st.markdown("---")
            st.markdown("#### ğŸ” åˆ†åˆ—åçš„ç±»åˆ«åˆ†å¸ƒå¿«é€Ÿä½“æ£€")
            st.caption("åˆ†åˆ—åé€šå¸¸ä¼šå‡ºç° *_1 / *_2 / *_key ç­‰åˆ—ï¼›è‹¥å‘ç°æŸç±»å æ¯”è¿‡é«˜ï¼Œå¯åœ¨å³ä¾§â€œç±»åˆ«å¹³è¡¡â€é¡µå¯¹è¯¥åˆ—æ‰§è¡Œé™åˆ¶ã€‚")

            preview_cols = [c for c in df.columns if c.endswith("_key") or re.search(r"_\d+$", c)]
            if preview_cols:
                prev_col = st.selectbox("é€‰æ‹©è¦æŸ¥çœ‹åˆ†å¸ƒçš„åˆ—", options=preview_cols)
                vc = df[prev_col].value_counts(dropna=False)
                if len(vc) > 0:
                    col_m1, col_m2, col_m3 = st.columns(3)
                    col_m1.metric("å”¯ä¸€ç±»åˆ«æ•°", int(len(vc)))
                    col_m2.metric("æœ€å¤§æ ·æœ¬æ•°", int(vc.max()))
                    col_m3.metric("ä¸­ä½æ•°æ ·æœ¬æ•°", int(vc.median()))
                    st.bar_chart(vc.head(10))

                    st.markdown("##### âš–ï¸ ä¸€é”®ç±»åˆ«ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰")
                    default_cap = int(max(1, vc.median()))
                    cap = st.slider(
                        "æ¯ä¸ªç±»åˆ«æœ€å¤§æ ·æœ¬æ•°",
                        min_value=1,
                        max_value=int(vc.max()),
                        value=default_cap,
                        help="å°†è¶…é«˜é¢‘çš„å•ä½“/é…æ–¹ä¸‹é‡‡æ ·åˆ°æŒ‡å®šä¸Šé™ï¼Œå‡å°‘æ•°æ®ä¸­â€œå•ç§åˆ†å­å•ä½“è¿‡å¤šâ€çš„åç½®ã€‚"
                    )
                    if st.button("âš–ï¸ ç«‹å³å¯¹è¯¥åˆ—æ‰§è¡Œå¹³è¡¡", key=f"quick_balance_{prev_col}"):
                        from core.data_processor import AdvancedDataCleaner
                        cleaner_tmp = AdvancedDataCleaner(df)
                        balanced_df = cleaner_tmp.balance_category_counts(prev_col, max_samples=int(cap))
                        st.session_state.processed_data = balanced_df
                        st.success(f"âœ… å·²å¯¹ {prev_col} æ‰§è¡Œç±»åˆ«å¹³è¡¡ï¼ˆmax_samples={int(cap)}ï¼‰")
                        st.rerun()

            else:
                st.info("å½“å‰æ•°æ®è¿˜æ²¡æœ‰ *_key æˆ– *_æ•°å­— çš„åˆ†åˆ—åˆ—ã€‚ä½ å¯ä»¥å…ˆç‚¹å‡»ä¸Šæ–¹æŒ‰é’®æ‰§è¡Œåˆ†åˆ—ã€‚")

    with tab6:
        st.markdown("### âš–ï¸ ç±»åˆ«å¹³è¡¡ (é’ˆå¯¹åŒ–å­¦ç»“æ„)")
        st.info(
            "ğŸ’¡ è§£å†³ç‰¹å®šå•ä½“/åˆ†å­é‡å¤æ¬¡æ•°è¿‡å¤šçš„é—®é¢˜ã€‚é€šè¿‡é™åˆ¶æ¯ä¸ªç±»åˆ«çš„æœ€å¤§æ ·æœ¬æ•°ï¼Œå¼ºåˆ¶æ•°æ®åˆ†å¸ƒæ›´å‡åŒ€ï¼Œé¿å…æ¨¡å‹åå‘å¸¸è§åˆ†å­ã€‚")

        # 1. é€‰æ‹©åˆ†ç±»åˆ—
        # é»˜è®¤å°è¯•æ‰¾ 'smiles' ç›¸å…³åˆ—
        text_cols = df.select_dtypes(include=['object']).columns.tolist()
        if text_cols:
            cat_col = st.selectbox("é€‰æ‹©è¦å¹³è¡¡çš„ç±»åˆ«åˆ— (é€šå¸¸æ˜¯SMILES)", text_cols)

            # 2. åˆ†æå½“å‰åˆ†å¸ƒ
            counts = df[cat_col].value_counts()
            n_unique = len(counts)

            col1, col2, col3 = st.columns(3)
            col1.metric("å”¯ä¸€ç±»åˆ«æ•°", n_unique)
            col2.metric("æœ€å¤§æ ·æœ¬æ•°", counts.max())
            col3.metric("ä¸­ä½æ•°æ ·æœ¬æ•°", int(counts.median()))

            st.markdown("#### Top 10 å‡ºç°æœ€é¢‘ç¹çš„åˆ†å­")
            st.bar_chart(counts.head(10))

            # 3. è®¾ç½®å¹³è¡¡å‚æ•°
            st.markdown("#### ğŸ”§ å¹³è¡¡è®¾ç½®")

            limit_val = st.slider(
                "æ¯ä¸ªç±»åˆ«çš„æœ€å¤§æ ·æœ¬æ•° (Max Samples per Category)",
                min_value=1,
                max_value=int(counts.max()),
                value=int(counts.median()) if n_unique > 0 else 10,
                help="å¦‚æœæŸåˆ†å­çš„å‡ºç°æ¬¡æ•°è¶…è¿‡æ­¤å€¼ï¼Œå¤šä½™çš„æ ·æœ¬å°†è¢«éšæœºä¸¢å¼ƒã€‚"
            )

            if st.button(f"âš–ï¸ æ‰§è¡Œå¹³è¡¡ (é™åˆ¶ä¸º {limit_val} ä¸ª)", type="primary"):
                old_len = len(df)
                cleaned_df = cleaner.balance_category_counts(cat_col, max_samples=limit_val)
                new_len = len(cleaned_df)

                st.session_state.processed_data = cleaned_df

                st.success(f"âœ… å¹³è¡¡å®Œæˆï¼")
                st.info(f"ğŸ“Š æ€»æ ·æœ¬æ•°ä» {old_len} å‡å°‘åˆ° {new_len} (åˆ é™¤äº† {old_len - new_len} ä¸ªè¿‡åº¦é‡å¤æ ·æœ¬)")
                st.rerun()
        else:
            st.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬åˆ—ï¼Œæ— æ³•æ‰§è¡Œç±»åˆ«å¹³è¡¡")


# ============================================================
# é¡µé¢ï¼šæ•°æ®å¢å¼º
# ============================================================
def page_data_enhancement():
    """æ•°æ®å¢å¼ºé¡µé¢"""
    st.title("âœ¨ æ•°æ®å¢å¼º")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    enhancer = DataEnhancer(df)

    tab1, tab2 = st.tabs(["ğŸ”® KNNæ™ºèƒ½å¡«å……", "ğŸ§¬ VAEç”Ÿæˆå¼å¢å¼º"])

    with tab1:
        st.markdown("### KNNæ™ºèƒ½å¡«å……")
        st.info("ä½¿ç”¨Kè¿‘é‚»ç®—æ³•é¢„æµ‹å¹¶å¡«å……ç¼ºå¤±å€¼ï¼Œæ¯”ç®€å•çš„å‡å€¼/ä¸­ä½æ•°å¡«å……æ›´å‡†ç¡®")

        n_neighbors = st.slider("Kå€¼ï¼ˆè¿‘é‚»æ•°é‡ï¼‰", 1, 20, 5)

        if st.button("ğŸ”§ æ‰§è¡ŒKNNå¡«å……", type="primary"):
            with st.spinner("æ­£åœ¨æ‰§è¡ŒKNNå¡«å……..."):
                filled_df = enhancer.knn_impute(n_neighbors=n_neighbors)
                st.session_state.processed_data = filled_df
                st.success("âœ… KNNå¡«å……å®Œæˆ")

                # å¯¹æ¯”
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("åŸå§‹ç¼ºå¤±å€¼", df.isnull().sum().sum())
                with col2:
                    st.metric("å¤„ç†åç¼ºå¤±å€¼", filled_df.isnull().sum().sum())

    with tab2:
        st.markdown("### VAEç”Ÿæˆå¼æ•°æ®å¢å¼º")
        st.info("ä½¿ç”¨å˜åˆ†è‡ªç¼–ç å™¨(VAE)å­¦ä¹ æ•°æ®åˆ†å¸ƒï¼Œç”Ÿæˆé«˜ä¿çœŸè™šæ‹Ÿæ•°æ®ç‚¹")

        col1, col2 = st.columns(2)
        with col1:
            n_samples = st.number_input("ç”Ÿæˆæ ·æœ¬æ•°é‡", 10, 1000, 100)
            latent_dim = st.slider("æ½œåœ¨ç©ºé—´ç»´åº¦", 4, 64, 16)
            h_dim = st.slider("éšè—å±‚ç»´åº¦", 32, 256, 128)

        with col2:
            epochs = st.slider("è®­ç»ƒè½®æ•°", 10, 500, 100)
            batch_size = st.selectbox("æ‰¹å¤§å°", [16, 32, 64, 128], index=1)
            learning_rate = st.number_input("å­¦ä¹ ç‡", 0.0001, 0.1, 0.001, format="%.4f")

        if st.button("ğŸš€ ç”Ÿæˆå¢å¼ºæ•°æ®", type="primary"):
            try:
                with st.spinner("æ­£åœ¨è®­ç»ƒVAEæ¨¡å‹..."):
                    progress_bar = st.progress(0)

                    generated_df, fig = enhancer.generate_with_vae(
                        n_samples=n_samples,
                        latent_dim=latent_dim,
                        h_dim=h_dim,
                        epochs=epochs,
                        batch_size=batch_size,
                        lr=learning_rate
                    )

                    progress_bar.progress(100)

                st.success(f"âœ… æˆåŠŸç”Ÿæˆ {len(generated_df)} ä¸ªæ ·æœ¬")

                # PCAå¯è§†åŒ–
                st.markdown("### ğŸ“Š PCAå¯è§†åŒ–å¯¹æ¯”")
                st.plotly_chart(fig, use_container_width=True)

                # åˆå¹¶é€‰é¡¹
                if st.checkbox("å°†ç”Ÿæˆæ•°æ®åˆå¹¶åˆ°åŸå§‹æ•°æ®"):
                    merged_df = pd.concat([df, generated_df], ignore_index=True)
                    st.session_state.processed_data = merged_df
                    st.success(f"âœ… åˆå¹¶åæ•°æ®: {merged_df.shape}")

            except Exception as e:
                st.error(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")


# ============================================================
# é¡µé¢ï¼šåˆ†å­ç‰¹å¾æå–ï¼ˆå®Œæ•´5ç§æ–¹æ³•ï¼‰
# ============================================================
def page_molecular_features():
    """åˆ†å­ç‰¹å¾æå–é¡µé¢ - å®Œæ•´è¿˜åŸ5ç§æ–¹æ³• + åˆ†å­æŒ‡çº¹ (é€‚é…åŒç»„åˆ†)"""
    st.title("ğŸ§¬ åˆ†å­ç‰¹å¾æå–")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    # ä¼˜å…ˆä½¿ç”¨å¤„ç†åçš„æ•°æ®
    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data

    # æ£€æµ‹SMILESåˆ—
    text_cols = df.select_dtypes(include=['object']).columns.tolist()
    smiles_candidates = [col for col in text_cols if 'smiles' in col.lower() or 'smi' in col.lower()]

    if not text_cols:
        st.warning("âš ï¸ æ•°æ®ä¸­æœªæ£€æµ‹åˆ°æ–‡æœ¬åˆ—ï¼Œæ— æ³•æå–åˆ†å­ç‰¹å¾")
        return

    st.markdown("### ğŸ”¬ SMILESåˆ—é€‰æ‹©")

    # -----------------------------
    # å¤šç»„åˆ†/æ··åˆç‰© SMILES å¤„ç†
    # è¯´æ˜ï¼š
    # 1) å•åˆ—é‡Œå¯èƒ½ç”¨ ";" ç­‰åˆ†éš”ç¬¦è¡¨ç¤ºå¤šä¸ªç»„åˆ†ï¼ˆRDKit ä¸èƒ½ç›´æ¥è§£æ ";"ï¼Œä½†èƒ½è§£æ "."ï¼‰
    # 2) ä¹Ÿå¯èƒ½æ¯ä¸ªç»„åˆ†å•ç‹¬å ä¸€åˆ—ï¼ˆå¦‚ resin_smiles_1, resin_smiles_2 ...ï¼‰
    # è¿™é‡ŒæŠŠå¤šä¸ªç»„åˆ†ç»Ÿä¸€è½¬æ¢ä¸ºâ€œå¤šç‰‡æ®µ SMILESâ€ï¼ˆç”¨ "." è¿æ¥ï¼‰ï¼Œå†äº¤ç»™ RDKit/æŒ‡çº¹æå–å™¨ã€‚
    # -----------------------------
    import re

    def _split_smiles_cell(x):
        """æŠŠå•å…ƒæ ¼é‡Œçš„ SMILES æ‹†æˆç»„åˆ†åˆ—è¡¨ã€‚

        ä»…æŠŠå¸¸è§â€œåˆ—è¡¨åˆ†éš”ç¬¦â€å½“ä½œç»„åˆ†è¾¹ç•Œï¼š;ã€ï¼›ã€|ã€ä»¥åŠå¸¦ç©ºæ ¼çš„ +
        æ³¨æ„ï¼šä¸æŠŠâ€œ/â€å½“åˆ†éš”ç¬¦ï¼ˆå®ƒæ˜¯ SMILES ç«‹ä½“åŒ–å­¦çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚
        """
        if x is None or (isinstance(x, float) and np.isnan(x)):
            return []
        s = str(x).strip()
        if not s or s.lower() == 'nan':
            return []
        # ç»Ÿä¸€ä¸­æ–‡åˆ†å·
        s = s.replace('ï¼›', ';')

        # å…ˆæŒ‰ ; æˆ– | åˆ†å‰²
        parts = re.split(r"\s*[;|]\s*", s)

        # å†æŒ‰â€œå¸¦ç©ºæ ¼çš„ +â€åˆ†å‰²ï¼ˆé¿å…è¯¯ä¼¤ [N+] è¿™ç±»å¸¦ç”µè·å†™æ³•ï¼‰
        final = []
        for p in parts:
            final.extend(re.split(r"\s+\+\s+", p))

        # æ¸…ç†ç©ºä¸²
        final = [p.strip() for p in final if p and p.strip()]
        return final

    def _combine_components(df_in: pd.DataFrame, cols: list[str]):
        """æŠŠå¤šåˆ—/å•åˆ— SMILES åˆå¹¶æˆå¤šç‰‡æ®µ SMILESï¼Œå¹¶è¿”å›(åˆå¹¶åçš„Series, ç»„åˆ†æ•°é‡Series)"""
        if not cols:
            return pd.Series([np.nan] * len(df_in)), pd.Series([0] * len(df_in))

        combined = []
        counts = []
        for _, row in df_in[cols].iterrows():
            comps = []
            for c in cols:
                comps.extend(_split_smiles_cell(row[c]))
            counts.append(len(comps))
            combined.append('.'.join(comps) if comps else np.nan)
        return pd.Series(combined), pd.Series(counts)

    col1, col2 = st.columns(2)
    with col1:
        default_idx = 0
        if smiles_candidates:
            default_idx = text_cols.index(smiles_candidates[0])

        # è¿™é‡Œæ˜ç¡®è¿™æ˜¯ç¬¬ä¸€ç»„åˆ†ï¼ˆé€šå¸¸æ˜¯æ ‘è„‚ï¼‰
        smiles_col = st.selectbox(
            "é€‰æ‹©åŒ…å«SMILESçš„åˆ— (æ ‘è„‚/ä¸»ä½“)",
            text_cols,
            index=default_idx
        )

    with col2:
        st.markdown("**ç¤ºä¾‹SMILES:**")
        samples = df[smiles_col].dropna().head(3).tolist()
        for s in samples:
            st.code(s[:50] + "..." if len(str(s)) > 50 else s)

    # --- å¤šç»„åˆ†è®¾ç½®ï¼ˆæ ‘è„‚ä¾§ï¼‰ ---
    st.markdown("#### ğŸ§© å¤šç»„åˆ†/æ··åˆç‰©è®¾ç½® (å¯é€‰)")
    resin_mix_mode = st.checkbox(
        "æ ‘è„‚ä¸ºå¤šç»„åˆ†ï¼ˆæˆ–å•å…ƒæ ¼å†…åŒ…å«å¤šä¸ªSMILESï¼‰",
        value=False,
        help="å¦‚æœä½ çš„æ ‘è„‚åˆ—é‡Œå‡ºç° 'A;B' è¿™ç§å†™æ³•ï¼Œæˆ–æœ‰ resin_smiles_1/resin_smiles_2 è¿™ç§å¤šåˆ—ç»„åˆ†ï¼Œè¯·å¼€å¯ã€‚"
    )

    resin_component_cols = [smiles_col]
    resin_mix_layout = "å•åˆ—"  # ä»…ç”¨äº UI è®°å½•
    add_component_count_features = False

    if resin_mix_mode:
        resin_mix_layout = st.radio(
            "æ ‘è„‚ç»„åˆ†åœ¨è¡¨æ ¼ä¸­çš„ç»„ç»‡æ–¹å¼",
            ["å•åˆ—ï¼ˆåŒä¸€å•å…ƒæ ¼ç”¨åˆ†éš”ç¬¦è¡¨ç¤ºå¤šä¸ªç»„åˆ†ï¼Œå¦‚ A;Bï¼‰", "å¤šåˆ—ï¼ˆæ¯åˆ—ä¸€ä¸ªç»„åˆ†ï¼Œå¦‚ resin_smiles_1/resin_smiles_2â€¦ï¼‰"],
            index=0
        )

        if resin_mix_layout.startswith("å¤šåˆ—"):
            # è‡ªåŠ¨æ¨èï¼šä¸æ‰€é€‰åˆ—åŒå‰ç¼€ã€ä¸”ä»¥ _æ•°å­— ç»“å°¾çš„åˆ—
            pattern = re.compile(rf"^{re.escape(smiles_col)}_\d+$")
            auto_cols = [c for c in text_cols if pattern.match(c)]
            # æŒ‰æœ«å°¾æ•°å­—æ’åº
            def _tail_num(colname: str):
                try:
                    return int(colname.split('_')[-1])
                except:
                    return 0
            auto_cols = sorted(auto_cols, key=_tail_num)
            resin_component_cols = st.multiselect(
                "é€‰æ‹©æ ‘è„‚ç»„åˆ†åˆ—",
                options=text_cols,
                default=auto_cols if auto_cols else [smiles_col],
                help="ç³»ç»Ÿä¼šæŠŠè¿™äº›åˆ—çš„æ‰€æœ‰éç©ºç»„åˆ†åˆå¹¶ä¸ºä¸€ä¸ªå¤šç‰‡æ®µSMILESï¼ˆç”¨ '.' è¿æ¥ï¼‰"
            )
        else:
            st.caption("å°†è‡ªåŠ¨æŠŠ ';'ã€'ï¼›'ã€'|'ã€ä»¥åŠå¸¦ç©ºæ ¼çš„ ' + ' è½¬æ¢ä¸ºå¤šç»„åˆ†åˆ†éš”ï¼Œå¹¶ç”¨ '.' è¿æ¥ã€‚")

        add_component_count_features = st.checkbox(
            "é¢å¤–åŠ å…¥ç»„åˆ†æ•°é‡ç‰¹å¾ï¼ˆresin_n_components / hardener_n_componentsï¼‰",
            value=True,
            help="å¯¹å¾ˆå¤šæ··é…ä½“ç³»ï¼Œç»„åˆ†æ•°é‡æœ¬èº«ä¹Ÿä¼šå½±å“æ€§èƒ½ï¼›æ­¤é€‰é¡¹ä¼šæŠŠç»„åˆ†æ•°ä½œä¸ºé¢å¤–æ•°å€¼ç‰¹å¾å¹¶å…¥æ•°æ®é›†ã€‚"
        )

    st.markdown("---")

    # ğŸ”¥ æ ¸å¿ƒåŠŸèƒ½ï¼š5ç§æå–æ–¹æ³•é€‰æ‹©
    st.markdown("### ğŸ› ï¸ æå–æ–¹æ³•é€‰æ‹©")

    extraction_method = st.radio(
        "é€‰æ‹©åˆ†å­ç‰¹å¾æå–æ–¹æ³•",
        [
            "ğŸ‘† åˆ†å­æŒ‡çº¹ (MACCS/Morgan) [æ–°]",
            "ğŸ”¹ RDKit æ ‡å‡†ç‰ˆ (æ¨èæ–°æ‰‹)",
            "ğŸš€ RDKit å¹¶è¡Œç‰ˆ (å¤§æ•°æ®é›†)",
            "ğŸ’¾ RDKit å†…å­˜ä¼˜åŒ–ç‰ˆ (ä½å†…å­˜)",
            "ğŸ”¬ Mordred æè¿°ç¬¦ (1600+ç‰¹å¾)",
            "ğŸ§Š 3Dæ„è±¡æè¿°ç¬¦ (RDKit3D+Coulomb) [æ–°]",
            "ğŸ§  é¢„è®­ç»ƒSMILES Transformer Embedding (ChemBERTaç­‰) [å¯é€‰]",
            "ğŸ•¸ï¸ å›¾ç¥ç»ç½‘ç»œç‰¹å¾ (æ‹“æ‰‘ç»“æ„)",
            "âš›ï¸ MLåŠ›åœºç‰¹å¾ (ANIèƒ½é‡/åŠ›)",
            "âš—ï¸ ç¯æ°§æ ‘è„‚ååº”ç‰¹å¾ (åŸºäºé¢†åŸŸçŸ¥è¯†)"
        ],
        help="ä¸åŒæ–¹æ³•é€‚ç”¨äºä¸åŒåœºæ™¯"
    )

    # UI å˜é‡åˆå§‹åŒ–
    fp_type = "MACCS"
    fp_bits = 2048
    fp_radius = 2
    hardener_col = None
    hardener_fusion_mode = "ä»…ç”¨äºæŒ‡çº¹/ååº”ç‰¹å¾ï¼ˆå½“å‰é»˜è®¤ï¼‰"  # åˆå§‹åŒ–å›ºåŒ–å‰‚åˆ—å˜é‡
    phr_col = None

    # ============== [ä¿®æ”¹] æŒ‡çº¹å‚æ•°è®¾ç½® ==============
    if "åˆ†å­æŒ‡çº¹" in extraction_method:
        st.info("ğŸ’¡ æç¤ºï¼šå¯¹äºç¯æ°§æ ‘è„‚ä½“ç³»ï¼Œå»ºè®®åŒæ—¶é€‰æ‹©æ ‘è„‚å’Œå›ºåŒ–å‰‚åˆ—ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æ‹¼æ¥ä¸¤è€…çš„æŒ‡çº¹ä»¥æè¿°å®Œæ•´ç½‘ç»œç»“æ„ã€‚")

        col_fp1, col_fp2, col_fp3 = st.columns(3)
        with col_fp1:
            fp_type = st.selectbox("æŒ‡çº¹ç±»å‹", ["MACCS", "Morgan"])

        if fp_type == "Morgan":
            with col_fp2:
                fp_radius = st.selectbox("åŠå¾„ (Radius)", [2, 3, 4], index=0)
            with col_fp3:
                fp_bits = st.selectbox("ä½é•¿ (Bits)", [1024, 2048, 4096], index=1)


        # ---- é¢„è®­ç»ƒ SMILES Transformer Embedding å‚æ•°ï¼ˆå¯é€‰ï¼‰----
        lm_model_name = "seyonec/ChemBERTa-zinc-base-v1"
        lm_pooling = "cls"
        lm_max_length = 128
        lm_batch_size = 16

        if "Transformer Embedding" in extraction_method:
            st.markdown("#### ğŸ§  é¢„è®­ç»ƒSMILES Transformer Embedding å‚æ•°")
            st.info("éœ€è¦å…ˆå®‰è£… transformersï¼›é¦–æ¬¡è¿è¡Œä¼šä¸‹è½½æ¨¡å‹æƒé‡ï¼ˆéœ€è¦è”ç½‘ï¼‰ã€‚æ¨¡å‹è¾“å‡ºç»´åº¦é€šå¸¸ä¸º 768ï¼Œå¯é…åˆåç»­ç‰¹å¾é€‰æ‹©/é™ç»´ä½¿ç”¨ã€‚")
            lm_model_name = st.text_input("HuggingFace æ¨¡å‹å", value=lm_model_name)
            col_lm1, col_lm2, col_lm3 = st.columns(3)
            with col_lm1:
                lm_pooling = st.selectbox("Pooling", ["cls", "mean"], index=0)
            with col_lm2:
                lm_max_length = st.selectbox("Max Length", [64, 128, 256], index=1)
            with col_lm3:
                lm_batch_size = st.selectbox("Batch Size", [4, 8, 16, 32], index=2)

        # [æ–°å¢] åŒç»„åˆ†é€‰æ‹© UI
        st.markdown("#### åŒç»„åˆ†è®¾ç½® (æ¨è)")
        col_h1, col_h2 = st.columns(2)
        with col_h1:
            # æ’é™¤å·²é€‰çš„æ ‘è„‚åˆ—ï¼Œé¿å…é‡å¤é€‰æ‹©
            candidate_cols = ["æ—  (ä»…æå–å•åˆ—)"] + [c for c in text_cols if c != smiles_col]
            hardener_col_opt = st.selectbox("é€‰æ‹©ã€å›ºåŒ–å‰‚ã€‘SMILESåˆ—", candidate_cols)

            if hardener_col_opt != "æ—  (ä»…æå–å•åˆ—)":
                hardener_col = hardener_col_opt

        with col_h2:
            hardener_fusion_mode = st.selectbox(
                "å›ºåŒ–å‰‚èå…¥æ–¹å¼",
                [
                    "ä»…ç”¨äºæŒ‡çº¹/ååº”ç‰¹å¾ï¼ˆå½“å‰é»˜è®¤ï¼‰",
                    "æ‹¼æ¥SMILESåç”¨äºæ‰€æœ‰åˆ†å­ç‰¹å¾ï¼ˆResin.Hardenerï¼‰"
                ],
                index=0,
                help="é€‰æ‹©ç¬¬äºŒé¡¹åï¼ŒRDKit/Mordred/3D/ANI/Transformer ç­‰æ–¹æ³•å°†å¯¹æ‹¼æ¥åçš„ SMILES æå–ç‰¹å¾ã€‚"
            )


    # ============== [UI] ç¯æ°§æ ‘è„‚ç‰¹å¾å‚æ•° ==============
    if "ç¯æ°§æ ‘è„‚ååº”ç‰¹å¾" in extraction_method:
        st.info("ğŸ’¡ è¯¥æ–¹æ³•éœ€è¦åŒæ—¶æä¾›ã€æ ‘è„‚ã€‘å’Œã€å›ºåŒ–å‰‚ã€‘çš„SMILESç»“æ„ã€‚")
        col_h, col_p = st.columns(2)
        with col_h:
            candidate_cols = [c for c in text_cols if c != smiles_col]
            hardener_col = st.selectbox("é€‰æ‹©ã€å›ºåŒ–å‰‚ã€‘SMILESåˆ—", candidate_cols)
        with col_p:
            num_cols = df.select_dtypes(include=np.number).columns.tolist()
            phr_col = st.selectbox("é€‰æ‹©ã€é…æ¯”/PHRã€‘åˆ— (å¯é€‰)", ["æ—  (å‡è®¾ç†æƒ³é…æ¯”)"] + num_cols)

    # å¹¶è¡Œç‰ˆå‚æ•°
    if "å¹¶è¡Œç‰ˆ" in extraction_method and OPTIMIZED_EXTRACTOR_AVAILABLE:
        col1, col2 = st.columns(2)
        with col1:
            n_jobs = st.slider("å¹¶è¡Œè¿›ç¨‹æ•°", 1, mp.cpu_count(), mp.cpu_count() // 2)
        with col2:
            batch_size = st.number_input("æ‰¹å¤„ç†å¤§å°", 100, 5000, 1000)

    st.markdown("---")

    # [ä¿®æ”¹] æŒ‰é’®åŒºåŸŸï¼šå¢åŠ æ¸…é™¤æŒ‰é’®
    col_btn1, col_btn2 = st.columns([1, 4])

    with col_btn1:
        run_extraction = st.button("ğŸš€ å¼€å§‹æå–åˆ†å­ç‰¹å¾", type="primary")

    with col_btn2:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤å·²æå–ç‰¹å¾"):
            # æ£€æŸ¥æ˜¯å¦æœ‰è®°å½•çš„ç‰¹å¾åˆ—å
            if st.session_state.get('molecular_feature_names'):
                current_df = st.session_state.processed_data
                # æ‰¾å‡ºå½“å‰æ•°æ®ä¸­å®é™…å­˜åœ¨çš„ç‰¹å¾åˆ—
                cols_to_remove = [c for c in st.session_state.molecular_feature_names if c in current_df.columns]

                if cols_to_remove:
                    # ä» processed_data ä¸­ç§»é™¤è¿™äº›åˆ—
                    st.session_state.processed_data = current_df.drop(columns=cols_to_remove)
                    # é‡ç½®çŠ¶æ€
                    st.session_state.molecular_features = None
                    st.session_state.molecular_feature_names = []
                    st.success(f"âœ… å·²æˆåŠŸæ¸…é™¤ {len(cols_to_remove)} ä¸ªåˆ†å­ç‰¹å¾åˆ—ï¼")
                    st.rerun()
                else:
                    st.warning("âš ï¸ æ•°æ®è¡¨ä¸­æœªæ‰¾åˆ°å¯æ¸…é™¤çš„ç‰¹å¾åˆ—ï¼ˆå¯èƒ½å·²è¢«ä¿®æ”¹ï¼‰ã€‚")

            elif st.session_state.get('molecular_features') is not None:
                # å…œåº•é€»è¾‘ï¼šå¦‚æœæœ‰ç‰¹å¾æ•°æ®ä½†æ²¡è®°å½•åˆ—åï¼ˆæ—§çŠ¶æ€ï¼‰ï¼Œå¼ºåˆ¶é‡ç½®çŠ¶æ€
                st.session_state.molecular_features = None
                st.warning("âš ï¸ ç‰¹å¾çŠ¶æ€å·²é‡ç½®ï¼Œä½†æ— æ³•è‡ªåŠ¨ä»æ•°æ®è¡¨ä¸­ç§»é™¤å…·ä½“åˆ—ï¼ˆå»ºè®®é‡æ–°ä¸Šä¼ æ•°æ®ï¼‰ã€‚")
                st.rerun()
            else:
                st.info("â„¹ï¸ å½“å‰æ²¡æœ‰å·²æå–çš„ç‰¹å¾ã€‚")

    # æ‰§è¡Œæå–é€»è¾‘
    if run_extraction:
        # -----------------------------
        # 1) ç”Ÿæˆâ€œå¯è¢« RDKit è§£æâ€çš„ SMILES åˆ—è¡¨
        #    - å•åˆ—å¤šç»„åˆ†ï¼šå°† ';' ç­‰åˆ†éš”ç¬¦è½¬æ¢ä¸º '.'
        #    - å¤šåˆ—å¤šç»„åˆ†ï¼šåˆå¹¶å¤šåˆ—ä¸º '.' è¿æ¥çš„å¤šç‰‡æ®µ SMILES
        # -----------------------------
        resin_smiles_series, resin_ncomp = _combine_components(df, resin_component_cols)
        smiles_list = resin_smiles_series.tolist()

        # 2) å›ºåŒ–å‰‚ï¼ˆå¯é€‰ï¼‰â€”â€”åŒæ ·æ”¯æŒå¤šç»„åˆ†
        hardener_list = None
        hardener_ncomp = None
        if hardener_col:
            # å¦‚æœç”¨æˆ·é€‰æ‹©äº† curing_agent_smiles è¿™ç±»åˆ—ï¼ŒåŒæ—¶å­˜åœ¨ curing_agent_smiles_1/2/â€¦ï¼Œåˆ™ç»™å‡ºå¤šåˆ—æ¨¡å¼
            hardener_component_cols = [hardener_col]
            if resin_mix_mode:
                # ä»…åœ¨å¯ç”¨å¤šç»„åˆ†æ¨¡å¼æ—¶æ‰å±•ç¤º/ä½¿ç”¨å›ºåŒ–å‰‚å¤šåˆ—åˆå¹¶é€»è¾‘ï¼Œé¿å… UI è¿‡å¤æ‚
                st.caption("ï¼ˆæç¤ºï¼‰å›ºåŒ–å‰‚ä¹Ÿæ”¯æŒå¤šç»„åˆ†ï¼šå¦‚æœæœ‰ hardener_col_1/2/â€¦ å¯åœ¨ä¸‹æ–¹è‡ªåŠ¨åˆå¹¶ã€‚")

            # è‡ªåŠ¨åˆå¹¶ï¼šå¦‚æœå­˜åœ¨ hardener_col_\d åˆ—ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨å®ƒä»¬ï¼ˆç”¨æˆ·æœªæ˜¾å¼å¤šé€‰æ—¶ï¼‰
            pattern_h = re.compile(rf"^{re.escape(hardener_col)}_\d+$")
            auto_h_cols = [c for c in text_cols if pattern_h.match(c)]
            if auto_h_cols:
                def _tail_num_h(colname: str):
                    try:
                        return int(colname.split('_')[-1])
                    except:
                        return 0
                auto_h_cols = sorted(auto_h_cols, key=_tail_num_h)
                hardener_component_cols = auto_h_cols

            hardener_smiles_series, hardener_ncomp = _combine_components(df, hardener_component_cols)
            hardener_list = hardener_smiles_series.tolist()

        try:
            progress_bar = st.progress(0)
            status_text = st.empty()

            # --- [æ–°å¢] å›ºåŒ–å‰‚èåˆï¼šå¯é€‰å°† Resin ä¸ Hardener SMILES æ‹¼æ¥åç”¨äºæ‰€æœ‰åˆ†å­ç‰¹å¾ ---
            smiles_list_input = smiles_list
            if hardener_list and isinstance(hardener_fusion_mode, str) and hardener_fusion_mode.startswith("æ‹¼æ¥SMILES"):
                def _safe_smiles(x):
                    if x is None or (isinstance(x, float) and np.isnan(x)):
                        return ""
                    s = str(x).strip()
                    return "" if s.lower() == "nan" else s

                smiles_list_input = []
                for r, h in zip(smiles_list, hardener_list):
                    rs = _safe_smiles(r)
                    hs = _safe_smiles(h)
                    if rs and hs:
                        smiles_list_input.append(f"{rs}.{hs}")
                    elif rs:
                        smiles_list_input.append(rs)
                    elif hs:
                        smiles_list_input.append(hs)
                    else:
                        smiles_list_input.append(np.nan)

            # --- [é€»è¾‘ä¿®æ”¹] åˆ†å‘æå–ä»»åŠ¡ ---
            if "åˆ†å­æŒ‡çº¹" in extraction_method:
                from core.molecular_features import FingerprintExtractor

                # æç¤ºç”¨æˆ·å½“å‰æ¨¡å¼
                mode_str = "åŒç»„åˆ†æ‹¼æ¥" if hardener_list else "å•ç»„åˆ†"
                status_text.text(f"æ­£åœ¨æå– {fp_type} æŒ‡çº¹ ({mode_str}æ¨¡å¼)...")

                extractor = FingerprintExtractor()
                # ä¼ å…¥ smiles_list_2 (å›ºåŒ–å‰‚)
                features_df, valid_indices = extractor.smiles_to_fingerprints(
                    smiles_list,
                    smiles_list_2=hardener_list,
                    fp_type=fp_type, n_bits=fp_bits, radius=fp_radius
                )

            elif "æ ‡å‡†ç‰ˆ" in extraction_method:
                status_text.text("æ­£åœ¨ä½¿ç”¨RDKitæ ‡å‡†ç‰ˆæå–...")
                extractor = AdvancedMolecularFeatureExtractor()
                features_df, valid_indices = extractor.smiles_to_rdkit_features(smiles_list_input)

            elif "å¹¶è¡Œç‰ˆ" in extraction_method:
                if OPTIMIZED_EXTRACTOR_AVAILABLE:
                    status_text.text(f"æ­£åœ¨ä½¿ç”¨RDKitå¹¶è¡Œç‰ˆæå– ({n_jobs}è¿›ç¨‹)...")
                    extractor = OptimizedRDKitFeatureExtractor(n_jobs=n_jobs, batch_size=batch_size)
                    features_df, valid_indices = extractor.smiles_to_rdkit_features(smiles_list_input)
                else:
                    st.warning("å¹¶è¡Œç‰ˆä¸å¯ç”¨ï¼Œå›é€€åˆ°æ ‡å‡†ç‰ˆ")
                    extractor = AdvancedMolecularFeatureExtractor()
                    features_df, valid_indices = extractor.smiles_to_rdkit_features(smiles_list_input)

            elif "å†…å­˜ä¼˜åŒ–ç‰ˆ" in extraction_method:
                status_text.text("æ­£åœ¨ä½¿ç”¨RDKitå†…å­˜ä¼˜åŒ–ç‰ˆ...")
                extractor = MemoryEfficientRDKitExtractor()
                features_df, valid_indices = extractor.smiles_to_rdkit_features(smiles_list_input)

            elif "Mordred" in extraction_method:
                status_text.text("æ­£åœ¨ä½¿ç”¨Mordredæå–...")
                extractor = AdvancedMolecularFeatureExtractor()
                features_df, valid_indices = extractor.smiles_to_mordred(smiles_list_input)

            elif "3Dæ„è±¡" in extraction_method:
                from core.molecular_features import RDKit3DDescriptorExtractor
                status_text.text("æ­£åœ¨æå–RDKit 3Dæ„è±¡æè¿°ç¬¦...")
                extractor = RDKit3DDescriptorExtractor()
                features_df, valid_indices = extractor.smiles_to_3d_descriptors(smiles_list_input)

            elif "Transformer Embedding" in extraction_method:
                from core.molecular_features import SmilesTransformerEmbeddingExtractor
                status_text.text("æ­£åœ¨åŠ è½½é¢„è®­ç»ƒTransformerå¹¶æå–Embedding...")
                extractor = SmilesTransformerEmbeddingExtractor(
                    model_name=lm_model_name,
                    pooling=lm_pooling,
                    max_length=lm_max_length
                )
                if not getattr(extractor, "AVAILABLE", False):
                    st.error("âŒ æœªæ£€æµ‹åˆ° transformersï¼Œè¯·å…ˆå®‰è£…ï¼špip install transformers")
                    st.stop()
                features_df, valid_indices = extractor.smiles_to_embeddings(smiles_list_input, batch_size=lm_batch_size)

            elif "å›¾ç¥ç»ç½‘ç»œ" in extraction_method:
                status_text.text("æ­£åœ¨æå–å›¾ç»“æ„ç‰¹å¾...")
                extractor = AdvancedMolecularFeatureExtractor()
                features_df, valid_indices = extractor.smiles_to_graph_features(smiles_list)

            elif "MLåŠ›åœº" in extraction_method:
                from core.molecular_features import MLForceFieldExtractor
                status_text.text("æ­£åœ¨è®¡ç®—ANIåŠ›åœºç‰¹å¾...")
                extractor = MLForceFieldExtractor()
                if not extractor.AVAILABLE:
                    st.error("TorchANI æœªå®‰è£…")
                    return
                features_df, valid_indices = extractor.smiles_to_ani_features(smiles_list_input)

            elif "ç¯æ°§æ ‘è„‚" in extraction_method:
                from core.molecular_features import EpoxyDomainFeatureExtractor
                status_text.text("æ­£åœ¨è®¡ç®—ç¯æ°§æ ‘è„‚é¢†åŸŸç‰¹å¾...")
                if hardener_col is None:
                    st.error("è¯·é€‰æ‹©å›ºåŒ–å‰‚åˆ—ï¼")
                    return

                phr_list = None
                if phr_col and phr_col != "æ—  (å‡è®¾ç†æƒ³é…æ¯”)":
                    phr_list = df[phr_col].tolist()

                extractor = EpoxyDomainFeatureExtractor()
                features_df, valid_indices = extractor.extract_features(smiles_list, hardener_list, phr_list)

            progress_bar.progress(100)

            # --- åˆå¹¶ç»“æœé€»è¾‘ ---
            if len(features_df) > 0:
                st.session_state.molecular_features = features_df
                prefix = f"{smiles_col}_"
                features_df = features_df.add_prefix(prefix)

                df_valid = df.iloc[valid_indices].reset_index(drop=True)
                features_df = features_df.reset_index(drop=True)

                # é˜²æ­¢åˆ—åå†²çªï¼šå¦‚æœæ–°ç‰¹å¾åå·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤æ—§çš„
                cols_to_drop = [col for col in features_df.columns if col in df_valid.columns]
                if cols_to_drop:
                    df_valid = df_valid.drop(columns=cols_to_drop)

                merged_df = pd.concat([df_valid, features_df], axis=1)

                # å¯é€‰ï¼šè¿½åŠ ç»„åˆ†æ•°é‡ç‰¹å¾
                if resin_mix_mode and add_component_count_features:
                    merged_df[f"{smiles_col}_resin_n_components"] = resin_ncomp.iloc[valid_indices].reset_index(drop=True)
                    if hardener_ncomp is not None:
                        merged_df[f"{smiles_col}_hardener_n_components"] = hardener_ncomp.iloc[valid_indices].reset_index(drop=True)

                merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]
                st.session_state.processed_data = merged_df
                # [æ–°å¢] ä¿å­˜ç‰¹å¾åˆ—ååˆ° Session Stateï¼Œä»¥ä¾¿åç»­æ¸…é™¤
                st.session_state.molecular_feature_names = features_df.columns.tolist()
                st.success(f"âœ… æˆåŠŸæå– {len(features_df)} ä¸ªæ ·æœ¬çš„ {features_df.shape[1]} ä¸ªåˆ†å­ç‰¹å¾")

                # ç»“æœç»Ÿè®¡
                col1, col2, col3 = st.columns(3)
                col1.metric("æœ‰æ•ˆæ ·æœ¬", len(valid_indices))
                col2.metric("ç‰¹å¾æ•°é‡", features_df.shape[1])
                col3.metric("åŒç»„åˆ†æ¨¡å¼", "æ˜¯" if hardener_list else "å¦")

                st.markdown("### ğŸ“‹ ç‰¹å¾é¢„è§ˆ")
                st.dataframe(features_df.head(), use_container_width=True)
            else:
                st.error("âŒ æœªèƒ½æå–ä»»ä½•ç‰¹å¾ï¼Œè¯·æ£€æŸ¥SMILESæ ¼å¼")

        except Exception as e:
            st.error(f"âŒ æå–å¤±è´¥: {str(e)}")
            st.code(traceback.format_exc())


# ============================================================
# é¡µé¢ï¼šç‰¹å¾é€‰æ‹©ï¼ˆå®Œæ•´ç‰ˆï¼‰
# ============================================================
def page_feature_selection():
    """ç‰¹å¾é€‰æ‹©é¡µé¢ - è°ƒç”¨å®Œæ•´çš„show_robust_feature_selection"""
    st.title("ğŸ¯ ç‰¹å¾é€‰æ‹©")

    # è°ƒç”¨å®Œæ•´çš„ç‰¹å¾é€‰æ‹©UI
    show_robust_feature_selection()



# ============================================================
# é¡µé¢ï¼šæ¨¡å‹è®­ç»ƒï¼ˆæ›´æ–°ç‰ˆï¼šå«è¡¨æ ¼ã€ä¸€é”®è¾“å‡ºã€å›¾ç‰‡é˜²æŠ–ï¼‰
# ============================================================
def page_model_training():
    """æ¨¡å‹è®­ç»ƒé¡µé¢ï¼ˆç¨³å¥ç‰ˆï¼šæ”¯æŒåˆ†å±‚/åˆ†ç»„åˆ’åˆ† + Repeated KFold CVï¼‰"""
    st.title("ğŸ¤– æ¨¡å‹è®­ç»ƒ")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    if not st.session_state.feature_cols:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ç‰¹å¾é€‰æ‹©é¡µé¢é€‰æ‹©ç‰¹å¾")
        return

    # æ•°æ®æº
    df_all = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    df = df_all.copy()

    # --- [P0-2] Tg å£å¾„è¿‡æ»¤ï¼šå»ºè®®åˆ†æ–¹æ³•å»ºæ¨¡ ---
    target_col = st.session_state.target_col
    if target_col and isinstance(target_col, str) and ("tg" in target_col.lower()) and ("tg_method" in df.columns):
        st.markdown("### ğŸ§ª Tg å£å¾„è¿‡æ»¤ï¼ˆtg_methodï¼‰")
        methods = df["tg_method"].dropna().astype(str).unique().tolist()
        methods = sorted(methods)
        method_options = ["å…¨éƒ¨"] + methods

        # é»˜è®¤ä¼˜å…ˆ DSCï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå¦åˆ™å…¨éƒ¨
        default_method = "å…¨éƒ¨"
        for prefer in ["DSC", "DMA-tanÎ´", "DMA-tanÎ´ (tanÎ´)", "DMA", "DSC (onset)"]:
            if prefer in methods:
                default_method = prefer
                break

        selected_method = st.selectbox(
            "é€‰æ‹©è®­ç»ƒæ•°æ®çš„ tg_method",
            options=method_options,
            index=method_options.index(default_method) if default_method in method_options else 0,
            help="ä¸åŒæµ‹è¯•å£å¾„ï¼ˆDSC / DMA-tanÎ´ ç­‰ï¼‰ä¼šé€ æˆç³»ç»Ÿåå·®ï¼›å»ºè®®åˆ†å£å¾„å»ºæ¨¡è·å¾—æ›´ç¨³å®šçš„æ³›åŒ–ã€‚"
        )

        if selected_method != "å…¨éƒ¨":
            before_n = len(df)
            df = df[df["tg_method"].astype(str) == str(selected_method)].copy()
            st.info(f"å·²è¿‡æ»¤ tg_method={selected_method}ï¼š{before_n} â†’ {len(df)} è¡Œ")

    # æ„é€  X / y
    try:
        X = df[st.session_state.feature_cols]
        y = df[target_col]
    except Exception as e:
        st.error(f"âŒ æ„é€ è®­ç»ƒæ•°æ®å¤±è´¥ï¼š{e}")
        return

    trainer = EnhancedModelTrainer()

    # åˆ†ç»„åˆ’åˆ†å¯ç”¨æ€§æ£€æµ‹
    has_resin = "resin_smiles" in df.columns
    has_hardener = "curing_agent_smiles" in df.columns
    group_key_options = []
    if has_resin and has_hardener:
        group_key_options = ["resin_smiles + curing_agent_smiles", "resin_smiles", "curing_agent_smiles"]
    elif has_resin:
        group_key_options = ["resin_smiles"]
    elif has_hardener:
        group_key_options = ["curing_agent_smiles"]

    col1, col2 = st.columns([1, 2])

    with col1:
        st.markdown("### ğŸ“¦ æ¨¡å‹é€‰æ‹©")
        model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", trainer.get_available_models())

        st.markdown("### âš™ï¸ è®­ç»ƒè®¾ç½®")
        test_size = st.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.4, 0.2)
        random_state = st.number_input("éšæœºç§å­", 0, 1000000, 42)

        # --- [P0-3 / P1-1] åˆ’åˆ†ç­–ç•¥ ---
        st.markdown("### ğŸ§© åˆ’åˆ†ç­–ç•¥")
        split_ui = st.selectbox(
            "é€‰æ‹©åˆ’åˆ†ç­–ç•¥",
            options=["éšæœºåˆ’åˆ†", "åˆ†å±‚åˆ’åˆ†(å›å½’åˆ†ç®±)", "æŒ‰é…æ–¹åˆ†ç»„åˆ’åˆ†"],
            index=1 if len(df) >= 50 else 0,
            help="å°æ ·æœ¬/è·¨åº¦å¤§å›å½’å»ºè®®ä½¿ç”¨â€œåˆ†å±‚åˆ’åˆ†â€ï¼›çœŸå®é…æ–¹æ³›åŒ–å»ºè®®ä½¿ç”¨â€œæŒ‰é…æ–¹åˆ†ç»„åˆ’åˆ†â€ã€‚"
        )

        split_strategy = "random"
        n_bins = 10
        groups = None
        group_key = None

        if split_ui.startswith("åˆ†å±‚"):
            split_strategy = "stratified"
            n_bins = st.slider("åˆ†å±‚åˆ†ç®±æ•°ï¼ˆå»ºè®® 8~12ï¼‰", 4, 20, 10)
        elif split_ui.startswith("æŒ‰é…æ–¹"):
            split_strategy = "group"
            if not group_key_options:
                st.warning("âš ï¸ å½“å‰æ•°æ®ç¼ºå°‘ resin_smiles / curing_agent_smilesï¼Œæ— æ³•ä½¿ç”¨åˆ†ç»„åˆ’åˆ†ï¼Œå°†å›é€€ä¸ºéšæœºåˆ’åˆ†ã€‚")
                split_strategy = "random"
            else:
                group_key = st.selectbox("åˆ†ç»„é”®", options=group_key_options, index=0)
                if group_key == "resin_smiles + curing_agent_smiles":
                    groups = df["resin_smiles"].astype(str) + "||" + df["curing_agent_smiles"].astype(str)
                elif group_key == "resin_smiles":
                    groups = df["resin_smiles"].astype(str)
                elif group_key == "curing_agent_smiles":
                    groups = df["curing_agent_smiles"].astype(str)

        # --- [P0-4 / P1-1] äº¤å‰éªŒè¯ ---
        st.markdown("### ğŸ§ª äº¤å‰éªŒè¯ (CV)")
        enable_cv = st.checkbox("åŒæ—¶è®¡ç®—äº¤å‰éªŒè¯ (æ¨è)", value=True)
        cv_folds = 5
        cv_repeats = 5
        if enable_cv:
            cv_folds = st.slider("CV folds", 3, 10, 5)
            cv_repeats = st.slider("CV repeatsï¼ˆä»…å¯¹ repeated kfold æœ‰æ•ˆï¼‰", 1, 10, 5)

    with col2:
        st.markdown("### ğŸ›ï¸ æ‰‹åŠ¨è°ƒå‚")

        # åº”ç”¨ä¼˜åŒ–å‚æ•°
        if st.session_state.best_params and st.session_state.get('optimized_model_name') == model_name:
            st.info(f"ğŸ’¡ æ£€æµ‹åˆ°ä¼˜åŒ–å‚æ•° (RÂ²: {st.session_state.get('best_score', 0):.4f})")
            if st.button("ğŸ”„ åº”ç”¨æœ€ä½³å‚æ•°"):
                for k, v in st.session_state.best_params.items():
                    st.session_state[f"param_{model_name}_{k}"] = v
                st.rerun()

        # ç”Ÿæˆå‚æ•°æ§ä»¶
        manual_params = {}
        if model_name in MANUAL_TUNING_PARAMS:
            configs = MANUAL_TUNING_PARAMS[model_name]
            p_cols = st.columns(2)
            for i, config in enumerate(configs):
                with p_cols[i % 2]:
                    key = f"param_{model_name}_{config['name']}"
                    if key not in st.session_state:
                        st.session_state[key] = config['default']

                    if config['widget'] == 'slider':
                        manual_params[config['name']] = st.slider(config['label'], key=key, **config.get('args', {}))
                    elif config['widget'] == 'number_input':
                        manual_params[config['name']] = st.number_input(config['label'], key=key, **config.get('args', {}))
                    elif config['widget'] == 'selectbox':
                        manual_params[config['name']] = st.selectbox(config['label'], options=config['args']['options'], key=key)
                    elif config['widget'] == 'text_input':
                        manual_params[config['name']] = st.text_input(config['label'], key=key)

    st.markdown("---")

    # æŒ‰é’®åŒº
    c_btn1, c_btn2 = st.columns(2)

    with c_btn1:
        if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ", type="primary"):
            with st.spinner("è®­ç»ƒä¸­..."):
                try:
                    # å‡†å¤‡å‚æ•°
                    params = manual_params.copy()
                    if 'random_state' in params:
                        params.pop('random_state')

                    # è®­ç»ƒï¼ˆæ”¯æŒ split_strategy / n_bins / groupsï¼‰
                    res = trainer.train_model(
                        X, y,
                        model_name=model_name,
                        test_size=test_size,
                        random_state=int(random_state),
                        split_strategy=split_strategy,
                        n_bins=int(n_bins),
                        groups=groups,
                        **params
                    )

                    # äº¤å‰éªŒè¯ï¼ˆå¯é€‰ï¼‰
                    cv_res = None
                    if enable_cv:
                        if split_strategy == "group" and groups is not None:
                            cv_strategy = "group_kfold"
                        elif split_strategy == "stratified":
                            cv_strategy = "stratified_kfold"
                        else:
                            cv_strategy = "repeated_kfold"

                        cv_res = trainer.cross_validate_model(
                            X, y,
                            model_name=model_name,
                            cv_strategy=cv_strategy,
                            n_splits=int(cv_folds),
                            n_repeats=int(cv_repeats),
                            random_state=int(random_state),
                            groups=groups,
                            n_bins=int(n_bins),
                            **params
                        )

                    # ä¿å­˜ç»“æœ
                    st.session_state.model = res['model']
                    st.session_state.pipeline = res.get('pipeline')
                    st.session_state.scaler = res.get('scaler')
                    st.session_state.imputer = res.get('imputer')
                    st.session_state.train_result = res
                    st.session_state.cv_result = cv_res

                    st.session_state.X_train = res['X_train']
                    st.session_state.X_test = res['X_test']
                    st.session_state.y_train = res['y_train']
                    st.session_state.y_test = res['y_test']
                    st.session_state.model_name = model_name
                    st.session_state.manual_params = params  # ç”¨äºè„šæœ¬å¯¼å‡º

                    st.success("âœ… è®­ç»ƒå®Œæˆ")

                    # --- æŒ‡æ ‡ ---
                    st.markdown("### ğŸ“Œ å•æ¬¡åˆ’åˆ†ï¼ˆTestï¼‰æŒ‡æ ‡")
                    m1, m2, m3, m4 = st.columns(4)
                    m1.metric("RÂ² (Test)", f"{res['r2']:.4f}")
                    m2.metric("RMSE (Test)", f"{res['rmse']:.4f}")
                    m3.metric("MAE (Test)", f"{res['mae']:.4f}")
                    m4.metric("Train Time (s)", f"{res.get('train_time', 0):.2f}")

                    if cv_res is not None:
                        st.markdown("### ğŸ§ª äº¤å‰éªŒè¯ï¼ˆCVï¼‰æŒ‡æ ‡")
                        c1, c2, c3 = st.columns(3)
                        c1.metric("CV RÂ² (meanÂ±std)", f"{cv_res['cv_r2_mean']:.4f} Â± {cv_res['cv_r2_std']:.4f}")
                        c2.metric("OOF RMSE", f"{cv_res['oof_rmse']:.4f}")
                        c3.metric("OOF MAE", f"{cv_res['oof_mae']:.4f}")

                        # æŠ˜åˆ†æ•°è¡¨
                        fold_df = pd.DataFrame({
                            "fold_r2": cv_res.get("fold_r2", []),
                            "fold_rmse": cv_res.get("fold_rmse", []),
                            "fold_mae": cv_res.get("fold_mae", []),
                        })
                        st.dataframe(fold_df, use_container_width=True, height=200)

                    # --- ç»“æœè¡¨æ ¼ä¸å¯¼å‡º ---
                    st.markdown("### ğŸ“ˆ æµ‹è¯•é›†é¢„æµ‹ç»“æœè¯¦æƒ…")
                    res_df = pd.DataFrame({
                        "çœŸå®å€¼": res['y_test'],
                        "é¢„æµ‹å€¼": res['y_pred_test'] if 'y_pred_test' in res else res['y_pred']
                    })
                    res_df["æ®‹å·®"] = res_df["çœŸå®å€¼"] - res_df["é¢„æµ‹å€¼"]

                    t1, t2 = st.columns([3, 1])
                    with t1:
                        st.dataframe(res_df, use_container_width=True, height=200)
                    with t2:
                        csv = res_df.to_csv(index=False).encode('utf-8')
                        st.download_button("ğŸ“¥ å¯¼å‡ºç»“æœ CSV", csv, "predictions_test.csv", "text/csv")

                    # --- å¯è§†åŒ– ---
                    st.markdown("### ğŸ“‰ æ€§èƒ½å¯è§†åŒ–")
                    visualizer = Visualizer()

                    if cv_res is not None:
                        tab_a, tab_b = st.tabs(["Train/Test", "CV (OOF)"])
                        with tab_a:
                            col_img1, col_img2, col_img3 = st.columns([1, 2, 1])
                            with col_img2:
                                fig_tt, _ = visualizer.plot_parity_train_test(
                                    res['y_train'], res['y_pred_train'],
                                    res['y_test'], res['y_pred_test'],
                                    target_name=target_col
                                )
                                st.pyplot(fig_tt, use_container_width=True)
                        with tab_b:
                            col_img1, col_img2, col_img3 = st.columns([1, 2, 1])
                            with col_img2:
                                fig_oof, _ = visualizer.plot_predictions_vs_true(
                                    cv_res['oof_true'],
                                    cv_res['oof_pred'],
                                    model_name=f"{model_name} (OOF)"
                                )
                                st.pyplot(fig_oof, use_container_width=True)
                    else:
                        col_img1, col_img2, col_img3 = st.columns([1, 2, 1])
                        with col_img2:
                            fig, _ = visualizer.plot_parity_train_test(
                                res['y_train'], res['y_pred_train'],
                                res['y_test'], res['y_pred_test'],
                                target_name=target_col
                            )
                            st.pyplot(fig, use_container_width=True)

                except Exception as e:
                    st.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")

    with c_btn2:
        # è„šæœ¬å¯¼å‡ºæŒ‰é’®
        if st.session_state.model and st.session_state.model_name == model_name:
            if 'generate_training_script_code' in globals():
                script = generate_training_script_code(
                    model_name,
                    manual_params,
                    st.session_state.feature_cols,
                    st.session_state.target_col
                )
                st.download_button("ğŸ’¾ å¯¼å‡º Python è®­ç»ƒè„šæœ¬", script, "train_script.py")

def page_model_interpretation():
    """æ¨¡å‹è§£é‡Šé¡µé¢"""
    st.title("ğŸ“Š æ¨¡å‹è§£é‡Š")

    if st.session_state.model is None:
        st.warning("âš ï¸ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return

    model = st.session_state.model
    model_name = st.session_state.model_name
    X_train = st.session_state.X_train
    y_train = st.session_state.y_train
    X_test = st.session_state.X_test
    y_test = st.session_state.y_test
    feature_names = st.session_state.feature_cols

    tab1, tab2, tab3 = st.tabs(["ğŸ” SHAPåˆ†æ", "ğŸ“ˆ é¢„æµ‹æ€§èƒ½", "ğŸ¯ ç‰¹å¾é‡è¦æ€§"])

    # --- 1. SHAP åˆ†æ (æ¢å¤é€‰é¡¹) ---
    with tab1:
        st.markdown("### SHAPç‰¹å¾é‡è¦æ€§")

        # æ¢å¤è¿™ä¸¤ä¸ªé€‰é¡¹æ§ä»¶
        c_opt1, c_opt2 = st.columns(2)
        with c_opt1:
            plot_type = st.selectbox("å›¾è¡¨ç±»å‹", ["bar", "beeswarm"], index=0)
        with c_opt2:
            max_display = st.slider("æ˜¾ç¤ºç‰¹å¾æ•°é‡", 5, 50, 20)

        if st.button("ğŸ” è®¡ç®—SHAPå€¼"):
            with st.spinner("æ­£åœ¨è®¡ç®— SHAP å€¼ (å¯èƒ½è¾ƒæ…¢)..."):
                try:
                    interp = EnhancedModelInterpreter(
                        model, X_train, y_train, X_test, y_test,
                        model_name, feature_names=feature_names
                    )
                    # è°ƒç”¨ä¿®æ”¹åçš„ plot_summaryï¼Œè·å–å›¾å’Œæ•°æ®
                    fig, df_shap = interp.plot_summary(plot_type=plot_type, max_display=max_display)

                    if fig:
                        # é™åˆ¶å›¾ç‰‡å®½åº¦
                        c1, c2, c3 = st.columns([1, 6, 1])
                        with c2:
                            st.pyplot(fig, use_container_width=True)

                            # SHAP æ•°æ®å¯¼å‡º
                            if df_shap is not None:
                                csv = df_shap.to_csv(index=False).encode('utf-8')
                                st.download_button("ğŸ“¥ å¯¼å‡º SHAP æ•°æ® (CSV)", csv, "shap_values.csv", "text/csv")
                    else:
                        st.error("æ— æ³•ç”Ÿæˆ SHAP å›¾ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒã€‚")
                except Exception as e:
                    st.error(f"è®¡ç®—å‡ºé”™: {str(e)}")

    # --- 2. é¢„æµ‹æ€§èƒ½ ---
    with tab2:
        st.markdown("### é¢„æµ‹æ€§èƒ½")
        visualizer = Visualizer()
        c1, c2, c3 = st.columns([1, 2, 1])
        with c2:
            fig, df_res = visualizer.plot_residuals(y_test, st.session_state.train_result['y_pred'], model_name)
            st.pyplot(fig, use_container_width=True)
            csv = df_res.to_csv(index=False).encode('utf-8')
            st.download_button("ğŸ“¥ å¯¼å‡ºæ®‹å·®æ•°æ®", csv, "residuals.csv")

    # --- 3. ç‰¹å¾é‡è¦æ€§ (å« MACCS è§£é‡Š) ---
    with tab3:
        st.markdown("### ç‰¹å¾é‡è¦æ€§")
        if hasattr(model, 'feature_importances_'):
            visualizer = Visualizer()
            c1, c2, c3 = st.columns([1, 2, 1])
            with c2:
                fig, df_imp = visualizer.plot_feature_importance(model.feature_importances_, feature_names, model_name)
                st.pyplot(fig, use_container_width=True)
                csv = df_imp.to_csv(index=False).encode('utf-8')
                st.download_button("ğŸ“¥ å¯¼å‡ºé‡è¦æ€§æ•°æ®", csv, "importance.csv")

            # MACCS è§£é‡Šè¡¨
            st.markdown("#### ğŸ§¬ ç‰¹å¾å«ä¹‰è§£æ")
            exps = []
            for f in df_imp.head(15)['Feature']:
                desc = "æ•°å€¼ç‰¹å¾"
                if "MACCS" in f:
                    try:
                        # åŠ¨æ€å¯¼å…¥é˜²æ­¢æŠ¥é”™
                        from core.molecular_features import get_maccs_description
                        idx = int(f.split('_')[-1])
                        desc = get_maccs_description(idx)
                    except:
                        desc = "MACCS æŒ‡çº¹ç‰‡æ®µ"
                exps.append({"ç‰¹å¾å": f, "å«ä¹‰": desc})
            st.table(pd.DataFrame(exps))
        else:
            st.info("è¯¥æ¨¡å‹ä¸æ”¯æŒåŸç”Ÿç‰¹å¾é‡è¦æ€§ï¼Œè¯·ä½¿ç”¨ SHAP åˆ†æã€‚")

def page_prediction():
    """é¢„æµ‹åº”ç”¨é¡µé¢ï¼ˆä¿®å¤ï¼šé¢„æµ‹é˜¶æ®µåº”ç”¨ imputer/scalerï¼›æ”¯æŒæŒ‡çº¹é€‚ç”¨åŸŸï¼‰"""
    st.title("ğŸ”® é¢„æµ‹åº”ç”¨")

    if st.session_state.model is None:
        st.warning("âš ï¸ è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        return

    model = st.session_state.model
    model_name = st.session_state.model_name
    feature_cols = st.session_state.feature_cols
    pipeline = st.session_state.get("pipeline", None)
    scaler = st.session_state.get("scaler", None)
    imputer = st.session_state.get("imputer", None)

    tab1, tab2, tab3 = st.tabs(["ğŸ“ å•æ ·æœ¬é¢„æµ‹", "ğŸ“ æ‰¹é‡é¢„æµ‹", "ğŸ¯ é€‚ç”¨åŸŸåˆ†æ"])

    # ============================================================
    # Tab1: å•æ ·æœ¬é¢„æµ‹
    # ============================================================
    with tab1:
        st.markdown("### å•æ ·æœ¬é¢„æµ‹")

        input_df = None

        # ç‰¹å¾è¿‡å¤šæ—¶ï¼Œç¦æ­¢æ¸²æŸ“å¤§é‡ number_inputï¼ˆä¼šå¡æ­»ï¼‰
        if len(feature_cols) <= 60:
            input_values = {}
            cols = st.columns(3)
            for i, feature in enumerate(feature_cols):
                with cols[i % 3]:
                    input_values[feature] = st.number_input(feature, value=0.0)
            input_df = pd.DataFrame([input_values])
        else:
            st.info(f"å½“å‰ç‰¹å¾æ•°é‡è¾ƒå¤šï¼ˆ{len(feature_cols)}ï¼‰ï¼Œå»ºè®®ç”¨â€œå•è¡Œæ–‡ä»¶â€ä¸Šä¼ è¿›è¡Œå•æ ·æœ¬é¢„æµ‹ã€‚")
            single_file = st.file_uploader("ä¸Šä¼ å•æ ·æœ¬æ–‡ä»¶ï¼ˆCSV/Excelï¼Œè‡³å°‘åŒ…å«æ‰€é€‰ç‰¹å¾åˆ—ï¼‰", type=['csv', 'xlsx', 'xls'], key="single_pred_file")
            if single_file is not None:
                try:
                    tmp_df = load_data_file(single_file)
                    if tmp_df.shape[0] == 0:
                        st.error("æ–‡ä»¶ä¸ºç©º")
                    else:
                        row_idx = 0
                        if tmp_df.shape[0] > 1:
                            row_idx = st.number_input("é€‰æ‹©é¢„æµ‹çš„è¡Œå·ï¼ˆä» 0 å¼€å§‹ï¼‰", 0, int(tmp_df.shape[0]-1), 0)
                        input_df = tmp_df.iloc[[int(row_idx)]].copy()
                except Exception as e:
                    st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")

        if st.button("ğŸ”® å¼€å§‹é¢„æµ‹", type="primary"):
            if input_df is None:
                st.error("è¯·å…ˆæä¾›è¾“å…¥æ ·æœ¬")
            else:
                # ä¿è¯åˆ—é½å…¨
                missing = [c for c in feature_cols if c not in input_df.columns]
                if missing:
                    st.error(f"è¾“å…¥æ ·æœ¬ç¼ºå°‘ç‰¹å¾åˆ—: {missing[:10]}{'...' if len(missing)>10 else ''}")
                else:
                    X_in = input_df[feature_cols]

                    try:
                        if pipeline is not None:
                            pred = pipeline.predict(X_in)[0]
                        else:
                            # [P0-5] ä¿®å¤ï¼šæ²¡æœ‰ pipeline æ—¶ä¹Ÿè¦åº”ç”¨ imputer + scaler
                            X_arr = X_in.values
                            if imputer is not None:
                                X_arr = imputer.transform(X_arr)
                            if scaler is not None:
                                X_arr = scaler.transform(X_arr)
                            pred = model.predict(X_arr)[0]

                        st.success(f"âœ… é¢„æµ‹ç»“æœï¼š{pred:.4f}")

                    except Exception as e:
                        st.error(f"âŒ é¢„æµ‹å¤±è´¥: {e}")

    # ============================================================
    # Tab2: æ‰¹é‡é¢„æµ‹
    # ============================================================
    with tab2:
        st.markdown("### æ‰¹é‡é¢„æµ‹")
        uploaded_file = st.file_uploader("ä¸Šä¼ å¾…é¢„æµ‹æ•°æ®", type=['csv', 'xlsx', 'xls'], key="batch_pred_file")

        if uploaded_file is not None:
            try:
                pred_df = load_data_file(uploaded_file)
                st.dataframe(pred_df.head(), use_container_width=True)

                if st.button("ğŸš€ æ‰§è¡Œæ‰¹é‡é¢„æµ‹", type="primary"):
                    missing = [c for c in feature_cols if c not in pred_df.columns]
                    if missing:
                        st.error(f"ç¼ºå°‘ç‰¹å¾åˆ—: {missing[:10]}{'...' if len(missing)>10 else ''}")
                    else:
                        X_pred = pred_df[feature_cols]

                        if pipeline is not None:
                            preds = pipeline.predict(X_pred)
                        else:
                            X_arr = X_pred.values
                            if imputer is not None:
                                X_arr = imputer.transform(X_arr)
                            if scaler is not None:
                                X_arr = scaler.transform(X_arr)
                            preds = model.predict(X_arr)

                        pred_df['prediction'] = preds
                        st.success("âœ… æ‰¹é‡é¢„æµ‹å®Œæˆ")
                        st.dataframe(pred_df.head(20), use_container_width=True)

                        csv = pred_df.to_csv(index=False).encode('utf-8')
                        st.download_button("ğŸ“¥ ä¸‹è½½é¢„æµ‹ç»“æœ CSV", csv, "batch_predictions.csv", "text/csv")

            except Exception as e:
                st.error(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")

    # ============================================================
    # Tab3: é€‚ç”¨åŸŸåˆ†æ
    # ============================================================
    with tab3:
        st.markdown("### ğŸ¯ é€‚ç”¨åŸŸåˆ†æ")
        st.caption("é€‚ç”¨åŸŸç”¨äºåˆ¤æ–­æ–°æ ·æœ¬æ˜¯å¦â€œè¶…å‡ºè®­ç»ƒæ•°æ®è¦†ç›–èŒƒå›´â€ã€‚PCA Hull æ›´é€šç”¨ï¼›Tanimoto æ›´é€‚ç”¨äºæŒ‡çº¹ç‰¹å¾ã€‚")

        # å¯ç”¨æ–¹æ³•
        fp_cols = [c for c in feature_cols if ("morgan" in c.lower()) or ("maccs" in c.lower())]
        has_fp = len(fp_cols) > 0

        methods = ["PCA Hullï¼ˆæ•°å€¼ç©ºé—´ï¼‰"]
        if has_fp and st.session_state.get("train_result") is not None and "X_train_raw" in st.session_state.train_result:
            methods.append("Tanimoto ç›¸ä¼¼åº¦ï¼ˆæŒ‡çº¹ï¼‰")

        ad_method = st.selectbox("é€‰æ‹©é€‚ç”¨åŸŸæ–¹æ³•", options=methods, index=0)

        # -------- PCA Hull --------
        if ad_method.startswith("PCA"):
            st.info("PCA Hullï¼šåœ¨é™ç»´ç©ºé—´æ„å»ºå‡¸åŒ…ï¼Œåˆ¤æ–­æ–°æ ·æœ¬æ˜¯å¦è½åœ¨è®­ç»ƒé›†è¦†ç›–èŒƒå›´å†…ï¼ˆå¯¹ä»»æ„æ•°å€¼ç‰¹å¾é€šç”¨ï¼‰ã€‚")

            input_df = None
            if len(feature_cols) <= 60:
                input_values = {}
                cols = st.columns(3)
                for i, feature in enumerate(feature_cols):
                    with cols[i % 3]:
                        input_values[feature] = st.number_input(feature, value=0.0, key=f"ad_pca_{feature}")
                input_df = pd.DataFrame([input_values])
            else:
                st.info(f"å½“å‰ç‰¹å¾æ•°é‡è¾ƒå¤šï¼ˆ{len(feature_cols)}ï¼‰ï¼Œå»ºè®®ç”¨â€œå•è¡Œæ–‡ä»¶â€ä¸Šä¼ è¿›è¡Œé€‚ç”¨åŸŸåˆ¤æ–­ã€‚")
                single_file = st.file_uploader("ä¸Šä¼ å•æ ·æœ¬æ–‡ä»¶ï¼ˆCSV/Excelï¼Œä¸€è¡Œå³å¯ï¼‰", type=['csv', 'xlsx', 'xls'], key="ad_single_file_pca")
                if single_file is not None:
                    try:
                        tmp_df = load_data_file(single_file)
                        if tmp_df.shape[0] > 0:
                            row_idx = 0
                            if tmp_df.shape[0] > 1:
                                row_idx = st.number_input("é€‰æ‹©åˆ†æçš„è¡Œå·ï¼ˆä» 0 å¼€å§‹ï¼‰", 0, int(tmp_df.shape[0]-1), 0, key="ad_pca_row_idx")
                            input_df = tmp_df.iloc[[int(row_idx)]].copy()
                    except Exception as e:
                        st.error(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")

            if st.button("ğŸ¯ é€‚ç”¨åŸŸåˆ†æï¼ˆPCA Hullï¼‰", type="primary"):
                if input_df is None:
                    st.error("è¯·å…ˆæä¾›è¾“å…¥æ ·æœ¬")
                else:
                    missing = [c for c in feature_cols if c not in input_df.columns]
                    if missing:
                        st.error(f"è¾“å…¥æ ·æœ¬ç¼ºå°‘ç‰¹å¾åˆ—: {missing[:10]}{'...' if len(missing)>10 else ''}")
                    else:
                        X_input = input_df[feature_cols].values
                        if imputer is not None:
                            X_input = imputer.transform(X_input)
                        if scaler is not None:
                            X_input = scaler.transform(X_input)

                        analyzer = ApplicabilityDomainAnalyzer(st.session_state.X_train.values)
                        analyzer.fit()

                        in_domain, distance = analyzer.is_within_domain(X_input)

                        if in_domain:
                            st.success(f"âœ… æ ·æœ¬åœ¨é€‚ç”¨åŸŸå†… (distance={distance:.4f})")
                        else:
                            st.warning(f"âš ï¸ æ ·æœ¬å¯èƒ½è¶…å‡ºé€‚ç”¨åŸŸ (distance={distance:.4f})")

        # -------- Tanimoto Similarity --------
        else:
            if not has_fp:
                st.warning("å½“å‰ç‰¹å¾ä¸­æœªæ£€æµ‹åˆ° MACCS/Morgan æŒ‡çº¹åˆ—ï¼Œæ— æ³•ä½¿ç”¨ Tanimoto é€‚ç”¨åŸŸã€‚")
            else:
                st.info("Tanimotoï¼šè®¡ç®—æ–°æ ·æœ¬ä¸è®­ç»ƒé›†ä¸­æœ€è¿‘é‚»çš„æŒ‡çº¹ç›¸ä¼¼åº¦ sim_maxã€‚sim_max è¿‡ä½é€šå¸¸æ„å‘³ç€ out-of-domainã€‚")

                threshold = st.slider("ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆå»ºè®® 0.20~0.30ï¼‰", 0.0, 1.0, 0.25, 0.01)
                top_k = st.slider("Top-K ç›¸ä¼¼æ ·æœ¬", 1, 20, 5)

                # æ„é€ è®­ç»ƒæŒ‡çº¹çŸ©é˜µ
                try:
                    X_train_raw = st.session_state.train_result["X_train_raw"]
                    y_train = st.session_state.train_result.get("y_train")
                    X_train_fp = X_train_raw[fp_cols]
                    analyzer = TanimotoADAnalyzer(X_train_fp, threshold=threshold, max_train_samples=5000, random_state=42)
                except Exception as e:
                    st.error(f"åˆå§‹åŒ– Tanimoto åˆ†æå™¨å¤±è´¥: {e}")
                    analyzer = None

                st.markdown("#### 1) å•æ ·æœ¬åˆ†æï¼ˆæ¨èï¼šä¸Šä¼ å•è¡Œæ–‡ä»¶ï¼‰")
                single_file = st.file_uploader("ä¸Šä¼ å•æ ·æœ¬æ–‡ä»¶ï¼ˆCSV/Excelï¼Œä¸€è¡Œå³å¯ï¼Œéœ€åŒ…å«æŒ‡çº¹åˆ—ï¼‰", type=['csv', 'xlsx', 'xls'], key="ad_single_file_tanimoto")
                if analyzer is not None and single_file is not None:
                    try:
                        qdf = load_data_file(single_file)
                        if qdf.shape[0] == 0:
                            st.error("æ–‡ä»¶ä¸ºç©º")
                        else:
                            row_idx = 0
                            if qdf.shape[0] > 1:
                                row_idx = st.number_input("é€‰æ‹©åˆ†æçš„è¡Œå·ï¼ˆä» 0 å¼€å§‹ï¼‰", 0, int(qdf.shape[0]-1), 0, key="ad_tani_row_idx")
                            qrow = qdf.iloc[int(row_idx)]
                            missing = [c for c in fp_cols if c not in qdf.columns]
                            if missing:
                                st.error(f"ç¼ºå°‘æŒ‡çº¹åˆ—: {missing[:10]}{'...' if len(missing)>10 else ''}")
                            else:
                                is_in, sim_max, top_df, fig = analyzer.analyze_single(qrow[fp_cols].values, top_k=top_k, threshold=threshold)
                                if is_in:
                                    st.success(f"âœ… åœ¨é€‚ç”¨åŸŸå†…ï¼šsim_max = {sim_max:.3f}")
                                else:
                                    st.warning(f"âš ï¸ å¯èƒ½è¶…å‡ºé€‚ç”¨åŸŸï¼šsim_max = {sim_max:.3f}")

                                # è¡¥å……ï¼šæ˜¾ç¤º top-k çš„ y_trainï¼ˆè‹¥æœ‰ï¼‰
                                if y_train is not None and len(y_train) >= top_df.shape[0]:
                                    top_df = top_df.copy()
                                    try:
                                        top_df["y_train"] = [float(y_train[int(i)]) for i in top_df["train_index"].values]
                                    except Exception:
                                        pass

                                st.dataframe(top_df, use_container_width=True, height=200)
                                st.pyplot(fig, use_container_width=True)

                    except Exception as e:
                        st.error(f"åˆ†æå¤±è´¥: {e}")

                st.markdown("---")
                st.markdown("#### 2) æ‰¹é‡åˆ†æï¼ˆè¾“å‡º sim_max / in_domainï¼‰")
                batch_file = st.file_uploader("ä¸Šä¼ æ‰¹é‡æ ·æœ¬æ–‡ä»¶ï¼ˆCSV/Excelï¼‰", type=['csv', 'xlsx', 'xls'], key="ad_batch_file_tanimoto")

                if analyzer is not None and batch_file is not None:
                    try:
                        qdf = load_data_file(batch_file)
                        missing = [c for c in fp_cols if c not in qdf.columns]
                        if missing:
                            st.error(f"ç¼ºå°‘æŒ‡çº¹åˆ—: {missing[:10]}{'...' if len(missing)>10 else ''}")
                        else:
                            sim_max_arr = analyzer.compute_batch_max_similarity(qdf[fp_cols])
                            out_df = qdf.copy()
                            out_df["sim_max"] = sim_max_arr
                            out_df["in_domain"] = out_df["sim_max"] >= threshold

                            st.success("âœ… æ‰¹é‡é€‚ç”¨åŸŸåˆ†æå®Œæˆ")
                            st.dataframe(out_df[["sim_max", "in_domain"]].head(20), use_container_width=True)

                            # å¯é€‰ï¼šå¦‚æœåŒ…å«ç›®æ ‡åˆ—ï¼Œå¯ç”» |error| vs sim_max
                            if st.session_state.get("target_col") in out_df.columns:
                                try:
                                    y_true = pd.to_numeric(out_df[st.session_state.target_col], errors="coerce")
                                    ok = y_true.notna()
                                    if ok.sum() >= 10:
                                        # é¢„æµ‹
                                        if pipeline is not None:
                                            y_pred = pipeline.predict(out_df.loc[ok, feature_cols])
                                        else:
                                            X_arr = out_df.loc[ok, feature_cols].values
                                            if imputer is not None:
                                                X_arr = imputer.transform(X_arr)
                                            if scaler is not None:
                                                X_arr = scaler.transform(X_arr)
                                            y_pred = model.predict(X_arr)
                                        abs_err = np.abs(y_true.loc[ok].values - y_pred)

                                        fig, ax = plt.subplots(figsize=(7, 4))
                                        ax.scatter(out_df.loc[ok, "sim_max"].values, abs_err, alpha=0.7, edgecolors="k", linewidth=0.3)
                                        ax.set_xlabel("sim_max (Tanimoto)")
                                        ax.set_ylabel("|error|")
                                        ax.set_title("|error| vs sim_max")
                                        ax.grid(True, linestyle="--", alpha=0.4)
                                        plt.tight_layout()
                                        st.pyplot(fig, use_container_width=True)
                                except Exception:
                                    pass

                            csv = out_df.to_csv(index=False).encode('utf-8')
                            st.download_button("ğŸ“¥ ä¸‹è½½é€‚ç”¨åŸŸç»“æœ CSV", csv, "tanimoto_ad_results.csv", "text/csv")

                    except Exception as e:
                        st.error(f"æ‰¹é‡åˆ†æå¤±è´¥: {e}")

def page_hyperparameter_optimization():
    """è¶…å‚æ•°ä¼˜åŒ–é¡µé¢"""
    st.title("âš™ï¸ è¶…å‚æ•°ä¼˜åŒ–")

    if st.session_state.data is None:
        st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    if not st.session_state.feature_cols or not st.session_state.target_col:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ç‰¹å¾é€‰æ‹©é¡µé¢é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡å˜é‡")
        return

    df = st.session_state.processed_data if st.session_state.processed_data is not None else st.session_state.data
    feature_cols = st.session_state.feature_cols
    target_col = st.session_state.target_col

    X = df[feature_cols]
    y = df[target_col]

    st.markdown("### Optunaæ™ºèƒ½è¶…å‚æ•°ä¼˜åŒ–")

    col1, col2 = st.columns(2)

    with col1:
        trainer = EnhancedModelTrainer()
        available_models = trainer.get_available_models()

        # æ”¯æŒä¼˜åŒ–çš„æ¨¡å‹
        optimizable_models = [
            "éšæœºæ£®æ—", "XGBoost", "LightGBM", "CatBoost",
            "SVR", "Ridgeå›å½’", "Lassoå›å½’", "ElasticNet",
            "AdaBoost", "æ¢¯åº¦æå‡æ ‘"
        ]
        optimizable_models = [m for m in optimizable_models if m in available_models]

        model_name = st.selectbox("é€‰æ‹©æ¨¡å‹", optimizable_models)

    with col2:
        n_trials = st.slider("ä¼˜åŒ–è½®æ•°", 10, 200, DEFAULT_OPTUNA_TRIALS)
        cv_folds = st.slider("äº¤å‰éªŒè¯æŠ˜æ•°", 3, 10, 5)

    # --- [æ–°å¢] è¿›åº¦æ¡ç»„ä»¶ ---
    progress_bar = st.progress(0)
    status_text = st.empty()

    if st.button("ğŸš€ å¼€å§‹ä¼˜åŒ–", type="primary"):
        try:
            optimizer = HyperparameterOptimizer()

            # å®šä¹‰è¿›åº¦æ›´æ–°å›è°ƒ
            def update_progress(p):
                progress_bar.progress(min(p, 1.0))
                status_text.text(f"æ­£åœ¨è¿›è¡Œä¼˜åŒ–... è¿›åº¦: {int(p * 100)}%")

            with st.spinner(f"æ­£åœ¨ä¼˜åŒ– {model_name}..."):
                # ä¼ é€’ progress_callback
                best_params, best_score, study = optimizer.optimize(
                    model_name, X, y,
                    n_trials=n_trials,
                    cv=cv_folds,
                    progress_callback=update_progress
                )

            # ä¼˜åŒ–å®Œæˆï¼Œè¿›åº¦æ¡æ»¡
            progress_bar.progress(100)
            status_text.text("ä¼˜åŒ–å®Œæˆï¼")

            st.success(f"âœ… ä¼˜åŒ–å®Œæˆï¼æœ€ä½³RÂ²åˆ†æ•°: {best_score:.4f}")

            # ä¿å­˜åˆ° session_state
            st.session_state.best_params = best_params
            st.session_state.best_score = best_score
            st.session_state.optimized_model_name = model_name  # è®°å½•ä¼˜åŒ–çš„æ˜¯å“ªä¸ªæ¨¡å‹

            st.markdown("### æœ€ä½³å‚æ•°")
            st.json(best_params)

            # ä¼˜åŒ–å†å²å¯è§†åŒ–
            if study is not None:
                st.markdown("### ä¼˜åŒ–å†å²")

                try:
                    import plotly.graph_objects as go

                    trials = study.trials
                    values = [t.value for t in trials if t.value is not None]

                    fig = go.Figure()
                    fig.add_trace(go.Scatter(
                        y=values,
                        mode='lines+markers',
                        name='RÂ² Score'
                    ))
                    fig.update_layout(
                        title='ä¼˜åŒ–è¿‡ç¨‹',
                        xaxis_title='Trial',
                        yaxis_title='RÂ² Score'
                    )
                    st.plotly_chart(fig, use_container_width=True)
                except:
                    pass

            # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒ
            if st.button("ğŸ¯ ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹"):
                result = trainer.train_model(
                    X, y,
                    model_name=model_name,
                    **best_params
                )

                st.session_state.model = result['model']
                st.session_state.pipeline = result.get('pipeline')
                st.session_state.scaler = result.get('scaler')
                st.session_state.imputer = result.get('imputer')
                st.session_state.X_train = result.get('X_train')
                st.session_state.X_test = result.get('X_test')
                st.session_state.y_train = result.get('y_train')
                st.session_state.y_test = result.get('y_test')
                st.session_state.model_name = model_name
                st.session_state.train_result = result
                st.session_state.cv_result = None
                st.session_state.best_params = best_params

                st.success(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼RÂ²: {result['r2']:.4f}")

        except Exception as e:
            st.error(f"âŒ ä¼˜åŒ–å¤±è´¥: {str(e)}")
            st.code(traceback.format_exc())


# ============================================================
# ä¸»å‡½æ•°
# ============================================================
def main():
    """ä¸»å‡½æ•°"""
    page = render_sidebar()

    if page == "ğŸ  é¦–é¡µ":
        page_home()
    elif page == "ğŸ“¤ æ•°æ®ä¸Šä¼ ":
        page_data_upload()
    elif page == "ğŸ” æ•°æ®æ¢ç´¢":
        page_data_explore()
    elif page == "ğŸ§¹ æ•°æ®æ¸…æ´—":
        page_data_cleaning()
    elif page == "âœ¨ æ•°æ®å¢å¼º":
        page_data_enhancement()
    elif page == "ğŸ§¬ åˆ†å­ç‰¹å¾":
        page_molecular_features()
    elif page == "ğŸ¯ ç‰¹å¾é€‰æ‹©":
        page_feature_selection()
    elif page == "ğŸ¤– æ¨¡å‹è®­ç»ƒ":
        page_model_training()
    elif page == "ğŸ“Š æ¨¡å‹è§£é‡Š":
        page_model_interpretation()
    elif page == "ğŸ”® é¢„æµ‹åº”ç”¨":
        page_prediction()
    elif page == "âš™ï¸ è¶…å‚ä¼˜åŒ–":
        page_hyperparameter_optimization()


if __name__ == "__main__":
    main()